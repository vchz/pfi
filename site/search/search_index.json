{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"PFI Documentation","text":"<p>PFI provides modular tools for score matching and flow regression on snapshot data. This documentation explains installation, basic usage, validation, and customization.</p>"},{"location":"#installation","title":"Installation","text":"<p>Clone the repository and install in editable mode:</p> <p><pre><code>git clone git@github.com:vchz/pfi.git\ncd pfi\npip install -e .\n</code></pre> The package currently depends on the following other packages: <code>numpy, torch, tqdm, POT, geomloss, torchcubicspline</code>.</p>"},{"location":"#how-to-use","title":"How To Use","text":"<p>In the <code>examples/</code> folder of the repository we provide two low-dimensional examples to get your hands on the package and how to perform probability flow inference. We summarize briefly below the few key points to work with this code.</p>"},{"location":"#prepare-the-data-matrix-x","title":"Prepare the data matrix <code>X</code>","text":"<p><code>X</code> must be a 2D array of shape <code>(n_samples_total, ndim + 1)</code>. The last column is time, and the first <code>ndim</code> columns are state coordinates.</p> <pre><code>import numpy as np\nfrom pfi.utils import X_from_snapshots\n\n# snaps: list of arrays, snaps[k].shape = (n_k, ndim)\n# times: array of shape (n_snaps,)\nX = X_from_snapshots(snaps, times)\n</code></pre> <p>We propose the PFI approach as a mean to fit arbitrary Fokker-Planck Equation (FPE) to such snapshots data. In brief, us and others showed that it amounts to fitting a flow model which depends on the drift of the FPE, but also on the gradient-log probability of the data, also known as score. We illustrate here how to do this for a specific model of Fokker-Planck Equation describing constitutive transcriptional dynamics of gene expression. However, this package allows to fit any flow model, even flow models that do not depend on the score.</p>"},{"location":"#train-a-score-model","title":"Train a score model","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom pfi.utils import DNN\nfrom pfi.score import ScoreMatching\n\nndim = X.shape[1] - 1\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nscore_model = DNN([ndim + 2, 64, 64, 64, ndim], activation=nn.ELU()).to(device)\n\nscore_reg = ScoreMatching(\n    model=score_model,\n    solver=\"dsm\",\n    solver_kwargs={\"L\": 10, \"n_epochs\": 2000, \"lr\": 1e-3, \"bs\": None, \"adp_flag\": 1},\n    device=device,\n)\nscore_reg.fit(X)\n</code></pre>"},{"location":"#validate-the-score","title":"Validate The Score","text":"<p><code>ScoreMatching.score(X)</code> computes per-time energy distance between generated and observed samples. Lower values indicate better match.</p> <p><pre><code>ed_per_time = score_reg.score(X)\nprint(\"Energy distance per time:\", ed_per_time)\nprint(\"Mean energy distance:\", ed_per_time.mean())\n</code></pre> You can also sample using the computed score. For the <code>solver='dsm'</code> the sampling is done with an annealed langevin dynamics scheme. A simple langevin dynamics could also do the trick.</p>"},{"location":"#choose-an-interpolant","title":"Choose an interpolant","text":"<pre><code>from pfi.flow.interpolants import ChebyshevInterpolant\n\ninterp = ChebyshevInterpolant(device=device)\n</code></pre> <p>Other available interpolants include <code>LinearInterpolant</code> and <code>SplineInterpolant</code>.</p>"},{"location":"#regress-a-flow-model","title":"Regress a flow model","text":"<pre><code>from pfi.flow import FlowRegression\nfrom pfi.flow.models import CLEFlow\n\ndrift_model = DNN([ndim, 64, 64, ndim], activation=nn.ELU()).to(device)\nflow_model = CLEFlow(\n    drift_model=drift_model,\n    score=score_reg.model_,\n    Ndim=ndim,\n    vol=1.0,\n    lx=1.0,\n)\n\nflow_reg = FlowRegression(\n    interp=interp,\n    model=flow_model,\n    growth_model=None,\n    solver=\"fm\",\n    solver_kwargs={\"n_epochs\": 2000, \"lr\": 1e-3, \"fac\": 1},\n    device=device,\n)\nflow_reg.fit(X)\n</code></pre>"},{"location":"#long-term-goal-of-this-package","title":"Long-term goal of this package","text":"<p>The package is designed for systematic model and solver comparisons. Currently, it is explicitely modular for the choice of the flow model, as long as it is an <code>nn.Module</code> which accepts input in of size<code>(batch_size, ndim+1)</code> with time as last variable. Models do not have to depend on the score, they can very well be any neural network which satisfies the aforementionned requirements.</p> <p>For the flow regression step, only the flow matching (<code>solver='fm'</code>) is available. We will work to implement other solvers (often less scalable) which we and other proposed in previous research papers. For systematic and benchmark and comparison purposes we plan to implement ode, sde and cnf based solvers for comparison purposes.</p> <p>For the score matching step, only the denoising score matching (<code>solver='dsm'</code>) is available. This is the most scalable solver we know to compute the score in high dimensions, but for benchmarking purposes we plan to implement other solvers like denoising score matching.</p> <p>We will work at updating this package by providing examples on single-cell RNA-seq data, with benchmarks and comparison.</p>"},{"location":"api/summary/","title":"Summary","text":"<ul> <li>pfi<ul> <li>flow<ul> <li>_base</li> <li>interpolants<ul> <li>_base</li> <li>_chebyshev</li> <li>_couplings</li> <li>_linear</li> <li>_spline</li> </ul> </li> <li>models</li> <li>solvers<ul> <li>_fm</li> <li>_ode</li> <li>_sde</li> </ul> </li> </ul> </li> <li>score<ul> <li>_base</li> <li>models</li> <li>solvers<ul> <li>_dsm</li> <li>_ssm</li> </ul> </li> </ul> </li> <li>utils<ul> <li>data</li> <li>nns</li> <li>simulations</li> </ul> </li> </ul> </li> </ul>"},{"location":"api/pfi/","title":"pfi","text":""},{"location":"api/pfi/#pfi","title":"<code>pfi</code>","text":"<p>Top-level package for PFI estimators and utilities.</p> <p>This module exposes the core subpackages for flow regression, score matching, and utility helpers.</p>"},{"location":"api/pfi/flow/","title":"flow","text":""},{"location":"api/pfi/flow/#pfi.flow","title":"<code>pfi.flow</code>","text":"<p>Public flow-regression APIs, models, and solver modules.</p> <p>This submodule provides flow regression estimators and the building blocks needed to train and evaluate flow models.</p>"},{"location":"api/pfi/flow/#pfi.flow.FlowRegression","title":"<code>FlowRegression(interp, model, growth_model=None, dt=1.0, solver='fm', solver_kwargs=None, device='cpu')</code>","text":"<p>Estimate drift (and optional growth) models from snapshot data.</p> <p>Parameters:</p> <ul> <li> <code>interp</code>               (<code>object</code>)           \u2013            <p>Interpolant object implementing <code>fit</code> and <code>predict</code>.</p> </li> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>Drift model consuming inputs of shape <code>(batch_size, ndim + 1)</code>.</p> </li> <li> <code>growth_model</code>               (<code>Module or None</code>, default:                   <code>None</code> )           \u2013            <p>Optional growth model used for unbalanced transport.</p> </li> <li> <code>dt</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Time step used by <code>predict</code> when advancing one step.</p> </li> <li> <code>solver</code>               (<code>fm</code>, default:                   <code>'fm'</code> )           \u2013            <p>Solver backend.</p> </li> <li> <code>solver_kwargs</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>Extra keyword arguments passed to the selected solver.</p> </li> <li> <code>device</code>               (<code>str or device</code>, default:                   <code>'cpu'</code> )           \u2013            <p>Device used for training and inference.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>Ndim_</code>               (<code>int</code>)           \u2013            <p>Inferred state dimension, set during <code>fit</code>.</p> </li> <li> <code>model_</code>               (<code>Module</code>)           \u2013            <p>Fitted drift model, set during <code>fit</code>.</p> </li> <li> <code>growth_model_</code>               (<code>Module or None</code>)           \u2013            <p>Fitted growth model when provided, set during <code>fit</code>.</p> </li> <li> <code>times_</code>               (<code>ndarray of shape (n_times,)</code>)           \u2013            <p>Sorted unique training times, set during <code>fit</code>.</p> </li> </ul> Source code in <code>pfi/flow/_base.py</code> <pre><code>def __init__(\n    self,\n    interp,\n    model,\n    growth_model=None,\n    dt=1.0,\n    solver=\"fm\",\n    solver_kwargs=None,\n    device=\"cpu\",\n):\n    self.model = model\n    self.interp = interp\n    self.growth_model = growth_model\n    self.dt = dt\n    self.solver = solver\n    self.solver_kwargs = solver_kwargs if solver_kwargs is not None else {}\n    self.device = device\n</code></pre>"},{"location":"api/pfi/flow/#pfi.flow.FlowRegression.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit flow models from time-augmented samples.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray of shape (n_samples, ndim + 1)</code>)           \u2013            <p>Input data where the last column contains time.</p> </li> <li> <code>y</code>               (<code>None</code>, default:                   <code>None</code> )           \u2013            <p>Ignored. Present for estimator API compatibility.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>self</code> (              <code>FlowRegression</code> )          \u2013            <p>Fitted estimator.</p> </li> </ul> Source code in <code>pfi/flow/_base.py</code> <pre><code>def fit(\n    self,\n    X,\n    y=None,\n):\n    \"\"\"Fit flow models from time-augmented samples.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, ndim + 1)\n        Input data where the last column contains time.\n    y : None, default=None\n        Ignored. Present for estimator API compatibility.\n\n    Returns\n    -------\n    self : FlowRegression\n        Fitted estimator.\n    \"\"\"\n    dist, times = snapshots_from_X(X)\n\n    self.Ndim_ = X.shape[1] - 1\n    self.model_ = self.model.to(self.device)\n    self.growth_model_ = self.growth_model\n    if self.growth_model_ is not None:\n        self.growth_model_ = self.growth_model_.to(self.device)\n\n    if self.solver == \"fm\":\n        self.model_, self.growth_model_, loss_hist = FM_(\n            dist,\n            times,\n            self.interp,\n            self.model_,\n            growth_model=self.growth_model_,\n            device=self.device,\n            **self.solver_kwargs,\n        )\n        self.loss_ = np.asarray(loss_hist)\n    else:\n        raise NotImplementedError(\"Other flow regression solvers (sde, ode, cnf) not implemented\")\n\n    self.times_ = np.unique(X[:, -1])\n    return self\n</code></pre>"},{"location":"api/pfi/flow/#pfi.flow.FlowRegression.predict","title":"<code>predict(X)</code>","text":"<p>Predict next-state positions by Euler stepping the learned drift.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>array-like of shape (n_samples, ndim + 1)</code>)           \u2013            <p>Input states with time in the last column.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>x_next</code> (              <code>ndarray of shape (n_samples, ndim)</code> )          \u2013            <p>One-step predictions <code>x + dt * f(x, t)</code>.</p> </li> </ul> Source code in <code>pfi/flow/_base.py</code> <pre><code>def predict(\n    self,\n    X,\n):\n    \"\"\"Predict next-state positions by Euler stepping the learned drift.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, ndim + 1)\n        Input states with time in the last column.\n\n    Returns\n    -------\n    x_next : ndarray of shape (n_samples, ndim)\n        One-step predictions ``x + dt * f(x, t)``.\n    \"\"\"\n    X = torch.tensor(X, dtype=torch.float32, device=self.device)\n    x = X[:, : self.Ndim_]\n    t = X[:, -1:]\n    inp = torch.cat([x, t], dim=1)\n    drift = self.model_(inp)\n    x_next = x + self.dt * drift\n    return x_next.detach().cpu().numpy()\n</code></pre>"},{"location":"api/pfi/flow/#pfi.flow.FlowRegression.score","title":"<code>score(X, y)</code>","text":"<p>Compute per-time energy distance between predictions and targets.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray of shape (n_samples, ndim + 1)</code>)           \u2013            <p>Inputs used for prediction.</p> </li> <li> <code>y</code>               (<code>ndarray of shape (n_targets, ndim) or (n_targets, ndim + 1)</code>)           \u2013            <p>Targets. If time is present, rows at <code>t + dt</code> are selected.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>scores</code> (              <code>ndarray of shape (n_times,)</code> )          \u2013            <p>Energy distance for each unique input time.</p> </li> </ul> Source code in <code>pfi/flow/_base.py</code> <pre><code>def score(\n    self,\n    X,\n    y,\n):\n    \"\"\"Compute per-time energy distance between predictions and targets.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, ndim + 1)\n        Inputs used for prediction.\n    y : ndarray of shape (n_targets, ndim) or (n_targets, ndim + 1)\n        Targets. If time is present, rows at ``t + dt`` are selected.\n\n    Returns\n    -------\n    scores : ndarray of shape (n_times,)\n        Energy distance for each unique input time.\n    \"\"\"\n    import geomloss\n\n    X = np.asarray(X)\n    y = np.asarray(y)\n    times = np.unique(X[:, -1])\n    scores = []\n\n    loss = geomloss.SamplesLoss(\"energy\")\n    for t in times:\n        x_t = X[X[:, -1] == t]\n        pred = self.predict(x_t)\n        if y.shape[1] == self.Ndim_ + 1:\n            y_t = y[np.isclose(y[:, -1], t + self.dt)]\n            y_t = y_t[:, : self.Ndim_]\n        else:\n            y_t = y\n        ed = loss(\n            torch.tensor(pred, dtype=torch.float32, device=self.device),\n            torch.tensor(y_t, dtype=torch.float32, device=self.device),\n        ).item()\n        scores.append(ed)\n\n    return np.asarray(scores)\n</code></pre>"},{"location":"api/pfi/flow/_base/","title":"_base","text":""},{"location":"api/pfi/flow/_base/#pfi.flow._base","title":"<code>pfi.flow._base</code>","text":"<p>Flow regression estimator wrapping flow-matching solvers.</p>"},{"location":"api/pfi/flow/_base/#pfi.flow._base.FlowRegression","title":"<code>FlowRegression(interp, model, growth_model=None, dt=1.0, solver='fm', solver_kwargs=None, device='cpu')</code>","text":"<p>Estimate drift (and optional growth) models from snapshot data.</p> <p>Parameters:</p> <ul> <li> <code>interp</code>               (<code>object</code>)           \u2013            <p>Interpolant object implementing <code>fit</code> and <code>predict</code>.</p> </li> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>Drift model consuming inputs of shape <code>(batch_size, ndim + 1)</code>.</p> </li> <li> <code>growth_model</code>               (<code>Module or None</code>, default:                   <code>None</code> )           \u2013            <p>Optional growth model used for unbalanced transport.</p> </li> <li> <code>dt</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Time step used by <code>predict</code> when advancing one step.</p> </li> <li> <code>solver</code>               (<code>fm</code>, default:                   <code>'fm'</code> )           \u2013            <p>Solver backend.</p> </li> <li> <code>solver_kwargs</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>Extra keyword arguments passed to the selected solver.</p> </li> <li> <code>device</code>               (<code>str or device</code>, default:                   <code>'cpu'</code> )           \u2013            <p>Device used for training and inference.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>Ndim_</code>               (<code>int</code>)           \u2013            <p>Inferred state dimension, set during <code>fit</code>.</p> </li> <li> <code>model_</code>               (<code>Module</code>)           \u2013            <p>Fitted drift model, set during <code>fit</code>.</p> </li> <li> <code>growth_model_</code>               (<code>Module or None</code>)           \u2013            <p>Fitted growth model when provided, set during <code>fit</code>.</p> </li> <li> <code>times_</code>               (<code>ndarray of shape (n_times,)</code>)           \u2013            <p>Sorted unique training times, set during <code>fit</code>.</p> </li> </ul> Source code in <code>pfi/flow/_base.py</code> <pre><code>def __init__(\n    self,\n    interp,\n    model,\n    growth_model=None,\n    dt=1.0,\n    solver=\"fm\",\n    solver_kwargs=None,\n    device=\"cpu\",\n):\n    self.model = model\n    self.interp = interp\n    self.growth_model = growth_model\n    self.dt = dt\n    self.solver = solver\n    self.solver_kwargs = solver_kwargs if solver_kwargs is not None else {}\n    self.device = device\n</code></pre>"},{"location":"api/pfi/flow/_base/#pfi.flow._base.FlowRegression.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit flow models from time-augmented samples.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray of shape (n_samples, ndim + 1)</code>)           \u2013            <p>Input data where the last column contains time.</p> </li> <li> <code>y</code>               (<code>None</code>, default:                   <code>None</code> )           \u2013            <p>Ignored. Present for estimator API compatibility.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>self</code> (              <code>FlowRegression</code> )          \u2013            <p>Fitted estimator.</p> </li> </ul> Source code in <code>pfi/flow/_base.py</code> <pre><code>def fit(\n    self,\n    X,\n    y=None,\n):\n    \"\"\"Fit flow models from time-augmented samples.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, ndim + 1)\n        Input data where the last column contains time.\n    y : None, default=None\n        Ignored. Present for estimator API compatibility.\n\n    Returns\n    -------\n    self : FlowRegression\n        Fitted estimator.\n    \"\"\"\n    dist, times = snapshots_from_X(X)\n\n    self.Ndim_ = X.shape[1] - 1\n    self.model_ = self.model.to(self.device)\n    self.growth_model_ = self.growth_model\n    if self.growth_model_ is not None:\n        self.growth_model_ = self.growth_model_.to(self.device)\n\n    if self.solver == \"fm\":\n        self.model_, self.growth_model_, loss_hist = FM_(\n            dist,\n            times,\n            self.interp,\n            self.model_,\n            growth_model=self.growth_model_,\n            device=self.device,\n            **self.solver_kwargs,\n        )\n        self.loss_ = np.asarray(loss_hist)\n    else:\n        raise NotImplementedError(\"Other flow regression solvers (sde, ode, cnf) not implemented\")\n\n    self.times_ = np.unique(X[:, -1])\n    return self\n</code></pre>"},{"location":"api/pfi/flow/_base/#pfi.flow._base.FlowRegression.predict","title":"<code>predict(X)</code>","text":"<p>Predict next-state positions by Euler stepping the learned drift.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>array-like of shape (n_samples, ndim + 1)</code>)           \u2013            <p>Input states with time in the last column.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>x_next</code> (              <code>ndarray of shape (n_samples, ndim)</code> )          \u2013            <p>One-step predictions <code>x + dt * f(x, t)</code>.</p> </li> </ul> Source code in <code>pfi/flow/_base.py</code> <pre><code>def predict(\n    self,\n    X,\n):\n    \"\"\"Predict next-state positions by Euler stepping the learned drift.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, ndim + 1)\n        Input states with time in the last column.\n\n    Returns\n    -------\n    x_next : ndarray of shape (n_samples, ndim)\n        One-step predictions ``x + dt * f(x, t)``.\n    \"\"\"\n    X = torch.tensor(X, dtype=torch.float32, device=self.device)\n    x = X[:, : self.Ndim_]\n    t = X[:, -1:]\n    inp = torch.cat([x, t], dim=1)\n    drift = self.model_(inp)\n    x_next = x + self.dt * drift\n    return x_next.detach().cpu().numpy()\n</code></pre>"},{"location":"api/pfi/flow/_base/#pfi.flow._base.FlowRegression.score","title":"<code>score(X, y)</code>","text":"<p>Compute per-time energy distance between predictions and targets.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray of shape (n_samples, ndim + 1)</code>)           \u2013            <p>Inputs used for prediction.</p> </li> <li> <code>y</code>               (<code>ndarray of shape (n_targets, ndim) or (n_targets, ndim + 1)</code>)           \u2013            <p>Targets. If time is present, rows at <code>t + dt</code> are selected.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>scores</code> (              <code>ndarray of shape (n_times,)</code> )          \u2013            <p>Energy distance for each unique input time.</p> </li> </ul> Source code in <code>pfi/flow/_base.py</code> <pre><code>def score(\n    self,\n    X,\n    y,\n):\n    \"\"\"Compute per-time energy distance between predictions and targets.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, ndim + 1)\n        Inputs used for prediction.\n    y : ndarray of shape (n_targets, ndim) or (n_targets, ndim + 1)\n        Targets. If time is present, rows at ``t + dt`` are selected.\n\n    Returns\n    -------\n    scores : ndarray of shape (n_times,)\n        Energy distance for each unique input time.\n    \"\"\"\n    import geomloss\n\n    X = np.asarray(X)\n    y = np.asarray(y)\n    times = np.unique(X[:, -1])\n    scores = []\n\n    loss = geomloss.SamplesLoss(\"energy\")\n    for t in times:\n        x_t = X[X[:, -1] == t]\n        pred = self.predict(x_t)\n        if y.shape[1] == self.Ndim_ + 1:\n            y_t = y[np.isclose(y[:, -1], t + self.dt)]\n            y_t = y_t[:, : self.Ndim_]\n        else:\n            y_t = y\n        ed = loss(\n            torch.tensor(pred, dtype=torch.float32, device=self.device),\n            torch.tensor(y_t, dtype=torch.float32, device=self.device),\n        ).item()\n        scores.append(ed)\n\n    return np.asarray(scores)\n</code></pre>"},{"location":"api/pfi/flow/models/","title":"models","text":""},{"location":"api/pfi/flow/models/#pfi.flow.models","title":"<code>pfi.flow.models</code>","text":"<p>Flow model classes used by flow-matching regression.</p>"},{"location":"api/pfi/flow/models/#pfi.flow.models.AdditiveFlow","title":"<code>AdditiveFlow(net, score, Ndim, lx=1.0)</code>","text":"<p>               Bases: <code>_CompoundModel</code></p> <p>Additive-noise flow model with autonomous drift and score correction.</p> Source code in <code>pfi/flow/models.py</code> <pre><code>def __init__(\n    self,\n    net,\n    score,\n    Ndim,\n    lx=1.0,\n):\n    super(AdditiveFlow, self).__init__(net)\n    self.score = score\n    self.Ndim = Ndim\n    self.lx = lx\n</code></pre>"},{"location":"api/pfi/flow/models/#pfi.flow.models.AutonomousFlow","title":"<code>AutonomousFlow(net, Ndim)</code>","text":"<p>               Bases: <code>_CompoundModel</code></p> <p>Time-independent drift model.</p> <p>Parameters:</p> <ul> <li> <code>net</code>               (<code>Module</code>)           \u2013            <p>Drift network acting on state coordinates only.</p> </li> <li> <code>Ndim</code>               (<code>int</code>)           \u2013            <p>State dimension.</p> </li> </ul> Source code in <code>pfi/flow/models.py</code> <pre><code>def __init__(\n    self,\n    net,\n    Ndim,\n):\n    super(AutonomousFlow, self).__init__(net)\n    self.Ndim = Ndim\n</code></pre>"},{"location":"api/pfi/flow/models/#pfi.flow.models.CLEFlow","title":"<code>CLEFlow(net, score, Ndim, vol=1.0, lx=1.0)</code>","text":"<p>               Bases: <code>_CompoundModel</code></p> <p>Chemical-Langevin-inspired flow model.</p> <p>Parameters:</p> <ul> <li> <code>net</code>               (<code>Module</code>)           \u2013            <p>Drift network taking <code>(batch_size, Ndim)</code> inputs.</p> </li> <li> <code>score</code>               (<code>Module</code>)           \u2013            <p>Score model taking <code>(batch_size, Ndim + 1)</code> inputs.</p> </li> <li> <code>Ndim</code>               (<code>int</code>)           \u2013            <p>State dimension.</p> </li> <li> <code>vol</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Volume scaling of stochastic corrections.</p> </li> <li> <code>lx</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Linear degradation coefficient.</p> </li> </ul> Source code in <code>pfi/flow/models.py</code> <pre><code>def __init__(\n    self,\n    net,\n    score,\n    Ndim,\n    vol=1.0,\n    lx=1.0,\n):\n    super(CLEFlow, self).__init__(net)\n    self.score = score\n    self.Ndim = Ndim\n    self.vol = vol\n    self.lx = lx\n</code></pre>"},{"location":"api/pfi/flow/models/#pfi.flow.models.GradientFlow","title":"<code>GradientFlow(net, Ndim)</code>","text":"<p>               Bases: <code>_CompoundModel</code></p> <p>Gradient flow model parameterized by a scalar potential.</p> <p>Parameters:</p> <ul> <li> <code>net</code>               (<code>Module</code>)           \u2013            <p>Network mapping <code>(batch_size, Ndim)</code> to scalar potential values.</p> </li> <li> <code>Ndim</code>               (<code>int</code>)           \u2013            <p>State dimension.</p> </li> </ul> Source code in <code>pfi/flow/models.py</code> <pre><code>def __init__(\n    self,\n    net,\n    Ndim,\n):\n    super(GradientFlow, self).__init__(net)\n    self.Ndim = Ndim\n</code></pre>"},{"location":"api/pfi/flow/models/#pfi.flow.models.MultiplicativeFlow","title":"<code>MultiplicativeFlow(net, score, Ndim, lx=1.0)</code>","text":"<p>               Bases: <code>_CompoundModel</code></p> <p>Multiplicative-noise flow model with autonomous drift and score correction.</p> Source code in <code>pfi/flow/models.py</code> <pre><code>def __init__(\n    self,\n    net,\n    score,\n    Ndim,\n    lx=1.0,\n):\n    super(MultiplicativeFlow, self).__init__(net)\n    self.score = score\n    self.Ndim = Ndim\n    self.lx = lx\n</code></pre>"},{"location":"api/pfi/flow/models/#pfi.flow.models.OUFlow","title":"<code>OUFlow(net, score, D)</code>","text":"<p>               Bases: <code>_CompoundModel</code></p> <p>Ornstein-Uhlenbeck flow model using an external score function.</p> <p>Parameters:</p> <ul> <li> <code>net</code>               (<code>torch.Tensor of shape (ndim, ndim)</code>)           \u2013            <p>Drift matrix.</p> </li> <li> <code>score</code>               (<code>Module</code>)           \u2013            <p>Score model mapping <code>(batch_size, ndim + 1)</code> to <code>(batch_size, ndim)</code>.</p> </li> <li> <code>D</code>               (<code>torch.Tensor of shape (ndim, ndim)</code>)           \u2013            <p>Diffusion matrix.</p> </li> </ul> Source code in <code>pfi/flow/models.py</code> <pre><code>def __init__(\n    self,\n    net,\n    score,\n    D,\n):\n    super(OUFlow, self).__init__(nn.Parameter(net))\n    self.score = score\n    self.Ndim = D.shape[0]\n    self.B = self.net\n    self.D = D\n</code></pre>"},{"location":"api/pfi/flow/interpolants/","title":"interpolants","text":""},{"location":"api/pfi/flow/interpolants/#pfi.flow.interpolants","title":"<code>pfi.flow.interpolants</code>","text":""},{"location":"api/pfi/flow/interpolants/#pfi.flow.interpolants.BaseInterpolant","title":"<code>BaseInterpolant(device='cpu')</code>","text":"<p>Abstract base class for trajectory interpolants.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>str or device</code>, default:                   <code>'cpu'</code> )           \u2013            <p>Device used by derived interpolants.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_base.py</code> <pre><code>def __init__(\n    self,\n    device=\"cpu\",\n):\n    self.device = device\n</code></pre>"},{"location":"api/pfi/flow/interpolants/#pfi.flow.interpolants.BaseInterpolant.fit","title":"<code>fit(nodes_fit, Dist)</code>","text":"<p>Store fit data and call implementation-specific fitting.</p> <p>Parameters:</p> <ul> <li> <code>nodes_fit</code>               (<code>torch.Tensor of shape (batch_size, n_nodes)</code>)           \u2013            <p>Time nodes used for fitting.</p> </li> <li> <code>Dist</code>               (<code>torch.Tensor of shape (batch_size, n_nodes, ndim)</code>)           \u2013            <p>Trajectory values at <code>nodes_fit</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>self</code> (              <code>BaseInterpolant</code> )          \u2013            <p>Fitted interpolant.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_base.py</code> <pre><code>def fit(\n    self,\n    nodes_fit,\n    Dist,\n):\n    \"\"\"Store fit data and call implementation-specific fitting.\n\n    Parameters\n    ----------\n    nodes_fit : torch.Tensor of shape (batch_size, n_nodes)\n        Time nodes used for fitting.\n    Dist : torch.Tensor of shape (batch_size, n_nodes, ndim)\n        Trajectory values at ``nodes_fit``.\n\n    Returns\n    -------\n    self : BaseInterpolant\n        Fitted interpolant.\n    \"\"\"\n    self.y_fit_ = Dist\n    self.t_fit_ = nodes_fit\n\n    self._fit()\n\n    return self\n</code></pre>"},{"location":"api/pfi/flow/interpolants/#pfi.flow.interpolants.BaseInterpolant.predict","title":"<code>predict(t_eval)</code>","text":"<p>Evaluate interpolant and derivative at requested nodes.</p> <p>Parameters:</p> <ul> <li> <code>t_eval</code>               (<code>torch.Tensor of shape (batch_size, n_eval)</code>)           \u2013            <p>Evaluation nodes.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>x_interp</code> (              <code>torch.Tensor of shape (batch_size, n_eval, ndim)</code> )          \u2013            <p>Interpolated trajectories.</p> </li> <li> <code>dx_interp</code> (              <code>torch.Tensor of shape (batch_size, n_eval, ndim)</code> )          \u2013            <p>Time derivatives at evaluation nodes.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_base.py</code> <pre><code>def predict(self, t_eval):\n    \"\"\"Evaluate interpolant and derivative at requested nodes.\n\n    Parameters\n    ----------\n    t_eval : torch.Tensor of shape (batch_size, n_eval)\n        Evaluation nodes.\n\n    Returns\n    -------\n    x_interp : torch.Tensor of shape (batch_size, n_eval, ndim)\n        Interpolated trajectories.\n    dx_interp : torch.Tensor of shape (batch_size, n_eval, ndim)\n        Time derivatives at evaluation nodes.\n    \"\"\"\n    x_interp, dx_interp = self._predict(t_eval)\n    return x_interp, dx_interp\n</code></pre>"},{"location":"api/pfi/flow/interpolants/#pfi.flow.interpolants.ChebyshevInterpolant","title":"<code>ChebyshevInterpolant(reg_=0.01, device='cpu')</code>","text":"<p>               Bases: <code>BaseInterpolant</code></p> <p>Regularized Chebyshev interpolant for batched trajectories.</p> <p>Parameters:</p> <ul> <li> <code>reg_</code>               (<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>Curvature regularization weight.</p> </li> <li> <code>device</code>               (<code>str or device</code>, default:                   <code>'cpu'</code> )           \u2013            <p>Computation device.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_chebyshev.py</code> <pre><code>def __init__(\n    self,\n    reg_=0.01,\n    device=\"cpu\",\n):\n    super().__init__(\n        device=device,\n    )\n    self.reg_ = reg_\n</code></pre>"},{"location":"api/pfi/flow/interpolants/#pfi.flow.interpolants.ChebyshevInterpolant.fit","title":"<code>fit(nodes_fit, Dist)</code>","text":"<p>Store fit data and call implementation-specific fitting.</p> <p>Parameters:</p> <ul> <li> <code>nodes_fit</code>               (<code>torch.Tensor of shape (batch_size, n_nodes)</code>)           \u2013            <p>Time nodes used for fitting.</p> </li> <li> <code>Dist</code>               (<code>torch.Tensor of shape (batch_size, n_nodes, ndim)</code>)           \u2013            <p>Trajectory values at <code>nodes_fit</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>self</code> (              <code>BaseInterpolant</code> )          \u2013            <p>Fitted interpolant.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_base.py</code> <pre><code>def fit(\n    self,\n    nodes_fit,\n    Dist,\n):\n    \"\"\"Store fit data and call implementation-specific fitting.\n\n    Parameters\n    ----------\n    nodes_fit : torch.Tensor of shape (batch_size, n_nodes)\n        Time nodes used for fitting.\n    Dist : torch.Tensor of shape (batch_size, n_nodes, ndim)\n        Trajectory values at ``nodes_fit``.\n\n    Returns\n    -------\n    self : BaseInterpolant\n        Fitted interpolant.\n    \"\"\"\n    self.y_fit_ = Dist\n    self.t_fit_ = nodes_fit\n\n    self._fit()\n\n    return self\n</code></pre>"},{"location":"api/pfi/flow/interpolants/#pfi.flow.interpolants.ChebyshevInterpolant.predict","title":"<code>predict(t_eval)</code>","text":"<p>Evaluate interpolated trajectories and derivatives.</p> <p>Parameters:</p> <ul> <li> <code>t_eval</code>               (<code>torch.Tensor of shape (batch_size, n_eval)</code>)           \u2013            <p>Evaluation nodes.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>x_interp</code> (              <code>torch.Tensor of shape (batch_size, n_eval, ndim)</code> )          \u2013            <p>Interpolated trajectories.</p> </li> <li> <code>dx_interp</code> (              <code>torch.Tensor of shape (batch_size, n_eval, ndim)</code> )          \u2013            <p>Time derivatives.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_chebyshev.py</code> <pre><code>def predict(\n    self,\n    t_eval,\n):\n    \"\"\"Evaluate interpolated trajectories and derivatives.\n\n    Parameters\n    ----------\n    t_eval : torch.Tensor of shape (batch_size, n_eval)\n        Evaluation nodes.\n\n    Returns\n    -------\n    x_interp : torch.Tensor of shape (batch_size, n_eval, ndim)\n        Interpolated trajectories.\n    dx_interp : torch.Tensor of shape (batch_size, n_eval, ndim)\n        Time derivatives.\n    \"\"\"\n    x_interp = self.p_(t_eval)\n    dx_interp = self.p_prime_(t_eval)\n    return x_interp, dx_interp\n</code></pre>"},{"location":"api/pfi/flow/interpolants/#pfi.flow.interpolants.LinearInterpolant","title":"<code>LinearInterpolant(device='cpu')</code>","text":"<p>               Bases: <code>BaseInterpolant</code></p> <p>Batched piecewise-linear interpolant.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>str or device</code>, default:                   <code>'cpu'</code> )           \u2013            <p>Computation device inherited from <code>pfi.flow.interpolants.BaseInterpolant</code>.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_base.py</code> <pre><code>def __init__(\n    self,\n    device=\"cpu\",\n):\n    self.device = device\n</code></pre>"},{"location":"api/pfi/flow/interpolants/#pfi.flow.interpolants.LinearInterpolant.fit","title":"<code>fit(nodes_fit, Dist)</code>","text":"<p>Store fit data and call implementation-specific fitting.</p> <p>Parameters:</p> <ul> <li> <code>nodes_fit</code>               (<code>torch.Tensor of shape (batch_size, n_nodes)</code>)           \u2013            <p>Time nodes used for fitting.</p> </li> <li> <code>Dist</code>               (<code>torch.Tensor of shape (batch_size, n_nodes, ndim)</code>)           \u2013            <p>Trajectory values at <code>nodes_fit</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>self</code> (              <code>BaseInterpolant</code> )          \u2013            <p>Fitted interpolant.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_base.py</code> <pre><code>def fit(\n    self,\n    nodes_fit,\n    Dist,\n):\n    \"\"\"Store fit data and call implementation-specific fitting.\n\n    Parameters\n    ----------\n    nodes_fit : torch.Tensor of shape (batch_size, n_nodes)\n        Time nodes used for fitting.\n    Dist : torch.Tensor of shape (batch_size, n_nodes, ndim)\n        Trajectory values at ``nodes_fit``.\n\n    Returns\n    -------\n    self : BaseInterpolant\n        Fitted interpolant.\n    \"\"\"\n    self.y_fit_ = Dist\n    self.t_fit_ = nodes_fit\n\n    self._fit()\n\n    return self\n</code></pre>"},{"location":"api/pfi/flow/interpolants/#pfi.flow.interpolants.LinearInterpolant.predict","title":"<code>predict(t_eval)</code>","text":"<p>Evaluate linear interpolation and piecewise-constant derivative.</p> <p>Parameters:</p> <ul> <li> <code>t_eval</code>               (<code>torch.Tensor of shape (batch_size, n_eval)</code>)           \u2013            <p>Evaluation nodes.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>interp</code> (              <code>torch.Tensor of shape (batch_size, n_eval, ndim)</code> )          \u2013            <p>Interpolated trajectories.</p> </li> <li> <code>deriv</code> (              <code>torch.Tensor of shape (batch_size, n_eval, ndim)</code> )          \u2013            <p>Piecewise-constant derivatives.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_linear.py</code> <pre><code>def predict(\n    self,\n    t_eval,\n):\n    \"\"\"Evaluate linear interpolation and piecewise-constant derivative.\n\n    Parameters\n    ----------\n    t_eval : torch.Tensor of shape (batch_size, n_eval)\n        Evaluation nodes.\n\n    Returns\n    -------\n    interp : torch.Tensor of shape (batch_size, n_eval, ndim)\n        Interpolated trajectories.\n    deriv : torch.Tensor of shape (batch_size, n_eval, ndim)\n        Piecewise-constant derivatives.\n    \"\"\"\n    y_fit = self.y_fit_\n    t_fit = self.t_fit_\n\n    B, T1, D = y_fit.shape\n\n    idx = torch.searchsorted(t_fit, t_eval, right=True) - 1\n    idx = idx.clamp(0, T1 - 2)\n\n    t0 = torch.gather(t_fit, 1, idx)\n    t1 = torch.gather(t_fit, 1, idx + 1)\n\n    y0 = torch.gather(y_fit, 1, idx.unsqueeze(-1).expand(-1, -1, D))\n    y1 = torch.gather(y_fit, 1, (idx + 1).unsqueeze(-1).expand(-1, -1, D))\n\n    delta_t = (t1 - t0).unsqueeze(-1)\n    alpha = ((t_eval - t0) / (t1 - t0)).unsqueeze(-1)\n    alpha = torch.where(delta_t != 0, alpha, torch.zeros_like(alpha))\n\n    interp = (1 - alpha) * y0 + alpha * y1\n    deriv = torch.where(delta_t != 0, (y1 - y0) / delta_t, torch.zeros_like(y0))\n\n    return interp, deriv\n</code></pre>"},{"location":"api/pfi/flow/interpolants/#pfi.flow.interpolants.SplineInterpolant","title":"<code>SplineInterpolant(device='cpu')</code>","text":"<p>               Bases: <code>BaseInterpolant</code></p> <p>Batched natural cubic spline interpolant.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>str or device</code>, default:                   <code>'cpu'</code> )           \u2013            <p>Computation device inherited from <code>pfi.flow.interpolants.BaseInterpolant</code>.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_base.py</code> <pre><code>def __init__(\n    self,\n    device=\"cpu\",\n):\n    self.device = device\n</code></pre>"},{"location":"api/pfi/flow/interpolants/#pfi.flow.interpolants.SplineInterpolant.fit","title":"<code>fit(nodes_fit, Dist)</code>","text":"<p>Store fit data and call implementation-specific fitting.</p> <p>Parameters:</p> <ul> <li> <code>nodes_fit</code>               (<code>torch.Tensor of shape (batch_size, n_nodes)</code>)           \u2013            <p>Time nodes used for fitting.</p> </li> <li> <code>Dist</code>               (<code>torch.Tensor of shape (batch_size, n_nodes, ndim)</code>)           \u2013            <p>Trajectory values at <code>nodes_fit</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>self</code> (              <code>BaseInterpolant</code> )          \u2013            <p>Fitted interpolant.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_base.py</code> <pre><code>def fit(\n    self,\n    nodes_fit,\n    Dist,\n):\n    \"\"\"Store fit data and call implementation-specific fitting.\n\n    Parameters\n    ----------\n    nodes_fit : torch.Tensor of shape (batch_size, n_nodes)\n        Time nodes used for fitting.\n    Dist : torch.Tensor of shape (batch_size, n_nodes, ndim)\n        Trajectory values at ``nodes_fit``.\n\n    Returns\n    -------\n    self : BaseInterpolant\n        Fitted interpolant.\n    \"\"\"\n    self.y_fit_ = Dist\n    self.t_fit_ = nodes_fit\n\n    self._fit()\n\n    return self\n</code></pre>"},{"location":"api/pfi/flow/interpolants/#pfi.flow.interpolants.SplineInterpolant.predict","title":"<code>predict(t_eval)</code>","text":"<p>Evaluate spline interpolation and derivative.</p> <p>Parameters:</p> <ul> <li> <code>t_eval</code>               (<code>torch.Tensor of shape (batch_size, n_eval) or (n_eval,)</code>)           \u2013            <p>Evaluation nodes.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>eval_</code> (              <code>torch.Tensor of shape (batch_size, n_eval, ndim)</code> )          \u2013            <p>Interpolated trajectories.</p> </li> <li> <code>derv_</code> (              <code>torch.Tensor of shape (batch_size, n_eval, ndim)</code> )          \u2013            <p>Derivative trajectories.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_spline.py</code> <pre><code>def predict(self, t_eval):\n    \"\"\"Evaluate spline interpolation and derivative.\n\n    Parameters\n    ----------\n    t_eval : torch.Tensor of shape (batch_size, n_eval) or (n_eval,)\n        Evaluation nodes.\n\n    Returns\n    -------\n    eval_ : torch.Tensor of shape (batch_size, n_eval, ndim)\n        Interpolated trajectories.\n    derv_ : torch.Tensor of shape (batch_size, n_eval, ndim)\n        Derivative trajectories.\n    \"\"\"\n    if t_eval.dim() == 2:\n        t_eval_1d = t_eval[0]\n    else:\n        t_eval_1d = t_eval\n\n    eval_ = self.spline_.evaluate(t_eval_1d)\n    derv_ = self.spline_.derivative(t_eval_1d)\n\n    eval_ = torch.permute(eval_, [2, 1, 0, 3]).squeeze()\n    derv_ = torch.permute(derv_, [2, 1, 0, 3]).squeeze()\n\n    eval_ = torch.permute(eval_, (1, 0, 2))\n    derv_ = torch.permute(derv_, (1, 0, 2))\n\n    return eval_, derv_\n</code></pre>"},{"location":"api/pfi/flow/interpolants/#pfi.flow.interpolants.MMOT_trajectories","title":"<code>MMOT_trajectories(dist, nb=1, device='cpu')</code>","text":"<p>Construct multi-marginal OT trajectories from snapshot lists.</p> <p>Parameters:</p> <ul> <li> <code>dist</code>               (<code>list of array-like, length nsnaps</code>)           \u2013            <p><code>dist[k]</code> has shape <code>(n_k, ndim)</code>.</p> </li> <li> <code>nb</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of mini-batches used for OT stitching.</p> </li> <li> <code>device</code>               (<code>str or device</code>, default:                   <code>'cpu'</code> )           \u2013            <p>Target device for returned trajectories.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dist</code> (              <code>torch.Tensor of shape (nsnaps, nsamples, ndim)</code> )          \u2013            <p>Subsampled and stacked snapshots.</p> </li> <li> <code>batch_ot_samples</code> (              <code>torch.Tensor of shape (nsnaps, nsamples, ndim)</code> )          \u2013            <p>OT-coupled trajectories across snapshots.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_couplings.py</code> <pre><code>def MMOT_trajectories(\n    dist,\n    nb=1,\n    device=\"cpu\",\n):\n    \"\"\"Construct multi-marginal OT trajectories from snapshot lists.\n\n    Parameters\n    ----------\n    dist : list of array-like, length nsnaps\n        ``dist[k]`` has shape ``(n_k, ndim)``.\n    nb : int, default=1\n        Number of mini-batches used for OT stitching.\n    device : str or torch.device, default='cpu'\n        Target device for returned trajectories.\n\n    Returns\n    -------\n    dist : torch.Tensor of shape (nsnaps, nsamples, ndim)\n        Subsampled and stacked snapshots.\n    batch_ot_samples : torch.Tensor of shape (nsnaps, nsamples, ndim)\n        OT-coupled trajectories across snapshots.\n    \"\"\"\n    dist = torch.stack(subsample_shuffle(dist), dim=0)\n    nsamples = dist.shape[1]\n    batch_ot_samples = torch.zeros_like(dist)\n\n    #\u00a0Note that we might loose some data points here (the remainder)\n    bs = int(nsamples / nb)\n    ind = np.arange(nsamples)\n    for k in range(nb):\n        np.random.shuffle(ind)\n        chunk = ind[:bs]\n        chunk_dist = dist[:, chunk, :]\n        pi_list = compute_pairwise_ot_plans(chunk_dist, method=\"exact\")\n        trajectories = sample_trajectory(pi_list, num_samples=len(chunk))\n        chunk_paths = get_sample_paths(chunk_dist, trajectories, len(chunk))\n        batch_ot_samples[:, k * bs:(k + 1) * bs, :] = chunk_paths\n\n    return dist, batch_ot_samples.to(device)\n</code></pre>"},{"location":"api/pfi/flow/interpolants/#pfi.flow.interpolants.select_best_lambda","title":"<code>select_best_lambda(batch_ot_samples, data_batch, eval_batch, device, lam_vals=None, rel_tol=0.8, verbose=True)</code>","text":"<p>Select regularization strength from velocity-magnitude reduction.</p> <p>Parameters:</p> <ul> <li> <code>batch_ot_samples</code>               (<code>torch.Tensor of shape (batch_size, n_nodes, ndim)</code>)           \u2013            <p>Training trajectories.</p> </li> <li> <code>data_batch</code>               (<code>torch.Tensor of shape (batch_size, n_nodes)</code>)           \u2013            <p>Fit nodes.</p> </li> <li> <code>eval_batch</code>               (<code>torch.Tensor of shape (batch_size, n_eval)</code>)           \u2013            <p>Evaluation nodes.</p> </li> <li> <code>device</code>               (<code>str or device</code>)           \u2013            <p>Computation device.</p> </li> <li> <code>lam_vals</code>               (<code>array-like of shape (n_lambdas,)</code>, default:                   <code>None</code> )           \u2013            <p>Candidate regularization values.</p> </li> <li> <code>rel_tol</code>               (<code>float</code>, default:                   <code>0.8</code> )           \u2013            <p>Minimum relative reduction threshold.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, print diagnostics.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>best_lambda</code> (              <code>float</code> )          \u2013            <p>Selected regularization value.</p> </li> <li> <code>lam_arr</code> (              <code>ndarray of shape (n_lambdas,)</code> )          \u2013            <p>Candidate values evaluated.</p> </li> <li> <code>vel_mag</code> (              <code>ndarray of shape (n_lambdas,)</code> )          \u2013            <p>Mean squared derivative magnitudes for each candidate.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_chebyshev.py</code> <pre><code>def select_best_lambda(\n    batch_ot_samples,\n    data_batch,\n    eval_batch,\n    device,\n    lam_vals=None,\n    rel_tol=0.8,\n    verbose=True,\n):\n    \"\"\"Select regularization strength from velocity-magnitude reduction.\n\n    Parameters\n    ----------\n    batch_ot_samples : torch.Tensor of shape (batch_size, n_nodes, ndim)\n        Training trajectories.\n    data_batch : torch.Tensor of shape (batch_size, n_nodes)\n        Fit nodes.\n    eval_batch : torch.Tensor of shape (batch_size, n_eval)\n        Evaluation nodes.\n    device : str or torch.device\n        Computation device.\n    lam_vals : array-like of shape (n_lambdas,), default=None\n        Candidate regularization values.\n    rel_tol : float, default=0.8\n        Minimum relative reduction threshold.\n    verbose : bool, default=True\n        If ``True``, print diagnostics.\n\n    Returns\n    -------\n    best_lambda : float\n        Selected regularization value.\n    lam_arr : ndarray of shape (n_lambdas,)\n        Candidate values evaluated.\n    vel_mag : ndarray of shape (n_lambdas,)\n        Mean squared derivative magnitudes for each candidate.\n    \"\"\"\n    if lam_vals is None:\n        lam_arr = np.array([0.001, 0.0025, 0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.2, 0.3, 0.4, 0.5, 0.8, 1.0])\n    else:\n        lam_arr = np.array(lam_vals)\n\n    vel_mag = np.zeros_like(lam_arr)\n\n    for k in range(len(lam_arr)):\n        interpolant = ChebyshevInterpolant(\n            reg_=lam_arr[k],\n            device=device,\n        )\n        interpolant.fit(data_batch, batch_ot_samples)\n        _, dx_interp = interpolant.predict(eval_batch)\n        vel_mag[k] = torch.mean(dx_interp ** 2).cpu().item()\n\n    err0 = vel_mag[0]\n    rel_err_drop = (err0 - vel_mag) / err0\n    mask = rel_err_drop &gt;= rel_tol\n\n    if np.any(mask):\n        idx = np.argmax(mask)\n        best_lambda = lam_arr[idx]\n    else:\n        best_lambda = 0.01\n\n    if verbose:\n        print(f\"[lambda-selection] Initial error: {err0:.4f}\")\n        print(f\"[lambda-selection] Best lambda (&gt;={rel_tol*100:.0f}% drop): {best_lambda:.4f}\")\n        print(f\"[lambda-selection] Vel magnitudes: {vel_mag}\")\n\n    return best_lambda, lam_arr, vel_mag\n</code></pre>"},{"location":"api/pfi/flow/interpolants/_base/","title":"_base","text":""},{"location":"api/pfi/flow/interpolants/_base/#pfi.flow.interpolants._base","title":"<code>pfi.flow.interpolants._base</code>","text":"<p>Base interpolant interface used by flow-matching solvers.</p>"},{"location":"api/pfi/flow/interpolants/_base/#pfi.flow.interpolants._base.BaseInterpolant","title":"<code>BaseInterpolant(device='cpu')</code>","text":"<p>Abstract base class for trajectory interpolants.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>str or device</code>, default:                   <code>'cpu'</code> )           \u2013            <p>Device used by derived interpolants.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_base.py</code> <pre><code>def __init__(\n    self,\n    device=\"cpu\",\n):\n    self.device = device\n</code></pre>"},{"location":"api/pfi/flow/interpolants/_base/#pfi.flow.interpolants._base.BaseInterpolant.fit","title":"<code>fit(nodes_fit, Dist)</code>","text":"<p>Store fit data and call implementation-specific fitting.</p> <p>Parameters:</p> <ul> <li> <code>nodes_fit</code>               (<code>torch.Tensor of shape (batch_size, n_nodes)</code>)           \u2013            <p>Time nodes used for fitting.</p> </li> <li> <code>Dist</code>               (<code>torch.Tensor of shape (batch_size, n_nodes, ndim)</code>)           \u2013            <p>Trajectory values at <code>nodes_fit</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>self</code> (              <code>BaseInterpolant</code> )          \u2013            <p>Fitted interpolant.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_base.py</code> <pre><code>def fit(\n    self,\n    nodes_fit,\n    Dist,\n):\n    \"\"\"Store fit data and call implementation-specific fitting.\n\n    Parameters\n    ----------\n    nodes_fit : torch.Tensor of shape (batch_size, n_nodes)\n        Time nodes used for fitting.\n    Dist : torch.Tensor of shape (batch_size, n_nodes, ndim)\n        Trajectory values at ``nodes_fit``.\n\n    Returns\n    -------\n    self : BaseInterpolant\n        Fitted interpolant.\n    \"\"\"\n    self.y_fit_ = Dist\n    self.t_fit_ = nodes_fit\n\n    self._fit()\n\n    return self\n</code></pre>"},{"location":"api/pfi/flow/interpolants/_base/#pfi.flow.interpolants._base.BaseInterpolant.predict","title":"<code>predict(t_eval)</code>","text":"<p>Evaluate interpolant and derivative at requested nodes.</p> <p>Parameters:</p> <ul> <li> <code>t_eval</code>               (<code>torch.Tensor of shape (batch_size, n_eval)</code>)           \u2013            <p>Evaluation nodes.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>x_interp</code> (              <code>torch.Tensor of shape (batch_size, n_eval, ndim)</code> )          \u2013            <p>Interpolated trajectories.</p> </li> <li> <code>dx_interp</code> (              <code>torch.Tensor of shape (batch_size, n_eval, ndim)</code> )          \u2013            <p>Time derivatives at evaluation nodes.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_base.py</code> <pre><code>def predict(self, t_eval):\n    \"\"\"Evaluate interpolant and derivative at requested nodes.\n\n    Parameters\n    ----------\n    t_eval : torch.Tensor of shape (batch_size, n_eval)\n        Evaluation nodes.\n\n    Returns\n    -------\n    x_interp : torch.Tensor of shape (batch_size, n_eval, ndim)\n        Interpolated trajectories.\n    dx_interp : torch.Tensor of shape (batch_size, n_eval, ndim)\n        Time derivatives at evaluation nodes.\n    \"\"\"\n    x_interp, dx_interp = self._predict(t_eval)\n    return x_interp, dx_interp\n</code></pre>"},{"location":"api/pfi/flow/interpolants/_chebyshev/","title":"_chebyshev","text":""},{"location":"api/pfi/flow/interpolants/_chebyshev/#pfi.flow.interpolants._chebyshev","title":"<code>pfi.flow.interpolants._chebyshev</code>","text":"<p>Chebyshev polynomial interpolants and regularization selection.</p>"},{"location":"api/pfi/flow/interpolants/_chebyshev/#pfi.flow.interpolants._chebyshev.ChebyshevInterpolant","title":"<code>ChebyshevInterpolant(reg_=0.01, device='cpu')</code>","text":"<p>               Bases: <code>BaseInterpolant</code></p> <p>Regularized Chebyshev interpolant for batched trajectories.</p> <p>Parameters:</p> <ul> <li> <code>reg_</code>               (<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>Curvature regularization weight.</p> </li> <li> <code>device</code>               (<code>str or device</code>, default:                   <code>'cpu'</code> )           \u2013            <p>Computation device.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_chebyshev.py</code> <pre><code>def __init__(\n    self,\n    reg_=0.01,\n    device=\"cpu\",\n):\n    super().__init__(\n        device=device,\n    )\n    self.reg_ = reg_\n</code></pre>"},{"location":"api/pfi/flow/interpolants/_chebyshev/#pfi.flow.interpolants._chebyshev.ChebyshevInterpolant.fit","title":"<code>fit(nodes_fit, Dist)</code>","text":"<p>Store fit data and call implementation-specific fitting.</p> <p>Parameters:</p> <ul> <li> <code>nodes_fit</code>               (<code>torch.Tensor of shape (batch_size, n_nodes)</code>)           \u2013            <p>Time nodes used for fitting.</p> </li> <li> <code>Dist</code>               (<code>torch.Tensor of shape (batch_size, n_nodes, ndim)</code>)           \u2013            <p>Trajectory values at <code>nodes_fit</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>self</code> (              <code>BaseInterpolant</code> )          \u2013            <p>Fitted interpolant.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_base.py</code> <pre><code>def fit(\n    self,\n    nodes_fit,\n    Dist,\n):\n    \"\"\"Store fit data and call implementation-specific fitting.\n\n    Parameters\n    ----------\n    nodes_fit : torch.Tensor of shape (batch_size, n_nodes)\n        Time nodes used for fitting.\n    Dist : torch.Tensor of shape (batch_size, n_nodes, ndim)\n        Trajectory values at ``nodes_fit``.\n\n    Returns\n    -------\n    self : BaseInterpolant\n        Fitted interpolant.\n    \"\"\"\n    self.y_fit_ = Dist\n    self.t_fit_ = nodes_fit\n\n    self._fit()\n\n    return self\n</code></pre>"},{"location":"api/pfi/flow/interpolants/_chebyshev/#pfi.flow.interpolants._chebyshev.ChebyshevInterpolant.predict","title":"<code>predict(t_eval)</code>","text":"<p>Evaluate interpolated trajectories and derivatives.</p> <p>Parameters:</p> <ul> <li> <code>t_eval</code>               (<code>torch.Tensor of shape (batch_size, n_eval)</code>)           \u2013            <p>Evaluation nodes.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>x_interp</code> (              <code>torch.Tensor of shape (batch_size, n_eval, ndim)</code> )          \u2013            <p>Interpolated trajectories.</p> </li> <li> <code>dx_interp</code> (              <code>torch.Tensor of shape (batch_size, n_eval, ndim)</code> )          \u2013            <p>Time derivatives.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_chebyshev.py</code> <pre><code>def predict(\n    self,\n    t_eval,\n):\n    \"\"\"Evaluate interpolated trajectories and derivatives.\n\n    Parameters\n    ----------\n    t_eval : torch.Tensor of shape (batch_size, n_eval)\n        Evaluation nodes.\n\n    Returns\n    -------\n    x_interp : torch.Tensor of shape (batch_size, n_eval, ndim)\n        Interpolated trajectories.\n    dx_interp : torch.Tensor of shape (batch_size, n_eval, ndim)\n        Time derivatives.\n    \"\"\"\n    x_interp = self.p_(t_eval)\n    dx_interp = self.p_prime_(t_eval)\n    return x_interp, dx_interp\n</code></pre>"},{"location":"api/pfi/flow/interpolants/_chebyshev/#pfi.flow.interpolants._chebyshev.batched_chebyshev_interpolate","title":"<code>batched_chebyshev_interpolate(t_points, x_points, degree=None, lambda_reg=0.0, penalty='none')</code>","text":"<p>Fit batched Chebyshev polynomials and return evaluation callables.</p> <p>Parameters:</p> <ul> <li> <code>t_points</code>               (<code>torch.Tensor of shape (batch_size, n_points)</code>)           \u2013            <p>Time nodes for fitting.</p> </li> <li> <code>x_points</code>               (<code>torch.Tensor of shape (batch_size, n_points, ndim)</code>)           \u2013            <p>Trajectory values at <code>t_points</code>.</p> </li> <li> <code>degree</code>               (<code>int or None</code>, default:                   <code>None</code> )           \u2013            <p>Polynomial degree. If <code>None</code>, uses <code>n_points - 1</code>.</p> </li> <li> <code>lambda_reg</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Regularization weight.</p> </li> <li> <code>penalty</code>               (<code>(none, l2, velocity, curvature)</code>, default:                   <code>'none'</code> )           \u2013            <p>Regularization profile on coefficients.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>coeffs</code> (              <code>torch.Tensor of shape (batch_size, degree + 1, ndim)</code> )          \u2013            <p>Fitted Chebyshev coefficients.</p> </li> <li> <code>interpolant</code> (              <code>callable</code> )          \u2013            <p>Function mapping <code>t_eval</code> of shape <code>(batch_size, n_eval)</code> to interpolated values of shape <code>(batch_size, n_eval, ndim)</code>.</p> </li> <li> <code>derivative</code> (              <code>callable</code> )          \u2013            <p>Function mapping <code>t_eval</code> of shape <code>(batch_size, n_eval)</code> to derivatives of shape <code>(batch_size, n_eval, ndim)</code>.</p> </li> <li> <code>bounds</code> (              <code>tuple of torch.Tensor</code> )          \u2013            <p>Tuple <code>(a, bmax)</code> each of shape <code>(batch_size, 1)</code> used for scaling.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_chebyshev.py</code> <pre><code>def batched_chebyshev_interpolate(\n    t_points,\n    x_points,\n    degree=None,\n    lambda_reg=0.0,\n    penalty=\"none\",\n):\n    \"\"\"Fit batched Chebyshev polynomials and return evaluation callables.\n\n    Parameters\n    ----------\n    t_points : torch.Tensor of shape (batch_size, n_points)\n        Time nodes for fitting.\n    x_points : torch.Tensor of shape (batch_size, n_points, ndim)\n        Trajectory values at ``t_points``.\n    degree : int or None, default=None\n        Polynomial degree. If ``None``, uses ``n_points - 1``.\n    lambda_reg : float, default=0.0\n        Regularization weight.\n    penalty : {'none', 'l2', 'velocity', 'curvature'}, default='none'\n        Regularization profile on coefficients.\n\n    Returns\n    -------\n    coeffs : torch.Tensor of shape (batch_size, degree + 1, ndim)\n        Fitted Chebyshev coefficients.\n    interpolant : callable\n        Function mapping ``t_eval`` of shape ``(batch_size, n_eval)`` to\n        interpolated values of shape ``(batch_size, n_eval, ndim)``.\n    derivative : callable\n        Function mapping ``t_eval`` of shape ``(batch_size, n_eval)`` to\n        derivatives of shape ``(batch_size, n_eval, ndim)``.\n    bounds : tuple of torch.Tensor\n        Tuple ``(a, bmax)`` each of shape ``(batch_size, 1)`` used for scaling.\n    \"\"\"\n    t_points = t_points.to(dtype=torch.float32)\n    x_points = x_points.to(dtype=torch.float32)\n    device = t_points.device\n    lambda_reg = float(lambda_reg)\n\n    bsz, n, _ = x_points.shape\n\n    if degree is None:\n        degree = n - 1\n\n    a = t_points.min(dim=1, keepdim=True).values\n    bmax = t_points.max(dim=1, keepdim=True).values\n    s_points = (2 * t_points - (a + bmax)) / (bmax - a)\n\n    v = chebyshev_basis_matrix(s_points, degree)\n    vt = v.transpose(1, 2)\n\n    powers = torch.arange(degree + 1, dtype=torch.float32, device=device)\n    if penalty == \"none\" or lambda_reg == 0.0:\n        r = torch.zeros_like(powers)\n    elif penalty == \"l2\":\n        r = torch.ones_like(powers)\n        r[0] = 0\n    elif penalty == \"velocity\":\n        r = powers ** 2\n        r[0] = 0\n    elif penalty == \"curvature\":\n        r = powers ** 4\n        r[0] = 0\n    else:\n        raise ValueError(\"Invalid penalty type\")\n\n    r_mat = torch.diag(r).unsqueeze(0).expand(bsz, -1, -1).contiguous()\n    lhs = vt @ v + lambda_reg * r_mat\n    rhs = vt @ x_points\n    coeffs = torch.linalg.solve(lhs, rhs).float()\n\n    def interpolant(t_eval):\n        t_eval = t_eval.to(dtype=torch.float32, device=device)\n        s_eval = (2 * t_eval - (a + bmax)) / (bmax - a)\n        basis = chebyshev_basis_matrix(s_eval, degree)\n        return torch.einsum(\"bnd,bdc-&gt;bnc\", basis, coeffs)\n\n    def derivative(t_eval):\n        t_eval = t_eval.to(dtype=torch.float32, device=device)\n        s_eval = (2 * t_eval - (a + bmax)) / (bmax - a)\n        dsdt = (2 / (bmax - a)).float().unsqueeze(-1)\n        u_basis = chebyshev_U_basis_matrix(s_eval, degree)\n\n        deriv = coeffs[:, 1:2, :].expand(-1, t_eval.shape[1], -1).clone()\n        for k in range(2, degree + 1):\n            uk = u_basis[:, :, k - 1:k]\n            coeff_k = coeffs[:, k:k + 1, :]\n            deriv += k * uk * coeff_k\n\n        return dsdt * deriv\n\n    return coeffs, interpolant, derivative, (a, bmax)\n</code></pre>"},{"location":"api/pfi/flow/interpolants/_chebyshev/#pfi.flow.interpolants._chebyshev.chebyshev_U_basis_matrix","title":"<code>chebyshev_U_basis_matrix(s, degree)</code>","text":"<p>Compute second-kind Chebyshev basis values.</p> <p>Parameters:</p> <ul> <li> <code>s</code>               (<code>torch.Tensor of shape (batch_size, n_points)</code>)           \u2013            <p>Scaled nodes in <code>[-1, 1]</code>.</p> </li> <li> <code>degree</code>               (<code>int</code>)           \u2013            <p>Maximum polynomial degree.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>basis</code> (              <code>torch.Tensor of shape (batch_size, n_points, degree)</code> )          \u2013            <p>Basis matrix <code>[U_0(s), ..., U_{degree-1}(s)]</code>.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_chebyshev.py</code> <pre><code>def chebyshev_U_basis_matrix(\n    s,\n    degree,\n):\n    \"\"\"Compute second-kind Chebyshev basis values.\n\n    Parameters\n    ----------\n    s : torch.Tensor of shape (batch_size, n_points)\n        Scaled nodes in ``[-1, 1]``.\n    degree : int\n        Maximum polynomial degree.\n\n    Returns\n    -------\n    basis : torch.Tensor of shape (batch_size, n_points, degree)\n        Basis matrix ``[U_0(s), ..., U_{degree-1}(s)]``.\n    \"\"\"\n    s = s.to(dtype=torch.float32)\n    device = s.device\n\n    u = [torch.ones_like(s, dtype=torch.float32, device=device)]\n    if degree &gt;= 1:\n        u.append(2 * s)\n\n    for k in range(2, degree):\n        u_next = 2 * s * u[-1] - u[-2]\n        u.append(u_next)\n\n    return torch.stack(u, dim=-1)\n</code></pre>"},{"location":"api/pfi/flow/interpolants/_chebyshev/#pfi.flow.interpolants._chebyshev.chebyshev_basis_matrix","title":"<code>chebyshev_basis_matrix(s, degree)</code>","text":"<p>Compute first-kind Chebyshev basis values.</p> <p>Parameters:</p> <ul> <li> <code>s</code>               (<code>torch.Tensor of shape (batch_size, n_points)</code>)           \u2013            <p>Scaled nodes in <code>[-1, 1]</code>.</p> </li> <li> <code>degree</code>               (<code>int</code>)           \u2013            <p>Maximum polynomial degree.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>basis</code> (              <code>torch.Tensor of shape (batch_size, n_points, degree + 1)</code> )          \u2013            <p>Basis matrix <code>[T_0(s), ..., T_degree(s)]</code>.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_chebyshev.py</code> <pre><code>def chebyshev_basis_matrix(\n    s,\n    degree,\n):\n    \"\"\"Compute first-kind Chebyshev basis values.\n\n    Parameters\n    ----------\n    s : torch.Tensor of shape (batch_size, n_points)\n        Scaled nodes in ``[-1, 1]``.\n    degree : int\n        Maximum polynomial degree.\n\n    Returns\n    -------\n    basis : torch.Tensor of shape (batch_size, n_points, degree + 1)\n        Basis matrix ``[T_0(s), ..., T_degree(s)]``.\n    \"\"\"\n    s = s.to(dtype=torch.float32)\n    device = s.device\n\n    v = [torch.ones_like(s, dtype=torch.float32, device=device), s]\n    for n in range(2, degree + 1):\n        tn = 2 * s * v[-1] - v[-2]\n        v.append(tn)\n\n    return torch.stack(v, dim=-1)\n</code></pre>"},{"location":"api/pfi/flow/interpolants/_chebyshev/#pfi.flow.interpolants._chebyshev.select_best_lambda","title":"<code>select_best_lambda(batch_ot_samples, data_batch, eval_batch, device, lam_vals=None, rel_tol=0.8, verbose=True)</code>","text":"<p>Select regularization strength from velocity-magnitude reduction.</p> <p>Parameters:</p> <ul> <li> <code>batch_ot_samples</code>               (<code>torch.Tensor of shape (batch_size, n_nodes, ndim)</code>)           \u2013            <p>Training trajectories.</p> </li> <li> <code>data_batch</code>               (<code>torch.Tensor of shape (batch_size, n_nodes)</code>)           \u2013            <p>Fit nodes.</p> </li> <li> <code>eval_batch</code>               (<code>torch.Tensor of shape (batch_size, n_eval)</code>)           \u2013            <p>Evaluation nodes.</p> </li> <li> <code>device</code>               (<code>str or device</code>)           \u2013            <p>Computation device.</p> </li> <li> <code>lam_vals</code>               (<code>array-like of shape (n_lambdas,)</code>, default:                   <code>None</code> )           \u2013            <p>Candidate regularization values.</p> </li> <li> <code>rel_tol</code>               (<code>float</code>, default:                   <code>0.8</code> )           \u2013            <p>Minimum relative reduction threshold.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, print diagnostics.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>best_lambda</code> (              <code>float</code> )          \u2013            <p>Selected regularization value.</p> </li> <li> <code>lam_arr</code> (              <code>ndarray of shape (n_lambdas,)</code> )          \u2013            <p>Candidate values evaluated.</p> </li> <li> <code>vel_mag</code> (              <code>ndarray of shape (n_lambdas,)</code> )          \u2013            <p>Mean squared derivative magnitudes for each candidate.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_chebyshev.py</code> <pre><code>def select_best_lambda(\n    batch_ot_samples,\n    data_batch,\n    eval_batch,\n    device,\n    lam_vals=None,\n    rel_tol=0.8,\n    verbose=True,\n):\n    \"\"\"Select regularization strength from velocity-magnitude reduction.\n\n    Parameters\n    ----------\n    batch_ot_samples : torch.Tensor of shape (batch_size, n_nodes, ndim)\n        Training trajectories.\n    data_batch : torch.Tensor of shape (batch_size, n_nodes)\n        Fit nodes.\n    eval_batch : torch.Tensor of shape (batch_size, n_eval)\n        Evaluation nodes.\n    device : str or torch.device\n        Computation device.\n    lam_vals : array-like of shape (n_lambdas,), default=None\n        Candidate regularization values.\n    rel_tol : float, default=0.8\n        Minimum relative reduction threshold.\n    verbose : bool, default=True\n        If ``True``, print diagnostics.\n\n    Returns\n    -------\n    best_lambda : float\n        Selected regularization value.\n    lam_arr : ndarray of shape (n_lambdas,)\n        Candidate values evaluated.\n    vel_mag : ndarray of shape (n_lambdas,)\n        Mean squared derivative magnitudes for each candidate.\n    \"\"\"\n    if lam_vals is None:\n        lam_arr = np.array([0.001, 0.0025, 0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.2, 0.3, 0.4, 0.5, 0.8, 1.0])\n    else:\n        lam_arr = np.array(lam_vals)\n\n    vel_mag = np.zeros_like(lam_arr)\n\n    for k in range(len(lam_arr)):\n        interpolant = ChebyshevInterpolant(\n            reg_=lam_arr[k],\n            device=device,\n        )\n        interpolant.fit(data_batch, batch_ot_samples)\n        _, dx_interp = interpolant.predict(eval_batch)\n        vel_mag[k] = torch.mean(dx_interp ** 2).cpu().item()\n\n    err0 = vel_mag[0]\n    rel_err_drop = (err0 - vel_mag) / err0\n    mask = rel_err_drop &gt;= rel_tol\n\n    if np.any(mask):\n        idx = np.argmax(mask)\n        best_lambda = lam_arr[idx]\n    else:\n        best_lambda = 0.01\n\n    if verbose:\n        print(f\"[lambda-selection] Initial error: {err0:.4f}\")\n        print(f\"[lambda-selection] Best lambda (&gt;={rel_tol*100:.0f}% drop): {best_lambda:.4f}\")\n        print(f\"[lambda-selection] Vel magnitudes: {vel_mag}\")\n\n    return best_lambda, lam_arr, vel_mag\n</code></pre>"},{"location":"api/pfi/flow/interpolants/_couplings/","title":"_couplings","text":""},{"location":"api/pfi/flow/interpolants/_couplings/#pfi.flow.interpolants._couplings","title":"<code>pfi.flow.interpolants._couplings</code>","text":"<p>Optimal-transport coupling and trajectory sampling utilities.</p>"},{"location":"api/pfi/flow/interpolants/_couplings/#pfi.flow.interpolants._couplings.MMOT_trajectories","title":"<code>MMOT_trajectories(dist, nb=1, device='cpu')</code>","text":"<p>Construct multi-marginal OT trajectories from snapshot lists.</p> <p>Parameters:</p> <ul> <li> <code>dist</code>               (<code>list of array-like, length nsnaps</code>)           \u2013            <p><code>dist[k]</code> has shape <code>(n_k, ndim)</code>.</p> </li> <li> <code>nb</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of mini-batches used for OT stitching.</p> </li> <li> <code>device</code>               (<code>str or device</code>, default:                   <code>'cpu'</code> )           \u2013            <p>Target device for returned trajectories.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dist</code> (              <code>torch.Tensor of shape (nsnaps, nsamples, ndim)</code> )          \u2013            <p>Subsampled and stacked snapshots.</p> </li> <li> <code>batch_ot_samples</code> (              <code>torch.Tensor of shape (nsnaps, nsamples, ndim)</code> )          \u2013            <p>OT-coupled trajectories across snapshots.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_couplings.py</code> <pre><code>def MMOT_trajectories(\n    dist,\n    nb=1,\n    device=\"cpu\",\n):\n    \"\"\"Construct multi-marginal OT trajectories from snapshot lists.\n\n    Parameters\n    ----------\n    dist : list of array-like, length nsnaps\n        ``dist[k]`` has shape ``(n_k, ndim)``.\n    nb : int, default=1\n        Number of mini-batches used for OT stitching.\n    device : str or torch.device, default='cpu'\n        Target device for returned trajectories.\n\n    Returns\n    -------\n    dist : torch.Tensor of shape (nsnaps, nsamples, ndim)\n        Subsampled and stacked snapshots.\n    batch_ot_samples : torch.Tensor of shape (nsnaps, nsamples, ndim)\n        OT-coupled trajectories across snapshots.\n    \"\"\"\n    dist = torch.stack(subsample_shuffle(dist), dim=0)\n    nsamples = dist.shape[1]\n    batch_ot_samples = torch.zeros_like(dist)\n\n    #\u00a0Note that we might loose some data points here (the remainder)\n    bs = int(nsamples / nb)\n    ind = np.arange(nsamples)\n    for k in range(nb):\n        np.random.shuffle(ind)\n        chunk = ind[:bs]\n        chunk_dist = dist[:, chunk, :]\n        pi_list = compute_pairwise_ot_plans(chunk_dist, method=\"exact\")\n        trajectories = sample_trajectory(pi_list, num_samples=len(chunk))\n        chunk_paths = get_sample_paths(chunk_dist, trajectories, len(chunk))\n        batch_ot_samples[:, k * bs:(k + 1) * bs, :] = chunk_paths\n\n    return dist, batch_ot_samples.to(device)\n</code></pre>"},{"location":"api/pfi/flow/interpolants/_couplings/#pfi.flow.interpolants._couplings.compute_pairwise_ot_plans","title":"<code>compute_pairwise_ot_plans(samples, method='exact', epsilon=0.01)</code>","text":"<p>Compute pairwise transport plans between consecutive snapshots.</p> <p>Parameters:</p> <ul> <li> <code>samples</code>               (<code>torch.Tensor of shape (k_plus_1, b, ndim)</code>)           \u2013            <p>Snapshot samples with equal sample count <code>b</code>.</p> </li> <li> <code>method</code>               (<code>(exact, sinkhorn)</code>, default:                   <code>'exact'</code> )           \u2013            <p>OT solver used for each consecutive pair.</p> </li> <li> <code>epsilon</code>               (<code>float</code>, default:                   <code>1e-2</code> )           \u2013            <p>Entropic regularization when <code>method='sinkhorn'</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>pi_list</code> (              <code>list of length k_plus_1 - 1</code> )          \u2013            <p>Each entry is an ndarray of shape <code>(b, b)</code> containing an OT plan.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_couplings.py</code> <pre><code>def compute_pairwise_ot_plans(\n    samples,\n    method=\"exact\",\n    epsilon=1e-2,\n):\n    \"\"\"Compute pairwise transport plans between consecutive snapshots.\n\n    Parameters\n    ----------\n    samples : torch.Tensor of shape (k_plus_1, b, ndim)\n        Snapshot samples with equal sample count ``b``.\n    method : {'exact', 'sinkhorn'}, default='exact'\n        OT solver used for each consecutive pair.\n    epsilon : float, default=1e-2\n        Entropic regularization when ``method='sinkhorn'``.\n\n    Returns\n    -------\n    pi_list : list of length k_plus_1 - 1\n        Each entry is an ndarray of shape ``(b, b)`` containing an OT plan.\n    \"\"\"\n    k_plus_1, b, _ = samples.shape\n    pi_list = []\n    uniform_weights = torch.ones(b) / b\n\n    for k in range(k_plus_1 - 1):\n        x = samples[k]\n        y = samples[k + 1]\n        c = ot.dist(x, y, metric=\"sqeuclidean\")\n\n        if method == \"sinkhorn\":\n            pi = ot.sinkhorn(uniform_weights, uniform_weights, c, reg=epsilon)\n        elif method == \"exact\":\n            pi = ot.emd(uniform_weights, uniform_weights, c)\n        else:\n            raise ValueError(\"Unknown OT method\")\n\n        pi_list.append(pi)\n\n    return pi_list\n</code></pre>"},{"location":"api/pfi/flow/interpolants/_couplings/#pfi.flow.interpolants._couplings.get_sample_paths","title":"<code>get_sample_paths(samples, trajectories, num_paths)</code>","text":"<p>Gather full sample paths from trajectory indices.</p> <p>Parameters:</p> <ul> <li> <code>samples</code>               (<code>torch.Tensor of shape (k_plus_1, b, ndim)</code>)           \u2013            <p>Snapshot samples.</p> </li> <li> <code>trajectories</code>               (<code>ndarray of shape (num_paths, k_plus_1)</code>)           \u2013            <p>Index trajectories.</p> </li> <li> <code>num_paths</code>               (<code>int</code>)           \u2013            <p>Number of paths to extract.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>sample_paths</code> (              <code>torch.Tensor of shape (k_plus_1, num_paths, ndim)</code> )          \u2013            <p>Reconstructed paths.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_couplings.py</code> <pre><code>def get_sample_paths(\n    samples,\n    trajectories,\n    num_paths,\n):\n    \"\"\"Gather full sample paths from trajectory indices.\n\n    Parameters\n    ----------\n    samples : torch.Tensor of shape (k_plus_1, b, ndim)\n        Snapshot samples.\n    trajectories : ndarray of shape (num_paths, k_plus_1)\n        Index trajectories.\n    num_paths : int\n        Number of paths to extract.\n\n    Returns\n    -------\n    sample_paths : torch.Tensor of shape (k_plus_1, num_paths, ndim)\n        Reconstructed paths.\n    \"\"\"\n    _, k_plus_1 = trajectories.shape\n    d = samples.shape[-1]\n    sample_paths = torch.zeros((k_plus_1, num_paths, d), dtype=torch.float32)\n\n    for t in range(k_plus_1):\n        sample_paths[t, :, :] = samples[t][trajectories[:, t]]\n\n    return sample_paths\n</code></pre>"},{"location":"api/pfi/flow/interpolants/_couplings/#pfi.flow.interpolants._couplings.sample_trajectory","title":"<code>sample_trajectory(pi_list, num_samples=1)</code>","text":"<p>Sample discrete index trajectories from chained OT plans.</p> <p>Parameters:</p> <ul> <li> <code>pi_list</code>               (<code>list of length k</code>)           \u2013            <p>Pairwise OT plans. Each plan has shape <code>(b, b)</code>.</p> </li> <li> <code>num_samples</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of trajectories to sample.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>trajectories</code> (              <code>ndarray of shape (num_samples, k + 1)</code> )          \u2013            <p>Sampled index trajectories through snapshots.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_couplings.py</code> <pre><code>def sample_trajectory(\n    pi_list,\n    num_samples=1,\n):\n    \"\"\"Sample discrete index trajectories from chained OT plans.\n\n    Parameters\n    ----------\n    pi_list : list of length k\n        Pairwise OT plans. Each plan has shape ``(b, b)``.\n    num_samples : int, default=1\n        Number of trajectories to sample.\n\n    Returns\n    -------\n    trajectories : ndarray of shape (num_samples, k + 1)\n        Sampled index trajectories through snapshots.\n    \"\"\"\n    k = len(pi_list) + 1\n    b = pi_list[0].shape[0]\n    trajectories = np.zeros((num_samples, k), dtype=int)\n\n    for n in range(num_samples):\n        i_k = np.random.choice(b)\n        trajectories[n, 0] = i_k\n        for t in range(k - 1):\n            probs = pi_list[t][i_k]\n            probs = probs / probs.sum()\n            i_kp1 = np.random.choice(b, p=probs)\n            trajectories[n, t + 1] = i_kp1\n            i_k = i_kp1\n\n    return trajectories\n</code></pre>"},{"location":"api/pfi/flow/interpolants/_linear/","title":"_linear","text":""},{"location":"api/pfi/flow/interpolants/_linear/#pfi.flow.interpolants._linear","title":"<code>pfi.flow.interpolants._linear</code>","text":"<p>Piecewise-linear interpolant for trajectory data.</p>"},{"location":"api/pfi/flow/interpolants/_linear/#pfi.flow.interpolants._linear.LinearInterpolant","title":"<code>LinearInterpolant(device='cpu')</code>","text":"<p>               Bases: <code>BaseInterpolant</code></p> <p>Batched piecewise-linear interpolant.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>str or device</code>, default:                   <code>'cpu'</code> )           \u2013            <p>Computation device inherited from <code>pfi.flow.interpolants.BaseInterpolant</code>.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_base.py</code> <pre><code>def __init__(\n    self,\n    device=\"cpu\",\n):\n    self.device = device\n</code></pre>"},{"location":"api/pfi/flow/interpolants/_linear/#pfi.flow.interpolants._linear.LinearInterpolant.fit","title":"<code>fit(nodes_fit, Dist)</code>","text":"<p>Store fit data and call implementation-specific fitting.</p> <p>Parameters:</p> <ul> <li> <code>nodes_fit</code>               (<code>torch.Tensor of shape (batch_size, n_nodes)</code>)           \u2013            <p>Time nodes used for fitting.</p> </li> <li> <code>Dist</code>               (<code>torch.Tensor of shape (batch_size, n_nodes, ndim)</code>)           \u2013            <p>Trajectory values at <code>nodes_fit</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>self</code> (              <code>BaseInterpolant</code> )          \u2013            <p>Fitted interpolant.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_base.py</code> <pre><code>def fit(\n    self,\n    nodes_fit,\n    Dist,\n):\n    \"\"\"Store fit data and call implementation-specific fitting.\n\n    Parameters\n    ----------\n    nodes_fit : torch.Tensor of shape (batch_size, n_nodes)\n        Time nodes used for fitting.\n    Dist : torch.Tensor of shape (batch_size, n_nodes, ndim)\n        Trajectory values at ``nodes_fit``.\n\n    Returns\n    -------\n    self : BaseInterpolant\n        Fitted interpolant.\n    \"\"\"\n    self.y_fit_ = Dist\n    self.t_fit_ = nodes_fit\n\n    self._fit()\n\n    return self\n</code></pre>"},{"location":"api/pfi/flow/interpolants/_linear/#pfi.flow.interpolants._linear.LinearInterpolant.predict","title":"<code>predict(t_eval)</code>","text":"<p>Evaluate linear interpolation and piecewise-constant derivative.</p> <p>Parameters:</p> <ul> <li> <code>t_eval</code>               (<code>torch.Tensor of shape (batch_size, n_eval)</code>)           \u2013            <p>Evaluation nodes.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>interp</code> (              <code>torch.Tensor of shape (batch_size, n_eval, ndim)</code> )          \u2013            <p>Interpolated trajectories.</p> </li> <li> <code>deriv</code> (              <code>torch.Tensor of shape (batch_size, n_eval, ndim)</code> )          \u2013            <p>Piecewise-constant derivatives.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_linear.py</code> <pre><code>def predict(\n    self,\n    t_eval,\n):\n    \"\"\"Evaluate linear interpolation and piecewise-constant derivative.\n\n    Parameters\n    ----------\n    t_eval : torch.Tensor of shape (batch_size, n_eval)\n        Evaluation nodes.\n\n    Returns\n    -------\n    interp : torch.Tensor of shape (batch_size, n_eval, ndim)\n        Interpolated trajectories.\n    deriv : torch.Tensor of shape (batch_size, n_eval, ndim)\n        Piecewise-constant derivatives.\n    \"\"\"\n    y_fit = self.y_fit_\n    t_fit = self.t_fit_\n\n    B, T1, D = y_fit.shape\n\n    idx = torch.searchsorted(t_fit, t_eval, right=True) - 1\n    idx = idx.clamp(0, T1 - 2)\n\n    t0 = torch.gather(t_fit, 1, idx)\n    t1 = torch.gather(t_fit, 1, idx + 1)\n\n    y0 = torch.gather(y_fit, 1, idx.unsqueeze(-1).expand(-1, -1, D))\n    y1 = torch.gather(y_fit, 1, (idx + 1).unsqueeze(-1).expand(-1, -1, D))\n\n    delta_t = (t1 - t0).unsqueeze(-1)\n    alpha = ((t_eval - t0) / (t1 - t0)).unsqueeze(-1)\n    alpha = torch.where(delta_t != 0, alpha, torch.zeros_like(alpha))\n\n    interp = (1 - alpha) * y0 + alpha * y1\n    deriv = torch.where(delta_t != 0, (y1 - y0) / delta_t, torch.zeros_like(y0))\n\n    return interp, deriv\n</code></pre>"},{"location":"api/pfi/flow/interpolants/_spline/","title":"_spline","text":""},{"location":"api/pfi/flow/interpolants/_spline/#pfi.flow.interpolants._spline","title":"<code>pfi.flow.interpolants._spline</code>","text":"<p>Natural cubic spline interpolant for trajectory data.</p>"},{"location":"api/pfi/flow/interpolants/_spline/#pfi.flow.interpolants._spline.SplineInterpolant","title":"<code>SplineInterpolant(device='cpu')</code>","text":"<p>               Bases: <code>BaseInterpolant</code></p> <p>Batched natural cubic spline interpolant.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>str or device</code>, default:                   <code>'cpu'</code> )           \u2013            <p>Computation device inherited from <code>pfi.flow.interpolants.BaseInterpolant</code>.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_base.py</code> <pre><code>def __init__(\n    self,\n    device=\"cpu\",\n):\n    self.device = device\n</code></pre>"},{"location":"api/pfi/flow/interpolants/_spline/#pfi.flow.interpolants._spline.SplineInterpolant.fit","title":"<code>fit(nodes_fit, Dist)</code>","text":"<p>Store fit data and call implementation-specific fitting.</p> <p>Parameters:</p> <ul> <li> <code>nodes_fit</code>               (<code>torch.Tensor of shape (batch_size, n_nodes)</code>)           \u2013            <p>Time nodes used for fitting.</p> </li> <li> <code>Dist</code>               (<code>torch.Tensor of shape (batch_size, n_nodes, ndim)</code>)           \u2013            <p>Trajectory values at <code>nodes_fit</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>self</code> (              <code>BaseInterpolant</code> )          \u2013            <p>Fitted interpolant.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_base.py</code> <pre><code>def fit(\n    self,\n    nodes_fit,\n    Dist,\n):\n    \"\"\"Store fit data and call implementation-specific fitting.\n\n    Parameters\n    ----------\n    nodes_fit : torch.Tensor of shape (batch_size, n_nodes)\n        Time nodes used for fitting.\n    Dist : torch.Tensor of shape (batch_size, n_nodes, ndim)\n        Trajectory values at ``nodes_fit``.\n\n    Returns\n    -------\n    self : BaseInterpolant\n        Fitted interpolant.\n    \"\"\"\n    self.y_fit_ = Dist\n    self.t_fit_ = nodes_fit\n\n    self._fit()\n\n    return self\n</code></pre>"},{"location":"api/pfi/flow/interpolants/_spline/#pfi.flow.interpolants._spline.SplineInterpolant.predict","title":"<code>predict(t_eval)</code>","text":"<p>Evaluate spline interpolation and derivative.</p> <p>Parameters:</p> <ul> <li> <code>t_eval</code>               (<code>torch.Tensor of shape (batch_size, n_eval) or (n_eval,)</code>)           \u2013            <p>Evaluation nodes.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>eval_</code> (              <code>torch.Tensor of shape (batch_size, n_eval, ndim)</code> )          \u2013            <p>Interpolated trajectories.</p> </li> <li> <code>derv_</code> (              <code>torch.Tensor of shape (batch_size, n_eval, ndim)</code> )          \u2013            <p>Derivative trajectories.</p> </li> </ul> Source code in <code>pfi/flow/interpolants/_spline.py</code> <pre><code>def predict(self, t_eval):\n    \"\"\"Evaluate spline interpolation and derivative.\n\n    Parameters\n    ----------\n    t_eval : torch.Tensor of shape (batch_size, n_eval) or (n_eval,)\n        Evaluation nodes.\n\n    Returns\n    -------\n    eval_ : torch.Tensor of shape (batch_size, n_eval, ndim)\n        Interpolated trajectories.\n    derv_ : torch.Tensor of shape (batch_size, n_eval, ndim)\n        Derivative trajectories.\n    \"\"\"\n    if t_eval.dim() == 2:\n        t_eval_1d = t_eval[0]\n    else:\n        t_eval_1d = t_eval\n\n    eval_ = self.spline_.evaluate(t_eval_1d)\n    derv_ = self.spline_.derivative(t_eval_1d)\n\n    eval_ = torch.permute(eval_, [2, 1, 0, 3]).squeeze()\n    derv_ = torch.permute(derv_, [2, 1, 0, 3]).squeeze()\n\n    eval_ = torch.permute(eval_, (1, 0, 2))\n    derv_ = torch.permute(derv_, (1, 0, 2))\n\n    return eval_, derv_\n</code></pre>"},{"location":"api/pfi/flow/solvers/","title":"solvers","text":""},{"location":"api/pfi/flow/solvers/#pfi.flow.solvers","title":"<code>pfi.flow.solvers</code>","text":""},{"location":"api/pfi/flow/solvers/#pfi.flow.solvers.FM_","title":"<code>FM_(dist, times, interp, net, growth_model=None, fac=1, nb=1, n_epochs=2000, lr=0.001, alpha_ann=0.5, device='cpu', verbose=True)</code>","text":"<p>Train drift and optional growth models by flow matching.</p> <p>Parameters:</p> <ul> <li> <code>dist</code>               (<code>list of torch.Tensor, length nsnaps</code>)           \u2013            <p><code>dist[k]</code> has shape <code>(n_k, ndim)</code>.</p> </li> <li> <code>times</code>               (<code>torch.Tensor of shape (nsnaps,)</code>)           \u2013            <p>Snapshot times.</p> </li> <li> <code>interp</code>               (<code>object</code>)           \u2013            <p>Interpolant implementing <code>fit(nodes, data)</code> and <code>predict(nodes) -&gt; (x_interp, dx_interp)</code>.</p> </li> <li> <code>net</code>               (<code>Module</code>)           \u2013            <p>Drift model mapping <code>(batch_size, ndim + 1)</code> to <code>(batch_size, ndim)</code>.</p> </li> <li> <code>growth_model</code>               (<code>Module or None</code>, default:                   <code>None</code> )           \u2013            <p>Optional growth model mapping <code>(batch_size, ndim + 1)</code> to <code>(batch_size, 1)</code>.</p> </li> <li> <code>fac</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Temporal upsampling factor for the uniform grid.</p> </li> <li> <code>nb</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of mini-batches used for OT stitching.</p> </li> <li> <code>n_epochs</code>               (<code>int</code>, default:                   <code>2000</code> )           \u2013            <p>Number of optimization epochs.</p> </li> <li> <code>lr</code>               (<code>float</code>, default:                   <code>1e-3</code> )           \u2013            <p>Learning rate.</p> </li> <li> <code>alpha_ann</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Exponential averaging factor for adaptive mass-loss weight.</p> </li> <li> <code>device</code>               (<code>str or device</code>, default:                   <code>'cpu'</code> )           \u2013            <p>Training device.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, show progress bars and diagnostics.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>drift_net</code> (              <code>Module</code> )          \u2013            <p>Trained drift model.</p> </li> <li> <code>growth_model</code> (              <code>Module or None</code> )          \u2013            <p>Trained growth model (or <code>None</code> when disabled).</p> </li> <li> <code>loss_hist</code> (              <code>list of float</code> )          \u2013            <p>Epoch-wise total objective values.</p> </li> </ul> Source code in <code>pfi/flow/solvers/_fm.py</code> <pre><code>def FM_(\n    dist,\n    times,\n    interp,\n    net,\n    growth_model=None,\n    fac=1,\n    nb=1,\n    n_epochs=2000,\n    lr=1e-3,\n    alpha_ann=0.5,\n    device=\"cpu\",\n    verbose=True,\n):\n    \"\"\"Train drift and optional growth models by flow matching.\n\n    Parameters\n    ----------\n    dist : list of torch.Tensor, length nsnaps\n        ``dist[k]`` has shape ``(n_k, ndim)``.\n    times : torch.Tensor of shape (nsnaps,)\n        Snapshot times.\n    interp : object\n        Interpolant implementing ``fit(nodes, data)`` and\n        ``predict(nodes) -&gt; (x_interp, dx_interp)``.\n    net : torch.nn.Module\n        Drift model mapping ``(batch_size, ndim + 1)`` to ``(batch_size, ndim)``.\n    growth_model : torch.nn.Module or None, default=None\n        Optional growth model mapping ``(batch_size, ndim + 1)`` to\n        ``(batch_size, 1)``.\n    fac : int, default=1\n        Temporal upsampling factor for the uniform grid.\n    nb : int, default=1\n        Number of mini-batches used for OT stitching.\n    n_epochs : int, default=2000\n        Number of optimization epochs.\n    lr : float, default=1e-3\n        Learning rate.\n    alpha_ann : float, default=0.5\n        Exponential averaging factor for adaptive mass-loss weight.\n    device : str or torch.device, default='cpu'\n        Training device.\n    verbose : bool, default=True\n        If ``True``, show progress bars and diagnostics.\n\n    Returns\n    -------\n    drift_net : torch.nn.Module\n        Trained drift model.\n    growth_model : torch.nn.Module or None\n        Trained growth model (or ``None`` when disabled).\n    loss_hist : list of float\n        Epoch-wise total objective values.\n    \"\"\"\n\n    dist_tensor = torch.stack(subsample_shuffle(dist), dim=0).to(device)\n    x_mean = dist_tensor.mean(dim=(0, 1)).unsqueeze(0)\n    x_std = dist_tensor.std(dim=(0, 1)).unsqueeze(0)\n    net.set_scales(x_mean, x_std)\n\n    dist_ot, batch_ot_samples = MMOT_trajectories(dist, nb=nb, device=device)\n    nsnaps, nsamples, ndim = dist_ot.shape\n\n    mass_vec = np.ones((nsnaps,))\n    for k in range(1, nsnaps):\n        mass_vec[k] = dist[k].shape[0] / dist[0].shape[0]\n    mass_vec = torch.tensor(mass_vec, dtype=torch.float32, device=device)\n\n    uniform_kind = torch.tensor(\n        np.linspace(times[0].item(), times[-1].item(), fac * times.shape[0]),\n        dtype=torch.float32,\n    )\n    data_nodes = times[None, :].repeat(batch_ot_samples.shape[1], 1)\n    uniform_nodes = uniform_kind.to(device).expand(batch_ot_samples.shape[1], -1)\n    batch_ot_samples_uniform = interpolate_old2new(batch_ot_samples, data_nodes, uniform_nodes).to(device)\n\n    batch_size = int(nsamples / 4)\n    print(nsamples, batch_size)\n    uniform_batch = uniform_kind.to(device).expand(batch_size, -1)\n\n    data_full = uniform_kind.to(device).expand(batch_ot_samples_uniform.shape[1], -1)\n\n    if isinstance(interp, ChebyshevInterpolant):\n        batch_ot_samples_uniform_ = torch.permute(batch_ot_samples_uniform[:, :, 0:ndim], (1, 0, 2))\n        best_lam, _, _ = select_best_lambda(\n            batch_ot_samples_uniform_,\n            data_full,\n            data_full,\n            device,\n            verbose=verbose,\n        )\n        interp.reg_ = best_lam\n\n    drift_net = net.to(device)\n\n    if growth_model is None:\n        growth_model = None\n        zero_growth = True\n    else:\n        growth_model = growth_model.to(device)\n        zero_growth = False\n\n    params = [{\"params\": drift_net.parameters(), \"lr\": lr}]\n    if not zero_growth:\n        params.append({\"params\": growth_model.parameters(), \"lr\": lr})\n\n    optimizer = torch.optim.Adam(params)\n    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [1000, 1500, 8000, 15000], gamma=0.1)\n\n    dt = times[1] - times[0]\n    lamb = 1.0\n    loss_hist = []\n\n    pbar = tqdm(range(n_epochs), desc=\"FM\", dynamic_ncols=True, disable=not verbose)\n    for epoch in pbar:\n        optimizer.zero_grad()\n\n        if zero_growth:\n            growth_ = torch.zeros_like(batch_ot_samples_uniform[..., 0])\n        else:\n            growth_ = growth_model(batch_ot_samples_uniform).squeeze(-1)\n\n        log_mass = torch.zeros_like(growth_)\n        log_mass[0] = np.log(1.0 / batch_ot_samples_uniform.shape[1])\n        for k in range(1, fac * nsnaps):\n            log_mass[k] = log_mass[k - 1] + 0.5 * (dt / fac) * (growth_[k] + growth_[k - 1])\n\n        mass_ = torch.exp(log_mass)\n        pred_mass = torch.sum(mass_, axis=1)\n\n        Xtrain, ytrain, weights_ = compute_conditional_distributions(\n            interp,\n            batch_ot_samples_uniform,\n            batch_size,\n            mass_,\n            uniform_batch,\n            uniform_batch,\n            device=device,\n            sigma=0.001,\n        )\n\n        inp_ = Xtrain.clone()\n        inp_.requires_grad_()\n        fold = drift_net(inp_)\n\n        res = (fold - ytrain).pow(2).sum(dim=1).reshape(batch_size, fac * nsnaps)\n        weights_ = weights_ / torch.sum(weights_, axis=0)[None, :]\n        wcfm_obj = torch.mean(weights_ * res, axis=0)\n        mass_balance = (pred_mass[::fac] - mass_vec) ** 2\n\n        l1 = torch.sum(wcfm_obj)\n        l2 = torch.sum(mass_balance)\n\n        if not zero_growth:\n            if epoch % 10 == 0:\n                with torch.no_grad():\n                    std_l1 = loss_grad_std(l1, drift_net, device)\n                    if zero_growth:\n                        std_l2 = torch.tensor(1.0, device=device)\n                    else:\n                        std_l2 = loss_grad_std(l2, growth_model, device)\n                    lamb_hat = std_l1 / std_l2\n                    lamb = (1 - alpha_ann) * lamb + alpha_ann * lamb_hat\n\n        total_loss = l1 + lamb * l2\n        loss_hist.append(total_loss.item())\n\n        total_loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        if verbose:\n            pbar.set_postfix(loss=f\"{total_loss.item():.3e}\", lr=f\"{optimizer.param_groups[0]['lr']:.2e}\")\n            if epoch % 500 == 0:\n                tqdm.write(\n                    f\"[epoch {epoch}] \"\n                    f\"inferred mass={torch.sum(mass_, axis=1)[::fac].detach().cpu().numpy()} \"\n                    f\"actual mass={mass_vec}\"\n                )\n\n    return drift_net, growth_model, loss_hist\n</code></pre>"},{"location":"api/pfi/flow/solvers/#pfi.flow.solvers.interpolate_old2new","title":"<code>interpolate_old2new(Dist, old_nodes, new_nodes)</code>","text":"<p>Interpolate trajectories from one temporal grid to another.</p> <p>Parameters:</p> <ul> <li> <code>Dist</code>               (<code>torch.Tensor of shape (n_old, batch_size, ndim)</code>)           \u2013            <p>Input trajectories at <code>old_nodes</code>.</p> </li> <li> <code>old_nodes</code>               (<code>torch.Tensor of shape (batch_size, n_old)</code>)           \u2013            <p>Original time nodes.</p> </li> <li> <code>new_nodes</code>               (<code>torch.Tensor of shape (batch_size, n_new)</code>)           \u2013            <p>Target time nodes.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>x_interp</code> (              <code>torch.Tensor of shape (n_new, batch_size, ndim)</code> )          \u2013            <p>Interpolated trajectories.</p> </li> </ul> Source code in <code>pfi/flow/solvers/_fm.py</code> <pre><code>def interpolate_old2new(\n    Dist,\n    old_nodes,\n    new_nodes,\n):\n    \"\"\"Interpolate trajectories from one temporal grid to another.\n\n    Parameters\n    ----------\n    Dist : torch.Tensor of shape (n_old, batch_size, ndim)\n        Input trajectories at ``old_nodes``.\n    old_nodes : torch.Tensor of shape (batch_size, n_old)\n        Original time nodes.\n    new_nodes : torch.Tensor of shape (batch_size, n_new)\n        Target time nodes.\n\n    Returns\n    -------\n    x_interp : torch.Tensor of shape (n_new, batch_size, ndim)\n        Interpolated trajectories.\n    \"\"\"\n    _, b, d = Dist.shape\n    x = torch.permute(Dist, (1, 0, 2))\n\n    x_interp = torch.stack(\n        [\n            torch.stack(\n                [\n                    torch.tensor(\n                        np.interp(\n                            new_nodes[i].cpu().numpy(),\n                            old_nodes[i].cpu().numpy(),\n                            x[i, :, j].cpu().numpy(),\n                        ),\n                        dtype=torch.float32,\n                    )\n                    for j in range(d)\n                ],\n                dim=1,\n            )\n            for i in range(b)\n        ],\n        dim=0,\n    )\n\n    return torch.permute(x_interp, (1, 0, 2))\n</code></pre>"},{"location":"api/pfi/flow/solvers/_fm/","title":"_fm","text":""},{"location":"api/pfi/flow/solvers/_fm/#pfi.flow.solvers._fm","title":"<code>pfi.flow.solvers._fm</code>","text":"<p>Flow-matching solver and helper routines.</p>"},{"location":"api/pfi/flow/solvers/_fm/#pfi.flow.solvers._fm.FM_","title":"<code>FM_(dist, times, interp, net, growth_model=None, fac=1, nb=1, n_epochs=2000, lr=0.001, alpha_ann=0.5, device='cpu', verbose=True)</code>","text":"<p>Train drift and optional growth models by flow matching.</p> <p>Parameters:</p> <ul> <li> <code>dist</code>               (<code>list of torch.Tensor, length nsnaps</code>)           \u2013            <p><code>dist[k]</code> has shape <code>(n_k, ndim)</code>.</p> </li> <li> <code>times</code>               (<code>torch.Tensor of shape (nsnaps,)</code>)           \u2013            <p>Snapshot times.</p> </li> <li> <code>interp</code>               (<code>object</code>)           \u2013            <p>Interpolant implementing <code>fit(nodes, data)</code> and <code>predict(nodes) -&gt; (x_interp, dx_interp)</code>.</p> </li> <li> <code>net</code>               (<code>Module</code>)           \u2013            <p>Drift model mapping <code>(batch_size, ndim + 1)</code> to <code>(batch_size, ndim)</code>.</p> </li> <li> <code>growth_model</code>               (<code>Module or None</code>, default:                   <code>None</code> )           \u2013            <p>Optional growth model mapping <code>(batch_size, ndim + 1)</code> to <code>(batch_size, 1)</code>.</p> </li> <li> <code>fac</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Temporal upsampling factor for the uniform grid.</p> </li> <li> <code>nb</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of mini-batches used for OT stitching.</p> </li> <li> <code>n_epochs</code>               (<code>int</code>, default:                   <code>2000</code> )           \u2013            <p>Number of optimization epochs.</p> </li> <li> <code>lr</code>               (<code>float</code>, default:                   <code>1e-3</code> )           \u2013            <p>Learning rate.</p> </li> <li> <code>alpha_ann</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Exponential averaging factor for adaptive mass-loss weight.</p> </li> <li> <code>device</code>               (<code>str or device</code>, default:                   <code>'cpu'</code> )           \u2013            <p>Training device.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, show progress bars and diagnostics.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>drift_net</code> (              <code>Module</code> )          \u2013            <p>Trained drift model.</p> </li> <li> <code>growth_model</code> (              <code>Module or None</code> )          \u2013            <p>Trained growth model (or <code>None</code> when disabled).</p> </li> <li> <code>loss_hist</code> (              <code>list of float</code> )          \u2013            <p>Epoch-wise total objective values.</p> </li> </ul> Source code in <code>pfi/flow/solvers/_fm.py</code> <pre><code>def FM_(\n    dist,\n    times,\n    interp,\n    net,\n    growth_model=None,\n    fac=1,\n    nb=1,\n    n_epochs=2000,\n    lr=1e-3,\n    alpha_ann=0.5,\n    device=\"cpu\",\n    verbose=True,\n):\n    \"\"\"Train drift and optional growth models by flow matching.\n\n    Parameters\n    ----------\n    dist : list of torch.Tensor, length nsnaps\n        ``dist[k]`` has shape ``(n_k, ndim)``.\n    times : torch.Tensor of shape (nsnaps,)\n        Snapshot times.\n    interp : object\n        Interpolant implementing ``fit(nodes, data)`` and\n        ``predict(nodes) -&gt; (x_interp, dx_interp)``.\n    net : torch.nn.Module\n        Drift model mapping ``(batch_size, ndim + 1)`` to ``(batch_size, ndim)``.\n    growth_model : torch.nn.Module or None, default=None\n        Optional growth model mapping ``(batch_size, ndim + 1)`` to\n        ``(batch_size, 1)``.\n    fac : int, default=1\n        Temporal upsampling factor for the uniform grid.\n    nb : int, default=1\n        Number of mini-batches used for OT stitching.\n    n_epochs : int, default=2000\n        Number of optimization epochs.\n    lr : float, default=1e-3\n        Learning rate.\n    alpha_ann : float, default=0.5\n        Exponential averaging factor for adaptive mass-loss weight.\n    device : str or torch.device, default='cpu'\n        Training device.\n    verbose : bool, default=True\n        If ``True``, show progress bars and diagnostics.\n\n    Returns\n    -------\n    drift_net : torch.nn.Module\n        Trained drift model.\n    growth_model : torch.nn.Module or None\n        Trained growth model (or ``None`` when disabled).\n    loss_hist : list of float\n        Epoch-wise total objective values.\n    \"\"\"\n\n    dist_tensor = torch.stack(subsample_shuffle(dist), dim=0).to(device)\n    x_mean = dist_tensor.mean(dim=(0, 1)).unsqueeze(0)\n    x_std = dist_tensor.std(dim=(0, 1)).unsqueeze(0)\n    net.set_scales(x_mean, x_std)\n\n    dist_ot, batch_ot_samples = MMOT_trajectories(dist, nb=nb, device=device)\n    nsnaps, nsamples, ndim = dist_ot.shape\n\n    mass_vec = np.ones((nsnaps,))\n    for k in range(1, nsnaps):\n        mass_vec[k] = dist[k].shape[0] / dist[0].shape[0]\n    mass_vec = torch.tensor(mass_vec, dtype=torch.float32, device=device)\n\n    uniform_kind = torch.tensor(\n        np.linspace(times[0].item(), times[-1].item(), fac * times.shape[0]),\n        dtype=torch.float32,\n    )\n    data_nodes = times[None, :].repeat(batch_ot_samples.shape[1], 1)\n    uniform_nodes = uniform_kind.to(device).expand(batch_ot_samples.shape[1], -1)\n    batch_ot_samples_uniform = interpolate_old2new(batch_ot_samples, data_nodes, uniform_nodes).to(device)\n\n    batch_size = int(nsamples / 4)\n    print(nsamples, batch_size)\n    uniform_batch = uniform_kind.to(device).expand(batch_size, -1)\n\n    data_full = uniform_kind.to(device).expand(batch_ot_samples_uniform.shape[1], -1)\n\n    if isinstance(interp, ChebyshevInterpolant):\n        batch_ot_samples_uniform_ = torch.permute(batch_ot_samples_uniform[:, :, 0:ndim], (1, 0, 2))\n        best_lam, _, _ = select_best_lambda(\n            batch_ot_samples_uniform_,\n            data_full,\n            data_full,\n            device,\n            verbose=verbose,\n        )\n        interp.reg_ = best_lam\n\n    drift_net = net.to(device)\n\n    if growth_model is None:\n        growth_model = None\n        zero_growth = True\n    else:\n        growth_model = growth_model.to(device)\n        zero_growth = False\n\n    params = [{\"params\": drift_net.parameters(), \"lr\": lr}]\n    if not zero_growth:\n        params.append({\"params\": growth_model.parameters(), \"lr\": lr})\n\n    optimizer = torch.optim.Adam(params)\n    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [1000, 1500, 8000, 15000], gamma=0.1)\n\n    dt = times[1] - times[0]\n    lamb = 1.0\n    loss_hist = []\n\n    pbar = tqdm(range(n_epochs), desc=\"FM\", dynamic_ncols=True, disable=not verbose)\n    for epoch in pbar:\n        optimizer.zero_grad()\n\n        if zero_growth:\n            growth_ = torch.zeros_like(batch_ot_samples_uniform[..., 0])\n        else:\n            growth_ = growth_model(batch_ot_samples_uniform).squeeze(-1)\n\n        log_mass = torch.zeros_like(growth_)\n        log_mass[0] = np.log(1.0 / batch_ot_samples_uniform.shape[1])\n        for k in range(1, fac * nsnaps):\n            log_mass[k] = log_mass[k - 1] + 0.5 * (dt / fac) * (growth_[k] + growth_[k - 1])\n\n        mass_ = torch.exp(log_mass)\n        pred_mass = torch.sum(mass_, axis=1)\n\n        Xtrain, ytrain, weights_ = compute_conditional_distributions(\n            interp,\n            batch_ot_samples_uniform,\n            batch_size,\n            mass_,\n            uniform_batch,\n            uniform_batch,\n            device=device,\n            sigma=0.001,\n        )\n\n        inp_ = Xtrain.clone()\n        inp_.requires_grad_()\n        fold = drift_net(inp_)\n\n        res = (fold - ytrain).pow(2).sum(dim=1).reshape(batch_size, fac * nsnaps)\n        weights_ = weights_ / torch.sum(weights_, axis=0)[None, :]\n        wcfm_obj = torch.mean(weights_ * res, axis=0)\n        mass_balance = (pred_mass[::fac] - mass_vec) ** 2\n\n        l1 = torch.sum(wcfm_obj)\n        l2 = torch.sum(mass_balance)\n\n        if not zero_growth:\n            if epoch % 10 == 0:\n                with torch.no_grad():\n                    std_l1 = loss_grad_std(l1, drift_net, device)\n                    if zero_growth:\n                        std_l2 = torch.tensor(1.0, device=device)\n                    else:\n                        std_l2 = loss_grad_std(l2, growth_model, device)\n                    lamb_hat = std_l1 / std_l2\n                    lamb = (1 - alpha_ann) * lamb + alpha_ann * lamb_hat\n\n        total_loss = l1 + lamb * l2\n        loss_hist.append(total_loss.item())\n\n        total_loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        if verbose:\n            pbar.set_postfix(loss=f\"{total_loss.item():.3e}\", lr=f\"{optimizer.param_groups[0]['lr']:.2e}\")\n            if epoch % 500 == 0:\n                tqdm.write(\n                    f\"[epoch {epoch}] \"\n                    f\"inferred mass={torch.sum(mass_, axis=1)[::fac].detach().cpu().numpy()} \"\n                    f\"actual mass={mass_vec}\"\n                )\n\n    return drift_net, growth_model, loss_hist\n</code></pre>"},{"location":"api/pfi/flow/solvers/_fm/#pfi.flow.solvers._fm.compute_conditional_distributions","title":"<code>compute_conditional_distributions(interp, Dist, batch_size, weights, nodes_fit, nodes_eval, device='cpu', sigma=0.001)</code>","text":"<p>Build conditional training pairs for weighted flow matching.</p> <p>Parameters:</p> <ul> <li> <code>interp</code>               (<code>object</code>)           \u2013            <p>Interpolant implementing <code>fit</code> and <code>predict</code>.</p> </li> <li> <code>Dist</code>               (<code>torch.Tensor of shape (n_nodes, nsamples, ndim)</code>)           \u2013            <p>OT trajectories on a temporal grid.</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>Number of trajectories sampled per iteration.</p> </li> <li> <code>weights</code>               (<code>torch.Tensor of shape (n_nodes, nsamples)</code>)           \u2013            <p>Per-trajectory importance weights.</p> </li> <li> <code>nodes_fit</code>               (<code>torch.Tensor of shape (batch_size, n_nodes)</code>)           \u2013            <p>Nodes used to fit interpolants.</p> </li> <li> <code>nodes_eval</code>               (<code>torch.Tensor of shape (batch_size, n_eval)</code>)           \u2013            <p>Nodes used to evaluate interpolants.</p> </li> <li> <code>device</code>               (<code>str or device</code>, default:                   <code>'cpu'</code> )           \u2013            <p>Device used for output tensors.</p> </li> <li> <code>sigma</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>Gaussian perturbation added to interpolated positions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xtrain</code> (              <code>torch.Tensor of shape (batch_size * n_eval, ndim + 1)</code> )          \u2013            <p>Inputs for drift training (state + time).</p> </li> <li> <code>ytrain</code> (              <code>torch.Tensor of shape (batch_size * n_eval, ndim)</code> )          \u2013            <p>Target velocities.</p> </li> <li> <code>weights</code> (              <code>torch.Tensor of shape (batch_size, n_nodes)</code> )          \u2013            <p>Sampled weights aligned with <code>xtrain</code>/<code>ytrain</code>.</p> </li> </ul> Source code in <code>pfi/flow/solvers/_fm.py</code> <pre><code>def compute_conditional_distributions(\n    interp,\n    Dist,\n    batch_size,\n    weights,\n    nodes_fit,\n    nodes_eval,\n    device=\"cpu\",\n    sigma=0.001,\n):\n    \"\"\"Build conditional training pairs for weighted flow matching.\n\n    Parameters\n    ----------\n    interp : object\n        Interpolant implementing ``fit`` and ``predict``.\n    Dist : torch.Tensor of shape (n_nodes, nsamples, ndim)\n        OT trajectories on a temporal grid.\n    batch_size : int\n        Number of trajectories sampled per iteration.\n    weights : torch.Tensor of shape (n_nodes, nsamples)\n        Per-trajectory importance weights.\n    nodes_fit : torch.Tensor of shape (batch_size, n_nodes)\n        Nodes used to fit interpolants.\n    nodes_eval : torch.Tensor of shape (batch_size, n_eval)\n        Nodes used to evaluate interpolants.\n    device : str or torch.device, default='cpu'\n        Device used for output tensors.\n    sigma : float, default=0.001\n        Gaussian perturbation added to interpolated positions.\n\n    Returns\n    -------\n    xtrain : torch.Tensor of shape (batch_size * n_eval, ndim + 1)\n        Inputs for drift training (state + time).\n    ytrain : torch.Tensor of shape (batch_size * n_eval, ndim)\n        Target velocities.\n    weights : torch.Tensor of shape (batch_size, n_nodes)\n        Sampled weights aligned with ``xtrain``/``ytrain``.\n    \"\"\"\n    nsamples = Dist.shape[1]\n    n_eval = nodes_eval.shape[1]\n    ndim = Dist.shape[2]\n\n    xtrain = torch.zeros((batch_size * n_eval, ndim + 1), dtype=torch.float32, device=device)\n    ytrain = torch.zeros((batch_size * n_eval, ndim), dtype=torch.float32, device=device)\n\n    xind = torch.randint(0, nsamples, (batch_size,))\n    Dist = torch.tensor(Dist, dtype=torch.float32, device=device)\n    weights = torch.tensor(weights, dtype=torch.float32, device=device)\n\n    weights = torch.permute(weights[:, xind], (1, 0))\n    Dist = torch.permute(Dist[:, xind, 0:ndim], (1, 0, 2))\n\n    interp.fit(nodes_fit, Dist)\n    x_interp, dx_interp = interp.predict(nodes_eval)\n\n    xtrain[:, ndim] = nodes_eval.reshape(batch_size * n_eval)\n\n    mut = x_interp.view(batch_size * n_eval, ndim)\n    xtrain[:, 0:ndim] = mut + sigma * torch.randn_like(mut)\n    ytrain[:, 0:ndim] = dx_interp.view(batch_size * n_eval, ndim)\n\n    return xtrain, ytrain, weights\n</code></pre>"},{"location":"api/pfi/flow/solvers/_fm/#pfi.flow.solvers._fm.interpolate_old2new","title":"<code>interpolate_old2new(Dist, old_nodes, new_nodes)</code>","text":"<p>Interpolate trajectories from one temporal grid to another.</p> <p>Parameters:</p> <ul> <li> <code>Dist</code>               (<code>torch.Tensor of shape (n_old, batch_size, ndim)</code>)           \u2013            <p>Input trajectories at <code>old_nodes</code>.</p> </li> <li> <code>old_nodes</code>               (<code>torch.Tensor of shape (batch_size, n_old)</code>)           \u2013            <p>Original time nodes.</p> </li> <li> <code>new_nodes</code>               (<code>torch.Tensor of shape (batch_size, n_new)</code>)           \u2013            <p>Target time nodes.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>x_interp</code> (              <code>torch.Tensor of shape (n_new, batch_size, ndim)</code> )          \u2013            <p>Interpolated trajectories.</p> </li> </ul> Source code in <code>pfi/flow/solvers/_fm.py</code> <pre><code>def interpolate_old2new(\n    Dist,\n    old_nodes,\n    new_nodes,\n):\n    \"\"\"Interpolate trajectories from one temporal grid to another.\n\n    Parameters\n    ----------\n    Dist : torch.Tensor of shape (n_old, batch_size, ndim)\n        Input trajectories at ``old_nodes``.\n    old_nodes : torch.Tensor of shape (batch_size, n_old)\n        Original time nodes.\n    new_nodes : torch.Tensor of shape (batch_size, n_new)\n        Target time nodes.\n\n    Returns\n    -------\n    x_interp : torch.Tensor of shape (n_new, batch_size, ndim)\n        Interpolated trajectories.\n    \"\"\"\n    _, b, d = Dist.shape\n    x = torch.permute(Dist, (1, 0, 2))\n\n    x_interp = torch.stack(\n        [\n            torch.stack(\n                [\n                    torch.tensor(\n                        np.interp(\n                            new_nodes[i].cpu().numpy(),\n                            old_nodes[i].cpu().numpy(),\n                            x[i, :, j].cpu().numpy(),\n                        ),\n                        dtype=torch.float32,\n                    )\n                    for j in range(d)\n                ],\n                dim=1,\n            )\n            for i in range(b)\n        ],\n        dim=0,\n    )\n\n    return torch.permute(x_interp, (1, 0, 2))\n</code></pre>"},{"location":"api/pfi/flow/solvers/_ode/","title":"_ode","text":""},{"location":"api/pfi/flow/solvers/_ode/#pfi.flow.solvers._ode","title":"<code>pfi.flow.solvers._ode</code>","text":"<p>Functions which should implement Unbalanced PFI</p>"},{"location":"api/pfi/flow/solvers/_sde/","title":"_sde","text":""},{"location":"api/pfi/flow/solvers/_sde/#pfi.flow.solvers._sde","title":"<code>pfi.flow.solvers._sde</code>","text":"<p>Functions which should implement PFI with SDEs (bad idea)</p>"},{"location":"api/pfi/score/","title":"score","text":""},{"location":"api/pfi/score/#pfi.score","title":"<code>pfi.score</code>","text":"<p>Public score-estimation APIs and solver modules.</p> <p>This submodule exposes the score matching estimator alongside score models and solver backends.</p>"},{"location":"api/pfi/score/#pfi.score.ScoreMatching","title":"<code>ScoreMatching(model, solver='dsm', solver_kwargs=None, device='cpu')</code>","text":"<p>Estimate score functions from snapshot data.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>Score model (dimensions of input change depending on solver)</p> </li> <li> <code>solver</code>               (<code>dsm</code>, default:                   <code>'dsm'</code> )           \u2013            <p>Solver backend.</p> </li> <li> <code>solver_kwargs</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>Extra keyword arguments passed to the selected solver.</p> </li> <li> <code>device</code>               (<code>str or device</code>, default:                   <code>'cpu'</code> )           \u2013            <p>Device used for training and inference.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>Ndim_</code>               (<code>int</code>)           \u2013            <p>Inferred state dimension, set during <code>fit</code>.</p> </li> <li> <code>times_</code>               (<code>ndarray of shape (n_times,)</code>)           \u2013            <p>Sorted unique training times, set during <code>fit</code>.</p> </li> <li> <code>model_</code>               (<code>Module</code>)           \u2013            <p>Fitted score model used at inference time, set during <code>fit</code>. The input of this fitted model is (x,t), dimension ndim + 1.</p> </li> </ul> Source code in <code>pfi/score/_base.py</code> <pre><code>def __init__(\n    self,\n    model,\n    solver=\"dsm\",\n    solver_kwargs=None,\n    device=\"cpu\",\n):\n    self.model = model\n    self.solver = solver\n    self.solver_kwargs = solver_kwargs if solver_kwargs is not None else {}\n    self.device = device\n</code></pre>"},{"location":"api/pfi/score/#pfi.score.ScoreMatching.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the score estimator on time-augmented data.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray of shape (n_samples, ndim + 1)</code>)           \u2013            <p>Input data where the last column contains time.</p> </li> <li> <code>y</code>               (<code>None</code>, default:                   <code>None</code> )           \u2013            <p>Ignored. Present for estimator API compatibility.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>self</code> (              <code>ScoreMatching</code> )          \u2013            <p>Fitted estimator.</p> </li> </ul> Source code in <code>pfi/score/_base.py</code> <pre><code>def fit(\n    self,\n    X,\n    y=None,\n):\n    \"\"\"Fit the score estimator on time-augmented data.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, ndim + 1)\n        Input data where the last column contains time.\n    y : None, default=None\n        Ignored. Present for estimator API compatibility.\n\n    Returns\n    -------\n    self : ScoreMatching\n        Fitted estimator.\n    \"\"\"\n    dist, times = snapshots_from_X(X)\n\n    self.Ndim_ = X.shape[1] - 1\n    self.model = self.model.to(self.device)\n    self.times_ = np.unique(X[:, -1])\n\n    if self.solver == \"dsm\":\n        self.model, loss_hist = DSM_(\n            dist,\n            times,\n            self.model,\n            device=self.device,\n            **self.solver_kwargs,\n        )\n        self.loss_ = np.asarray(loss_hist)\n        self.model_ = FreezeVarDNN(\n            dnn=self.model,\n            var_index=self.Ndim_,\n            var_value=0.01,\n        )\n        self.model.eval()\n    else:\n        raise NotImplementedError(\"Other score matching solvers not implemented yet.\")\n\n    self.model_ = self.model_.eval()\n    return self\n</code></pre>"},{"location":"api/pfi/score/#pfi.score.ScoreMatching.predict","title":"<code>predict(X)</code>","text":"<p>Predict score vectors for input samples.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>array-like of shape (n_samples, ndim + 1)</code>)           \u2013            <p>Inputs containing state and time columns. The internal noise-level feature is inserted by <code>model_</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>score</code> (              <code>ndarray of shape (n_samples, ndim)</code> )          \u2013            <p>Predicted score vectors.</p> </li> </ul> Source code in <code>pfi/score/_base.py</code> <pre><code>def predict(\n    self,\n    X,\n):\n    \"\"\"Predict score vectors for input samples.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, ndim + 1)\n        Inputs containing state and time columns. The internal noise-level\n        feature is inserted by `model_`.\n\n    Returns\n    -------\n    score : ndarray of shape (n_samples, ndim)\n        Predicted score vectors.\n    \"\"\"\n    X = torch.tensor(X, dtype=torch.float32, device=self.device)\n    score = self.model_(X)\n\n    return score.detach().cpu().numpy()\n</code></pre>"},{"location":"api/pfi/score/#pfi.score.ScoreMatching.sample","title":"<code>sample(X, nsamples=None, maxiter=100)</code>","text":"<p>Generate samples using the fitted score model.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray of shape of shape (n_samples, ndim + 1)</code>)           \u2013            <p>Conditioning samples with time in the last column.</p> </li> <li> <code>nsamples</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Number of generated samples. If <code>None</code>, uses <code>X.shape[0]</code>.</p> </li> <li> <code>maxiter</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Number of Langevin updates per noise level.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>gen</code> (              <code>ndarray of shape (nsamples, ndim)</code> )          \u2013            <p>Generated samples.</p> </li> </ul> Source code in <code>pfi/score/_base.py</code> <pre><code>def sample(\n    self,\n    X,\n    nsamples=None,\n    maxiter=100,\n):\n    \"\"\"Generate samples using the fitted score model.\n\n    Parameters\n    ----------\n    X : ndarray of shape of shape (n_samples, ndim + 1)\n        Conditioning samples with time in the last column.\n    nsamples : int, default=None\n        Number of generated samples. If ``None``, uses ``X.shape[0]``.\n    maxiter : int, default=100\n        Number of Langevin updates per noise level.\n\n    Returns\n    -------\n    gen : ndarray of shape (nsamples, ndim)\n        Generated samples.\n    \"\"\"\n    X = torch.tensor(X, \n                     dtype=torch.float32, \n                     device=self.device)\n\n    if nsamples is None:\n        nsamples = X.shape[0]\n\n    if self.solver == \"dsm\":\n\n        init_ = 4*torch.rand((nsamples,self.Ndim_+2)) + 1\n        init_[:,0:self.Ndim_] = X[:,0:self.Ndim_]\n        time_ = X[0, -1]\n        with torch.no_grad():\n            gen, _ = generate_data_DSM(\n                maxiter=maxiter,\n                infNet=self.model,\n                nsamples=nsamples,\n                init_=init_,\n                time_=time_,\n                L=self.solver_kwargs[\"L\"],\n                ndim=self.Ndim_,\n                device=self.device,\n            )\n        return gen[:, : self.Ndim_]\n\n    raise NotImplementedError(\"Langevin sampling not implemented yet.\")\n</code></pre>"},{"location":"api/pfi/score/#pfi.score.ScoreMatching.score","title":"<code>score(X, y=None, maxiter=100)</code>","text":"<p>Compute per-time energy distance between generated and observed data.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray of shape (n_samples, ndim + 1)</code>)           \u2013            <p>Input data with time in the last column.</p> </li> <li> <code>y</code>               (<code>None</code>, default:                   <code>None</code> )           \u2013            <p>Ignored. Present for estimator API compatibility.</p> </li> <li> <code>maxiter</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Number of Langevin updates used during sampling.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>scores</code> (              <code>ndarray of shape (n_times,)</code> )          \u2013            <p>Energy distance at each unique time in <code>X</code>.</p> </li> </ul> Source code in <code>pfi/score/_base.py</code> <pre><code>def score(\n    self,\n    X,\n    y=None,\n    maxiter=100,\n):\n    \"\"\"Compute per-time energy distance between generated and observed data.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, ndim + 1)\n        Input data with time in the last column.\n    y : None, default=None\n        Ignored. Present for estimator API compatibility.\n    maxiter : int, default=100\n        Number of Langevin updates used during sampling.\n\n    Returns\n    -------\n    scores : ndarray of shape (n_times,)\n        Energy distance at each unique time in ``X``.\n    \"\"\"\n    import geomloss\n\n    X = torch.tensor(X, \n                     dtype=torch.float32, \n                     device=self.device)\n    times = torch.unique(X[:, -1])\n    scores = []\n\n    loss = geomloss.SamplesLoss(\"energy\")\n    for t in times:\n        x_t = X[X[:, -1] == t]\n        gen = self.sample(x_t, nsamples=x_t.shape[0], maxiter=maxiter)\n        y_t = x_t[:, : self.Ndim_]\n        ed = loss(\n            torch.tensor(gen, dtype=torch.float32, device=self.device),\n            torch.tensor(y_t, dtype=torch.float32, device=self.device),\n        ).item()\n        scores.append(ed)\n\n    return np.asarray(scores)\n</code></pre>"},{"location":"api/pfi/score/_base/","title":"_base","text":""},{"location":"api/pfi/score/_base/#pfi.score._base","title":"<code>pfi.score._base</code>","text":"<p>Score-matching estimator interface and evaluation utilities.</p>"},{"location":"api/pfi/score/_base/#pfi.score._base.ScoreMatching","title":"<code>ScoreMatching(model, solver='dsm', solver_kwargs=None, device='cpu')</code>","text":"<p>Estimate score functions from snapshot data.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>Score model (dimensions of input change depending on solver)</p> </li> <li> <code>solver</code>               (<code>dsm</code>, default:                   <code>'dsm'</code> )           \u2013            <p>Solver backend.</p> </li> <li> <code>solver_kwargs</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>Extra keyword arguments passed to the selected solver.</p> </li> <li> <code>device</code>               (<code>str or device</code>, default:                   <code>'cpu'</code> )           \u2013            <p>Device used for training and inference.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>Ndim_</code>               (<code>int</code>)           \u2013            <p>Inferred state dimension, set during <code>fit</code>.</p> </li> <li> <code>times_</code>               (<code>ndarray of shape (n_times,)</code>)           \u2013            <p>Sorted unique training times, set during <code>fit</code>.</p> </li> <li> <code>model_</code>               (<code>Module</code>)           \u2013            <p>Fitted score model used at inference time, set during <code>fit</code>. The input of this fitted model is (x,t), dimension ndim + 1.</p> </li> </ul> Source code in <code>pfi/score/_base.py</code> <pre><code>def __init__(\n    self,\n    model,\n    solver=\"dsm\",\n    solver_kwargs=None,\n    device=\"cpu\",\n):\n    self.model = model\n    self.solver = solver\n    self.solver_kwargs = solver_kwargs if solver_kwargs is not None else {}\n    self.device = device\n</code></pre>"},{"location":"api/pfi/score/_base/#pfi.score._base.ScoreMatching.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the score estimator on time-augmented data.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray of shape (n_samples, ndim + 1)</code>)           \u2013            <p>Input data where the last column contains time.</p> </li> <li> <code>y</code>               (<code>None</code>, default:                   <code>None</code> )           \u2013            <p>Ignored. Present for estimator API compatibility.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>self</code> (              <code>ScoreMatching</code> )          \u2013            <p>Fitted estimator.</p> </li> </ul> Source code in <code>pfi/score/_base.py</code> <pre><code>def fit(\n    self,\n    X,\n    y=None,\n):\n    \"\"\"Fit the score estimator on time-augmented data.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, ndim + 1)\n        Input data where the last column contains time.\n    y : None, default=None\n        Ignored. Present for estimator API compatibility.\n\n    Returns\n    -------\n    self : ScoreMatching\n        Fitted estimator.\n    \"\"\"\n    dist, times = snapshots_from_X(X)\n\n    self.Ndim_ = X.shape[1] - 1\n    self.model = self.model.to(self.device)\n    self.times_ = np.unique(X[:, -1])\n\n    if self.solver == \"dsm\":\n        self.model, loss_hist = DSM_(\n            dist,\n            times,\n            self.model,\n            device=self.device,\n            **self.solver_kwargs,\n        )\n        self.loss_ = np.asarray(loss_hist)\n        self.model_ = FreezeVarDNN(\n            dnn=self.model,\n            var_index=self.Ndim_,\n            var_value=0.01,\n        )\n        self.model.eval()\n    else:\n        raise NotImplementedError(\"Other score matching solvers not implemented yet.\")\n\n    self.model_ = self.model_.eval()\n    return self\n</code></pre>"},{"location":"api/pfi/score/_base/#pfi.score._base.ScoreMatching.predict","title":"<code>predict(X)</code>","text":"<p>Predict score vectors for input samples.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>array-like of shape (n_samples, ndim + 1)</code>)           \u2013            <p>Inputs containing state and time columns. The internal noise-level feature is inserted by <code>model_</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>score</code> (              <code>ndarray of shape (n_samples, ndim)</code> )          \u2013            <p>Predicted score vectors.</p> </li> </ul> Source code in <code>pfi/score/_base.py</code> <pre><code>def predict(\n    self,\n    X,\n):\n    \"\"\"Predict score vectors for input samples.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, ndim + 1)\n        Inputs containing state and time columns. The internal noise-level\n        feature is inserted by `model_`.\n\n    Returns\n    -------\n    score : ndarray of shape (n_samples, ndim)\n        Predicted score vectors.\n    \"\"\"\n    X = torch.tensor(X, dtype=torch.float32, device=self.device)\n    score = self.model_(X)\n\n    return score.detach().cpu().numpy()\n</code></pre>"},{"location":"api/pfi/score/_base/#pfi.score._base.ScoreMatching.sample","title":"<code>sample(X, nsamples=None, maxiter=100)</code>","text":"<p>Generate samples using the fitted score model.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray of shape of shape (n_samples, ndim + 1)</code>)           \u2013            <p>Conditioning samples with time in the last column.</p> </li> <li> <code>nsamples</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Number of generated samples. If <code>None</code>, uses <code>X.shape[0]</code>.</p> </li> <li> <code>maxiter</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Number of Langevin updates per noise level.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>gen</code> (              <code>ndarray of shape (nsamples, ndim)</code> )          \u2013            <p>Generated samples.</p> </li> </ul> Source code in <code>pfi/score/_base.py</code> <pre><code>def sample(\n    self,\n    X,\n    nsamples=None,\n    maxiter=100,\n):\n    \"\"\"Generate samples using the fitted score model.\n\n    Parameters\n    ----------\n    X : ndarray of shape of shape (n_samples, ndim + 1)\n        Conditioning samples with time in the last column.\n    nsamples : int, default=None\n        Number of generated samples. If ``None``, uses ``X.shape[0]``.\n    maxiter : int, default=100\n        Number of Langevin updates per noise level.\n\n    Returns\n    -------\n    gen : ndarray of shape (nsamples, ndim)\n        Generated samples.\n    \"\"\"\n    X = torch.tensor(X, \n                     dtype=torch.float32, \n                     device=self.device)\n\n    if nsamples is None:\n        nsamples = X.shape[0]\n\n    if self.solver == \"dsm\":\n\n        init_ = 4*torch.rand((nsamples,self.Ndim_+2)) + 1\n        init_[:,0:self.Ndim_] = X[:,0:self.Ndim_]\n        time_ = X[0, -1]\n        with torch.no_grad():\n            gen, _ = generate_data_DSM(\n                maxiter=maxiter,\n                infNet=self.model,\n                nsamples=nsamples,\n                init_=init_,\n                time_=time_,\n                L=self.solver_kwargs[\"L\"],\n                ndim=self.Ndim_,\n                device=self.device,\n            )\n        return gen[:, : self.Ndim_]\n\n    raise NotImplementedError(\"Langevin sampling not implemented yet.\")\n</code></pre>"},{"location":"api/pfi/score/_base/#pfi.score._base.ScoreMatching.score","title":"<code>score(X, y=None, maxiter=100)</code>","text":"<p>Compute per-time energy distance between generated and observed data.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray of shape (n_samples, ndim + 1)</code>)           \u2013            <p>Input data with time in the last column.</p> </li> <li> <code>y</code>               (<code>None</code>, default:                   <code>None</code> )           \u2013            <p>Ignored. Present for estimator API compatibility.</p> </li> <li> <code>maxiter</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Number of Langevin updates used during sampling.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>scores</code> (              <code>ndarray of shape (n_times,)</code> )          \u2013            <p>Energy distance at each unique time in <code>X</code>.</p> </li> </ul> Source code in <code>pfi/score/_base.py</code> <pre><code>def score(\n    self,\n    X,\n    y=None,\n    maxiter=100,\n):\n    \"\"\"Compute per-time energy distance between generated and observed data.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, ndim + 1)\n        Input data with time in the last column.\n    y : None, default=None\n        Ignored. Present for estimator API compatibility.\n    maxiter : int, default=100\n        Number of Langevin updates used during sampling.\n\n    Returns\n    -------\n    scores : ndarray of shape (n_times,)\n        Energy distance at each unique time in ``X``.\n    \"\"\"\n    import geomloss\n\n    X = torch.tensor(X, \n                     dtype=torch.float32, \n                     device=self.device)\n    times = torch.unique(X[:, -1])\n    scores = []\n\n    loss = geomloss.SamplesLoss(\"energy\")\n    for t in times:\n        x_t = X[X[:, -1] == t]\n        gen = self.sample(x_t, nsamples=x_t.shape[0], maxiter=maxiter)\n        y_t = x_t[:, : self.Ndim_]\n        ed = loss(\n            torch.tensor(gen, dtype=torch.float32, device=self.device),\n            torch.tensor(y_t, dtype=torch.float32, device=self.device),\n        ).item()\n        scores.append(ed)\n\n    return np.asarray(scores)\n</code></pre>"},{"location":"api/pfi/score/models/","title":"models","text":""},{"location":"api/pfi/score/models/#pfi.score.models","title":"<code>pfi.score.models</code>","text":"<p>Score models used by score-matching.</p>"},{"location":"api/pfi/score/models/#pfi.score.models.OUScore","title":"<code>OUScore(net, m0, S0, D)</code>","text":"<p>               Bases: <code>_CompoundModel</code></p> <p>Analytical score function for Gaussian Ornstein-Uhlenbeck dynamics.</p> <p>Parameters:</p> <ul> <li> <code>net</code>               (<code>torch.Tensor of shape (ndim, ndim)</code>)           \u2013            <p>Drift matrix.</p> </li> <li> <code>m0</code>               (<code>torch.Tensor of shape (ndim,)</code>)           \u2013            <p>Initial mean.</p> </li> <li> <code>S0</code>               (<code>torch.Tensor of shape (ndim, ndim)</code>)           \u2013            <p>Initial covariance matrix.</p> </li> <li> <code>D</code>               (<code>torch.Tensor of shape (ndim, ndim)</code>)           \u2013            <p>Diffusion matrix.</p> </li> </ul> Source code in <code>pfi/score/models.py</code> <pre><code>def __init__(\n    self,\n    net,\n    m0,\n    S0,\n    D,\n):\n    super(OUScore, self).__init__(nn.Parameter(net))\n    self.Ndim = m0.shape[0]\n    self.S0 = S0\n    self.m0 = m0\n    self.D = D\n</code></pre>"},{"location":"api/pfi/score/solvers/","title":"solvers","text":""},{"location":"api/pfi/score/solvers/#pfi.score.solvers","title":"<code>pfi.score.solvers</code>","text":""},{"location":"api/pfi/score/solvers/#pfi.score.solvers.DSM_","title":"<code>DSM_(dist, times, net, L=10, n_epochs=2000, bs=None, adp_flag=0, lr=0.0001, device='cpu', verbose=True)</code>","text":"<p>Train a score network with denoising score matching.</p> <p>Parameters:</p> <ul> <li> <code>dist</code>               (<code>list of torch.Tensor, length nsnaps</code>)           \u2013            <p><code>dist[k]</code> has shape <code>(n_k, ndim)</code>.</p> </li> <li> <code>times</code>               (<code>torch.Tensor of shape (nsnaps,)</code>)           \u2013            <p>Snapshot times.</p> </li> <li> <code>net</code>               (<code>Module</code>)           \u2013            <p>Score network that accepts inputs of shape <code>(nsnaps, L, batch_size, ndim + 2)</code> after batching.</p> </li> <li> <code>L</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of noise levels.</p> </li> <li> <code>n_epochs</code>               (<code>int</code>, default:                   <code>2000</code> )           \u2013            <p>Number of optimization epochs.</p> </li> <li> <code>bs</code>               (<code>int or None</code>, default:                   <code>None</code> )           \u2013            <p>Mini-batch size over sample dimension. If <code>None</code>, uses all samples.</p> </li> <li> <code>adp_flag</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>If set to <code>1</code>, enable adaptive per-time weighting.</p> </li> <li> <code>lr</code>               (<code>float</code>, default:                   <code>1e-4</code> )           \u2013            <p>Learning rate.</p> </li> <li> <code>device</code>               (<code>str or device</code>, default:                   <code>'cpu'</code> )           \u2013            <p>Training device.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, show progress bars and diagnostics.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>net</code> (              <code>Module</code> )          \u2013            <p>Trained score network.</p> </li> <li> <code>loss_hist</code> (              <code>list of float</code> )          \u2013            <p>Epoch-wise training loss values.</p> </li> </ul> Source code in <code>pfi/score/solvers/_dsm.py</code> <pre><code>def DSM_(\n    dist,\n    times,\n    net,\n    L=10,\n    n_epochs=2000,\n    bs=None,\n    adp_flag=0,\n    lr=1e-4,\n    device=\"cpu\",\n    verbose=True,\n):\n    \"\"\"Train a score network with denoising score matching.\n\n    Parameters\n    ----------\n    dist : list of torch.Tensor, length nsnaps\n        ``dist[k]`` has shape ``(n_k, ndim)``.\n    times : torch.Tensor of shape (nsnaps,)\n        Snapshot times.\n    net : torch.nn.Module\n        Score network that accepts inputs of shape\n        ``(nsnaps, L, batch_size, ndim + 2)`` after batching.\n    L : int, default=10\n        Number of noise levels.\n    n_epochs : int, default=2000\n        Number of optimization epochs.\n    bs : int or None, default=None\n        Mini-batch size over sample dimension. If ``None``, uses all samples.\n    adp_flag : int, default=0\n        If set to ``1``, enable adaptive per-time weighting.\n    lr : float, default=1e-4\n        Learning rate.\n    device : str or torch.device, default='cpu'\n        Training device.\n    verbose : bool, default=True\n        If ``True``, show progress bars and diagnostics.\n\n    Returns\n    -------\n    net : torch.nn.Module\n        Trained score network.\n    loss_hist : list of float\n        Epoch-wise training loss values.\n    \"\"\"\n    dist = torch.stack(subsample_shuffle(dist), dim=0)\n\n    nsnaps, nsamples, ndim = dist.shape\n    if bs is None:\n        bs = nsamples\n\n    x_train, x_data, x_mean, x_std, sigma = generate_noisy_training_data_batch(\n        dist,\n        ndim,\n        times,\n        L,\n        nsamples,\n        nsnaps,\n        device,\n    )\n\n    net.set_scales(x_mean.to(device), x_std.to(device))\n\n    xtrain = torch.tensor(x_train, dtype=torch.float32).to(device)\n    xdata = torch.tensor(x_data, dtype=torch.float32).to(device)\n\n    loader = FastTensorDataLoader(\n        torch.permute(xtrain, (2, 0, 1, 3)),\n        torch.permute(xdata, (1, 0, 2)),\n        batch_size=bs,\n        shuffle=True,\n    )\n\n    optimizer = torch.optim.Adam(list(net.parameters()), lr=lr)\n    scheduler = MultiStepLR(optimizer, milestones=[2500, 6500, 8500], gamma=0.1)\n\n    c_ = torch.ones((nsnaps,), dtype=torch.float32, device=device)\n    alpha_ann = 0.5\n    adapt_int = 10\n    weight_decay = 1e-4\n\n    sigma_tensor = torch.tensor(sigma, dtype=torch.float32, device=x_train.device)\n    sigma_reshaped = sigma_tensor.view(L, 1, 1)\n    pbar = tqdm(range(n_epochs), desc=\"DSM\", dynamic_ncols=True, disable=not verbose)\n    loss_hist = []\n\n    for epoch in pbar:\n        for _, (xbatch, ybatch) in enumerate(loader):\n            optimizer.zero_grad()\n\n            xbatch = xbatch.clone().detach().requires_grad_(True)\n            ybatch = ybatch.clone().detach().requires_grad_(True)\n\n            xbatch = torch.permute(xbatch, (1, 2, 0, 3))\n            ybatch = torch.permute(ybatch, (1, 0, 2))\n\n            uhat = net(xbatch)\n            lcomp = torch.zeros((nsnaps,), device=device)\n            std_ = torch.zeros((nsnaps,), device=device)\n\n            for tind in range(nsnaps):\n                u_pred = uhat[tind]\n                x_t = xbatch[tind, :, :, :ndim]\n                x_data = ybatch[tind, :, :ndim][np.newaxis, :, :]\n\n                u_true = (x_t - x_data) / (sigma_reshaped ** 2)\n                residual = u_pred + u_true\n                residual_squared = residual.pow(2).mean(dim=(1, 2))\n                loss_sum = 0.5 * torch.sum((sigma_tensor ** 2) * residual_squared)\n\n                if adp_flag == 1 and epoch % adapt_int == 0:\n                    with torch.no_grad():\n                        std_[tind] = loss_grad_std(loss_sum, net, device)\n\n                lcomp[tind] = loss_sum\n\n            if adp_flag == 1 and epoch % adapt_int == 0:\n                with torch.no_grad():\n                    lamb_hat = torch.max(std_) / std_\n                    c_ = (1 - alpha_ann) * c_ + alpha_ann * lamb_hat\n                    c_ = c_ / torch.sum(c_)\n\n            loss = sum(c_[tind] * lcomp[tind] for tind in range(nsnaps))\n            weight_norm = sum((p ** 2).sum() for p in net.parameters())\n            loss = loss + weight_decay * weight_norm\n\n            loss.backward()\n            optimizer.step()\n\n        scheduler.step()\n        loss_hist.append(loss.item())\n        if verbose:\n            pbar.set_postfix(loss=f\"{loss.item():.3e}\", lr=f\"{optimizer.param_groups[0]['lr']:.2e}\")\n            if epoch % 500 == 0:\n                tqdm.write(f\"epoch: {epoch} c_: {c_.detach().cpu().numpy()}\")\n\n    return net, loss_hist\n</code></pre>"},{"location":"api/pfi/score/solvers/#pfi.score.solvers.generate_data_DSM","title":"<code>generate_data_DSM(maxiter, infNet, nsamples, init_, time_, L, ndim, device)</code>","text":"<p>Sample from a trained DSM model using annealed Langevin dynamics.</p> <p>Parameters:</p> <ul> <li> <code>maxiter</code>               (<code>int</code>)           \u2013            <p>Number of Langevin steps per noise level.</p> </li> <li> <code>infNet</code>               (<code>Module</code>)           \u2013            <p>Trained score model.</p> </li> <li> <code>nsamples</code>               (<code>int</code>)           \u2013            <p>Number of samples to generate.</p> </li> <li> <code>init_</code>               (<code>ndarray of shape (nsamples, ndim + 2)</code>)           \u2013            <p>Initial states including noise-level and time columns.</p> </li> <li> <code>time_</code>               (<code>float</code>)           \u2013            <p>Time value assigned to all generated samples.</p> </li> <li> <code>L</code>               (<code>int</code>)           \u2013            <p>Number of noise levels.</p> </li> <li> <code>ndim</code>               (<code>int</code>)           \u2013            <p>State dimension.</p> </li> <li> <code>device</code>               (<code>str or device</code>)           \u2013            <p>Device used for generation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>sol</code> (              <code>ndarray of shape (nsamples, ndim + 2)</code> )          \u2013            <p>Generated states including auxiliary columns.</p> </li> <li> <code>gen_mean</code> (              <code>torch.Tensor of shape (ndim,)</code> )          \u2013            <p>Mean of generated state coordinates.</p> </li> </ul> Source code in <code>pfi/score/solvers/_dsm.py</code> <pre><code>def generate_data_DSM(\n    maxiter,\n    infNet,\n    nsamples,\n    init_,\n    time_,\n    L,\n    ndim,\n    device,\n):\n    \"\"\"Sample from a trained DSM model using annealed Langevin dynamics.\n\n    Parameters\n    ----------\n    maxiter : int\n        Number of Langevin steps per noise level.\n    infNet : torch.nn.Module\n        Trained score model.\n    nsamples : int\n        Number of samples to generate.\n    init_ : ndarray of shape (nsamples, ndim + 2)\n        Initial states including noise-level and time columns.\n    time_ : float\n        Time value assigned to all generated samples.\n    L : int\n        Number of noise levels.\n    ndim : int\n        State dimension.\n    device : str or torch.device\n        Device used for generation.\n\n    Returns\n    -------\n    sol : ndarray of shape (nsamples, ndim + 2)\n        Generated states including auxiliary columns.\n    gen_mean : torch.Tensor of shape (ndim,)\n        Mean of generated state coordinates.\n    \"\"\"\n    eps = 1e-4\n    sol = torch.tensor(init_, dtype=torch.float32).to(device)\n    sol[:, ndim + 1] = time_\n\n    sigma = geometric_sequence(L)\n\n    for k in range(0, L):\n        alpha = eps * ((sigma[k] ** 2) / (sigma[L - 1] ** 2))\n        sol[:, ndim] = sigma[k]\n\n        for _ in range(0, maxiter):\n            z = torch.normal(0, 1, size=(nsamples, ndim)).to(device)\n            guru = infNet(sol)\n            sol[:, 0:ndim] = sol[:, 0:ndim] + 0.5 * alpha * guru + np.sqrt(alpha) * z\n\n    gen_mean = torch.mean(sol, axis=0)[0:ndim]\n    sol = sol.cpu().data.numpy().reshape(nsamples, ndim + 2)\n\n    return sol, gen_mean\n</code></pre>"},{"location":"api/pfi/score/solvers/#pfi.score.solvers.generate_noisy_training_data_batch","title":"<code>generate_noisy_training_data_batch(Dist, ndim, tp, L, nsamples, nsnaps, device)</code>","text":"<p>Generate noisy DSM training inputs and normalization statistics.</p> <p>Parameters:</p> <ul> <li> <code>Dist</code>               (<code>torch.Tensor or ndarray of shape (nsnaps, nsamples, ndim)</code>)           \u2013            <p>Snapshot samples.</p> </li> <li> <code>ndim</code>               (<code>int</code>)           \u2013            <p>State dimension.</p> </li> <li> <code>tp</code>               (<code>array-like of shape (nsnaps,)</code>)           \u2013            <p>Snapshot times.</p> </li> <li> <code>L</code>               (<code>int</code>)           \u2013            <p>Number of noise levels.</p> </li> <li> <code>nsamples</code>               (<code>int</code>)           \u2013            <p>Number of samples per snapshot.</p> </li> <li> <code>nsnaps</code>               (<code>int</code>)           \u2013            <p>Number of snapshots.</p> </li> <li> <code>device</code>               (<code>str or device</code>)           \u2013            <p>Target device for returned tensors.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>x_train</code> (              <code>torch.Tensor of shape (nsnaps, L, nsamples, ndim + 2)</code> )          \u2013            <p>Noisy training inputs with appended noise level and time.</p> </li> <li> <code>x_data</code> (              <code>torch.Tensor of shape (nsnaps, nsamples, ndim)</code> )          \u2013            <p>Clean data tensor.</p> </li> <li> <code>x_mean</code> (              <code>torch.Tensor of shape (1, ndim + 2)</code> )          \u2013            <p>Feature-wise mean used for input normalization.</p> </li> <li> <code>x_std</code> (              <code>torch.Tensor of shape (1, ndim + 2)</code> )          \u2013            <p>Feature-wise standard deviation used for input normalization.</p> </li> <li> <code>sigma</code> (              <code>ndarray of shape (L,)</code> )          \u2013            <p>Noise schedule.</p> </li> </ul> Source code in <code>pfi/score/solvers/_dsm.py</code> <pre><code>def generate_noisy_training_data_batch(\n    Dist,\n    ndim,\n    tp,\n    L,\n    nsamples,\n    nsnaps,\n    device,\n):\n    \"\"\"Generate noisy DSM training inputs and normalization statistics.\n\n    Parameters\n    ----------\n    Dist : torch.Tensor or ndarray of shape (nsnaps, nsamples, ndim)\n        Snapshot samples.\n    ndim : int\n        State dimension.\n    tp : array-like of shape (nsnaps,)\n        Snapshot times.\n    L : int\n        Number of noise levels.\n    nsamples : int\n        Number of samples per snapshot.\n    nsnaps : int\n        Number of snapshots.\n    device : str or torch.device\n        Target device for returned tensors.\n\n    Returns\n    -------\n    x_train : torch.Tensor of shape (nsnaps, L, nsamples, ndim + 2)\n        Noisy training inputs with appended noise level and time.\n    x_data : torch.Tensor of shape (nsnaps, nsamples, ndim)\n        Clean data tensor.\n    x_mean : torch.Tensor of shape (1, ndim + 2)\n        Feature-wise mean used for input normalization.\n    x_std : torch.Tensor of shape (1, ndim + 2)\n        Feature-wise standard deviation used for input normalization.\n    sigma : ndarray of shape (L,)\n        Noise schedule.\n    \"\"\"\n    sigma = geometric_sequence(L)\n    transform_data = np.zeros((nsnaps, L, nsamples, ndim + 2))\n\n    for tind in range(nsnaps):\n        for i in range(0, L):\n            for j in range(0, nsamples):\n                mean = Dist[tind, j, 0:ndim]\n                cov = (sigma[i] ** 2) * np.eye(ndim)\n                transform_data[tind, i, j, 0:ndim] = np.random.multivariate_normal(mean, cov)\n                transform_data[tind, i, j, ndim] = sigma[i]\n                transform_data[tind, i, j, ndim + 1] = tp[tind]\n\n    x_train = torch.tensor(transform_data, dtype=torch.float32, requires_grad=True, device=device)\n    x_data = torch.tensor(Dist, dtype=torch.float32, device=device)\n\n    x_mean = torch.zeros((ndim + 2,))\n    x_std = torch.ones((ndim + 2,))\n    for i in range(0, ndim + 2):\n        x_mean[i] = torch.tensor(\n            np.mean(transform_data[:, :, :, i].flatten(), axis=0, keepdims=True),\n            dtype=torch.float32,\n        )\n        x_std[i] = torch.tensor(\n            np.std(transform_data[:, :, :, i].flatten(), axis=0, keepdims=True),\n            dtype=torch.float32,\n        )\n\n    x_mean = x_mean[np.newaxis, :]\n    x_std = x_std[np.newaxis, :]\n\n    return x_train, x_data, x_mean, x_std, sigma\n</code></pre>"},{"location":"api/pfi/score/solvers/_dsm/","title":"_dsm","text":""},{"location":"api/pfi/score/solvers/_dsm/#pfi.score.solvers._dsm","title":"<code>pfi.score.solvers._dsm</code>","text":"<p>Denoising score-matching (DSM) solvers and sampling utilities.</p>"},{"location":"api/pfi/score/solvers/_dsm/#pfi.score.solvers._dsm.DSM_","title":"<code>DSM_(dist, times, net, L=10, n_epochs=2000, bs=None, adp_flag=0, lr=0.0001, device='cpu', verbose=True)</code>","text":"<p>Train a score network with denoising score matching.</p> <p>Parameters:</p> <ul> <li> <code>dist</code>               (<code>list of torch.Tensor, length nsnaps</code>)           \u2013            <p><code>dist[k]</code> has shape <code>(n_k, ndim)</code>.</p> </li> <li> <code>times</code>               (<code>torch.Tensor of shape (nsnaps,)</code>)           \u2013            <p>Snapshot times.</p> </li> <li> <code>net</code>               (<code>Module</code>)           \u2013            <p>Score network that accepts inputs of shape <code>(nsnaps, L, batch_size, ndim + 2)</code> after batching.</p> </li> <li> <code>L</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of noise levels.</p> </li> <li> <code>n_epochs</code>               (<code>int</code>, default:                   <code>2000</code> )           \u2013            <p>Number of optimization epochs.</p> </li> <li> <code>bs</code>               (<code>int or None</code>, default:                   <code>None</code> )           \u2013            <p>Mini-batch size over sample dimension. If <code>None</code>, uses all samples.</p> </li> <li> <code>adp_flag</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>If set to <code>1</code>, enable adaptive per-time weighting.</p> </li> <li> <code>lr</code>               (<code>float</code>, default:                   <code>1e-4</code> )           \u2013            <p>Learning rate.</p> </li> <li> <code>device</code>               (<code>str or device</code>, default:                   <code>'cpu'</code> )           \u2013            <p>Training device.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, show progress bars and diagnostics.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>net</code> (              <code>Module</code> )          \u2013            <p>Trained score network.</p> </li> <li> <code>loss_hist</code> (              <code>list of float</code> )          \u2013            <p>Epoch-wise training loss values.</p> </li> </ul> Source code in <code>pfi/score/solvers/_dsm.py</code> <pre><code>def DSM_(\n    dist,\n    times,\n    net,\n    L=10,\n    n_epochs=2000,\n    bs=None,\n    adp_flag=0,\n    lr=1e-4,\n    device=\"cpu\",\n    verbose=True,\n):\n    \"\"\"Train a score network with denoising score matching.\n\n    Parameters\n    ----------\n    dist : list of torch.Tensor, length nsnaps\n        ``dist[k]`` has shape ``(n_k, ndim)``.\n    times : torch.Tensor of shape (nsnaps,)\n        Snapshot times.\n    net : torch.nn.Module\n        Score network that accepts inputs of shape\n        ``(nsnaps, L, batch_size, ndim + 2)`` after batching.\n    L : int, default=10\n        Number of noise levels.\n    n_epochs : int, default=2000\n        Number of optimization epochs.\n    bs : int or None, default=None\n        Mini-batch size over sample dimension. If ``None``, uses all samples.\n    adp_flag : int, default=0\n        If set to ``1``, enable adaptive per-time weighting.\n    lr : float, default=1e-4\n        Learning rate.\n    device : str or torch.device, default='cpu'\n        Training device.\n    verbose : bool, default=True\n        If ``True``, show progress bars and diagnostics.\n\n    Returns\n    -------\n    net : torch.nn.Module\n        Trained score network.\n    loss_hist : list of float\n        Epoch-wise training loss values.\n    \"\"\"\n    dist = torch.stack(subsample_shuffle(dist), dim=0)\n\n    nsnaps, nsamples, ndim = dist.shape\n    if bs is None:\n        bs = nsamples\n\n    x_train, x_data, x_mean, x_std, sigma = generate_noisy_training_data_batch(\n        dist,\n        ndim,\n        times,\n        L,\n        nsamples,\n        nsnaps,\n        device,\n    )\n\n    net.set_scales(x_mean.to(device), x_std.to(device))\n\n    xtrain = torch.tensor(x_train, dtype=torch.float32).to(device)\n    xdata = torch.tensor(x_data, dtype=torch.float32).to(device)\n\n    loader = FastTensorDataLoader(\n        torch.permute(xtrain, (2, 0, 1, 3)),\n        torch.permute(xdata, (1, 0, 2)),\n        batch_size=bs,\n        shuffle=True,\n    )\n\n    optimizer = torch.optim.Adam(list(net.parameters()), lr=lr)\n    scheduler = MultiStepLR(optimizer, milestones=[2500, 6500, 8500], gamma=0.1)\n\n    c_ = torch.ones((nsnaps,), dtype=torch.float32, device=device)\n    alpha_ann = 0.5\n    adapt_int = 10\n    weight_decay = 1e-4\n\n    sigma_tensor = torch.tensor(sigma, dtype=torch.float32, device=x_train.device)\n    sigma_reshaped = sigma_tensor.view(L, 1, 1)\n    pbar = tqdm(range(n_epochs), desc=\"DSM\", dynamic_ncols=True, disable=not verbose)\n    loss_hist = []\n\n    for epoch in pbar:\n        for _, (xbatch, ybatch) in enumerate(loader):\n            optimizer.zero_grad()\n\n            xbatch = xbatch.clone().detach().requires_grad_(True)\n            ybatch = ybatch.clone().detach().requires_grad_(True)\n\n            xbatch = torch.permute(xbatch, (1, 2, 0, 3))\n            ybatch = torch.permute(ybatch, (1, 0, 2))\n\n            uhat = net(xbatch)\n            lcomp = torch.zeros((nsnaps,), device=device)\n            std_ = torch.zeros((nsnaps,), device=device)\n\n            for tind in range(nsnaps):\n                u_pred = uhat[tind]\n                x_t = xbatch[tind, :, :, :ndim]\n                x_data = ybatch[tind, :, :ndim][np.newaxis, :, :]\n\n                u_true = (x_t - x_data) / (sigma_reshaped ** 2)\n                residual = u_pred + u_true\n                residual_squared = residual.pow(2).mean(dim=(1, 2))\n                loss_sum = 0.5 * torch.sum((sigma_tensor ** 2) * residual_squared)\n\n                if adp_flag == 1 and epoch % adapt_int == 0:\n                    with torch.no_grad():\n                        std_[tind] = loss_grad_std(loss_sum, net, device)\n\n                lcomp[tind] = loss_sum\n\n            if adp_flag == 1 and epoch % adapt_int == 0:\n                with torch.no_grad():\n                    lamb_hat = torch.max(std_) / std_\n                    c_ = (1 - alpha_ann) * c_ + alpha_ann * lamb_hat\n                    c_ = c_ / torch.sum(c_)\n\n            loss = sum(c_[tind] * lcomp[tind] for tind in range(nsnaps))\n            weight_norm = sum((p ** 2).sum() for p in net.parameters())\n            loss = loss + weight_decay * weight_norm\n\n            loss.backward()\n            optimizer.step()\n\n        scheduler.step()\n        loss_hist.append(loss.item())\n        if verbose:\n            pbar.set_postfix(loss=f\"{loss.item():.3e}\", lr=f\"{optimizer.param_groups[0]['lr']:.2e}\")\n            if epoch % 500 == 0:\n                tqdm.write(f\"epoch: {epoch} c_: {c_.detach().cpu().numpy()}\")\n\n    return net, loss_hist\n</code></pre>"},{"location":"api/pfi/score/solvers/_dsm/#pfi.score.solvers._dsm.generate_data_DSM","title":"<code>generate_data_DSM(maxiter, infNet, nsamples, init_, time_, L, ndim, device)</code>","text":"<p>Sample from a trained DSM model using annealed Langevin dynamics.</p> <p>Parameters:</p> <ul> <li> <code>maxiter</code>               (<code>int</code>)           \u2013            <p>Number of Langevin steps per noise level.</p> </li> <li> <code>infNet</code>               (<code>Module</code>)           \u2013            <p>Trained score model.</p> </li> <li> <code>nsamples</code>               (<code>int</code>)           \u2013            <p>Number of samples to generate.</p> </li> <li> <code>init_</code>               (<code>ndarray of shape (nsamples, ndim + 2)</code>)           \u2013            <p>Initial states including noise-level and time columns.</p> </li> <li> <code>time_</code>               (<code>float</code>)           \u2013            <p>Time value assigned to all generated samples.</p> </li> <li> <code>L</code>               (<code>int</code>)           \u2013            <p>Number of noise levels.</p> </li> <li> <code>ndim</code>               (<code>int</code>)           \u2013            <p>State dimension.</p> </li> <li> <code>device</code>               (<code>str or device</code>)           \u2013            <p>Device used for generation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>sol</code> (              <code>ndarray of shape (nsamples, ndim + 2)</code> )          \u2013            <p>Generated states including auxiliary columns.</p> </li> <li> <code>gen_mean</code> (              <code>torch.Tensor of shape (ndim,)</code> )          \u2013            <p>Mean of generated state coordinates.</p> </li> </ul> Source code in <code>pfi/score/solvers/_dsm.py</code> <pre><code>def generate_data_DSM(\n    maxiter,\n    infNet,\n    nsamples,\n    init_,\n    time_,\n    L,\n    ndim,\n    device,\n):\n    \"\"\"Sample from a trained DSM model using annealed Langevin dynamics.\n\n    Parameters\n    ----------\n    maxiter : int\n        Number of Langevin steps per noise level.\n    infNet : torch.nn.Module\n        Trained score model.\n    nsamples : int\n        Number of samples to generate.\n    init_ : ndarray of shape (nsamples, ndim + 2)\n        Initial states including noise-level and time columns.\n    time_ : float\n        Time value assigned to all generated samples.\n    L : int\n        Number of noise levels.\n    ndim : int\n        State dimension.\n    device : str or torch.device\n        Device used for generation.\n\n    Returns\n    -------\n    sol : ndarray of shape (nsamples, ndim + 2)\n        Generated states including auxiliary columns.\n    gen_mean : torch.Tensor of shape (ndim,)\n        Mean of generated state coordinates.\n    \"\"\"\n    eps = 1e-4\n    sol = torch.tensor(init_, dtype=torch.float32).to(device)\n    sol[:, ndim + 1] = time_\n\n    sigma = geometric_sequence(L)\n\n    for k in range(0, L):\n        alpha = eps * ((sigma[k] ** 2) / (sigma[L - 1] ** 2))\n        sol[:, ndim] = sigma[k]\n\n        for _ in range(0, maxiter):\n            z = torch.normal(0, 1, size=(nsamples, ndim)).to(device)\n            guru = infNet(sol)\n            sol[:, 0:ndim] = sol[:, 0:ndim] + 0.5 * alpha * guru + np.sqrt(alpha) * z\n\n    gen_mean = torch.mean(sol, axis=0)[0:ndim]\n    sol = sol.cpu().data.numpy().reshape(nsamples, ndim + 2)\n\n    return sol, gen_mean\n</code></pre>"},{"location":"api/pfi/score/solvers/_dsm/#pfi.score.solvers._dsm.generate_noisy_training_data_batch","title":"<code>generate_noisy_training_data_batch(Dist, ndim, tp, L, nsamples, nsnaps, device)</code>","text":"<p>Generate noisy DSM training inputs and normalization statistics.</p> <p>Parameters:</p> <ul> <li> <code>Dist</code>               (<code>torch.Tensor or ndarray of shape (nsnaps, nsamples, ndim)</code>)           \u2013            <p>Snapshot samples.</p> </li> <li> <code>ndim</code>               (<code>int</code>)           \u2013            <p>State dimension.</p> </li> <li> <code>tp</code>               (<code>array-like of shape (nsnaps,)</code>)           \u2013            <p>Snapshot times.</p> </li> <li> <code>L</code>               (<code>int</code>)           \u2013            <p>Number of noise levels.</p> </li> <li> <code>nsamples</code>               (<code>int</code>)           \u2013            <p>Number of samples per snapshot.</p> </li> <li> <code>nsnaps</code>               (<code>int</code>)           \u2013            <p>Number of snapshots.</p> </li> <li> <code>device</code>               (<code>str or device</code>)           \u2013            <p>Target device for returned tensors.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>x_train</code> (              <code>torch.Tensor of shape (nsnaps, L, nsamples, ndim + 2)</code> )          \u2013            <p>Noisy training inputs with appended noise level and time.</p> </li> <li> <code>x_data</code> (              <code>torch.Tensor of shape (nsnaps, nsamples, ndim)</code> )          \u2013            <p>Clean data tensor.</p> </li> <li> <code>x_mean</code> (              <code>torch.Tensor of shape (1, ndim + 2)</code> )          \u2013            <p>Feature-wise mean used for input normalization.</p> </li> <li> <code>x_std</code> (              <code>torch.Tensor of shape (1, ndim + 2)</code> )          \u2013            <p>Feature-wise standard deviation used for input normalization.</p> </li> <li> <code>sigma</code> (              <code>ndarray of shape (L,)</code> )          \u2013            <p>Noise schedule.</p> </li> </ul> Source code in <code>pfi/score/solvers/_dsm.py</code> <pre><code>def generate_noisy_training_data_batch(\n    Dist,\n    ndim,\n    tp,\n    L,\n    nsamples,\n    nsnaps,\n    device,\n):\n    \"\"\"Generate noisy DSM training inputs and normalization statistics.\n\n    Parameters\n    ----------\n    Dist : torch.Tensor or ndarray of shape (nsnaps, nsamples, ndim)\n        Snapshot samples.\n    ndim : int\n        State dimension.\n    tp : array-like of shape (nsnaps,)\n        Snapshot times.\n    L : int\n        Number of noise levels.\n    nsamples : int\n        Number of samples per snapshot.\n    nsnaps : int\n        Number of snapshots.\n    device : str or torch.device\n        Target device for returned tensors.\n\n    Returns\n    -------\n    x_train : torch.Tensor of shape (nsnaps, L, nsamples, ndim + 2)\n        Noisy training inputs with appended noise level and time.\n    x_data : torch.Tensor of shape (nsnaps, nsamples, ndim)\n        Clean data tensor.\n    x_mean : torch.Tensor of shape (1, ndim + 2)\n        Feature-wise mean used for input normalization.\n    x_std : torch.Tensor of shape (1, ndim + 2)\n        Feature-wise standard deviation used for input normalization.\n    sigma : ndarray of shape (L,)\n        Noise schedule.\n    \"\"\"\n    sigma = geometric_sequence(L)\n    transform_data = np.zeros((nsnaps, L, nsamples, ndim + 2))\n\n    for tind in range(nsnaps):\n        for i in range(0, L):\n            for j in range(0, nsamples):\n                mean = Dist[tind, j, 0:ndim]\n                cov = (sigma[i] ** 2) * np.eye(ndim)\n                transform_data[tind, i, j, 0:ndim] = np.random.multivariate_normal(mean, cov)\n                transform_data[tind, i, j, ndim] = sigma[i]\n                transform_data[tind, i, j, ndim + 1] = tp[tind]\n\n    x_train = torch.tensor(transform_data, dtype=torch.float32, requires_grad=True, device=device)\n    x_data = torch.tensor(Dist, dtype=torch.float32, device=device)\n\n    x_mean = torch.zeros((ndim + 2,))\n    x_std = torch.ones((ndim + 2,))\n    for i in range(0, ndim + 2):\n        x_mean[i] = torch.tensor(\n            np.mean(transform_data[:, :, :, i].flatten(), axis=0, keepdims=True),\n            dtype=torch.float32,\n        )\n        x_std[i] = torch.tensor(\n            np.std(transform_data[:, :, :, i].flatten(), axis=0, keepdims=True),\n            dtype=torch.float32,\n        )\n\n    x_mean = x_mean[np.newaxis, :]\n    x_std = x_std[np.newaxis, :]\n\n    return x_train, x_data, x_mean, x_std, sigma\n</code></pre>"},{"location":"api/pfi/score/solvers/_dsm/#pfi.score.solvers._dsm.geometric_sequence","title":"<code>geometric_sequence(L)</code>","text":"<p>Build the geometric noise schedule used by DSM.</p> <p>Parameters:</p> <ul> <li> <code>L</code>               (<code>int</code>)           \u2013            <p>Number of noise levels.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>sigma</code> (              <code>ndarray of shape (L,)</code> )          \u2013            <p>Geometrically decaying standard deviations.</p> </li> </ul> Source code in <code>pfi/score/solvers/_dsm.py</code> <pre><code>def geometric_sequence(\n    L,\n):\n    \"\"\"Build the geometric noise schedule used by DSM.\n\n    Parameters\n    ----------\n    L : int\n        Number of noise levels.\n\n    Returns\n    -------\n    sigma : ndarray of shape (L,)\n        Geometrically decaying standard deviations.\n    \"\"\"\n    r = np.exp((-2 / L) * np.log(10))\n    sigma = np.zeros((L,))\n    for i in range(0, L):\n        sigma[i] = 1 * r ** i\n    return sigma\n</code></pre>"},{"location":"api/pfi/score/solvers/_ssm/","title":"_ssm","text":""},{"location":"api/pfi/score/solvers/_ssm/#pfi.score.solvers._ssm","title":"<code>pfi.score.solvers._ssm</code>","text":"<p>Functions that should implement sliced score matching</p>"},{"location":"api/pfi/utils/","title":"utils","text":""},{"location":"api/pfi/utils/#pfi.utils","title":"<code>pfi.utils</code>","text":"<p>Public utility exports for simulations, data handling, and neural nets.</p> <p>This submodule collects helper functions and classes used across the package.</p>"},{"location":"api/pfi/utils/#pfi.utils.BatchNorm","title":"<code>BatchNorm(mean, std)</code>","text":"<p>               Bases: <code>object</code></p> <p>Simple affine normalizer <code>(x - mean) / std</code>.</p> <p>Parameters:</p> <ul> <li> <code>mean</code>               (<code>float or torch.Tensor of shape (n_features,) or (1, n_features)</code>)           \u2013            <p>Feature-wise mean.</p> </li> <li> <code>std</code>               (<code>float or torch.Tensor of shape (n_features,) or (1, n_features)</code>)           \u2013            <p>Feature-wise standard deviation.</p> </li> </ul> Source code in <code>pfi/utils/nns.py</code> <pre><code>def __init__(\n    self,\n    mean,\n    std,\n):\n    self.mean = mean\n    self.std = std\n</code></pre>"},{"location":"api/pfi/utils/#pfi.utils.DNN","title":"<code>DNN(sizes, mean=0, std=1, seed=0, activation=nn.Tanh())</code>","text":"<p>               Bases: <code>Module</code></p> <p>Fully-connected network with optional feature normalization.</p> <p>Parameters:</p> <ul> <li> <code>sizes</code>               (<code>list of int</code>)           \u2013            <p>Layer sizes including input and output dimensions.</p> </li> <li> <code>mean</code>               (<code>float or Tensor</code>, default:                   <code>0</code> )           \u2013            <p>Initial mean used by the internal normalizer.</p> </li> <li> <code>std</code>               (<code>float or Tensor</code>, default:                   <code>1</code> )           \u2013            <p>Initial standard deviation used by the internal normalizer.</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Random seed for layer initialization.</p> </li> <li> <code>activation</code>               (<code>Module</code>, default:                   <code>torch.nn.Tanh()</code> )           \u2013            <p>Hidden activation module.</p> </li> </ul> Source code in <code>pfi/utils/nns.py</code> <pre><code>def __init__(\n    self,\n    sizes,\n    mean=0,\n    std=1,\n    seed=0,\n    activation=nn.Tanh(),\n):\n    super(DNN, self).__init__()\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    self.bn = BatchNorm(mean, std)\n    layer = []\n    for i in range(len(sizes) - 2):\n        linear = LayerNoWN(sizes[i], sizes[i + 1], seed, activation)\n        layer += [linear, activation]\n    layer += [LayerNoWN(sizes[-2], sizes[-1], seed, activation)]\n    self.net = nn.Sequential(*layer)\n</code></pre>"},{"location":"api/pfi/utils/#pfi.utils.DNN.set_scales","title":"<code>set_scales(mean, std)</code>","text":"<p>Update normalization statistics.</p> <p>Parameters:</p> <ul> <li> <code>mean</code>               (<code>torch.Tensor of shape (1, n_features)</code>)           \u2013            <p>New feature means.</p> </li> <li> <code>std</code>               (<code>torch.Tensor of shape (1, n_features)</code>)           \u2013            <p>New feature standard deviations.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>self</code> (              <code>DNN</code> )          \u2013            <p>Estimator instance.</p> </li> </ul> Source code in <code>pfi/utils/nns.py</code> <pre><code>def set_scales(self, mean, std):\n    \"\"\"Update normalization statistics.\n\n    Parameters\n    ----------\n    mean : torch.Tensor of shape (1, n_features)\n        New feature means.\n    std : torch.Tensor of shape (1, n_features)\n        New feature standard deviations.\n\n    Returns\n    -------\n    self : DNN\n        Estimator instance.\n    \"\"\"\n    self.bn = BatchNorm(mean, std)\n    return self\n</code></pre>"},{"location":"api/pfi/utils/#pfi.utils.FastTensorDataLoader","title":"<code>FastTensorDataLoader(*tensors, batch_size=32, shuffle=False)</code>","text":"<p>Lightweight mini-batch iterator over in-memory tensors.</p> <p>Parameters:</p> <ul> <li> <code>*tensors</code>               (<code>tuple of torch.Tensor</code>, default:                   <code>()</code> )           \u2013            <p>Tensors with matching first dimension.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>Number of samples per yielded batch.</p> </li> <li> <code>shuffle</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, shuffle samples at each iteration.</p> </li> </ul> Source code in <code>pfi/utils/nns.py</code> <pre><code>def __init__(\n    self,\n    *tensors,\n    batch_size=32,\n    shuffle=False,\n):\n    assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n    self.tensors = tensors\n    self.dataset_len = self.tensors[0].shape[0]\n    self.batch_size = batch_size\n    self.shuffle = shuffle\n    n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n    if remainder &gt; 0:\n        n_batches += 1\n    self.n_batches = n_batches\n</code></pre>"},{"location":"api/pfi/utils/#pfi.utils.FreezeVarDNN","title":"<code>FreezeVarDNN(dnn, var_index, var_value)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper that fixes one input feature to a constant before inference.</p> <p>Parameters:</p> <ul> <li> <code>dnn</code>               (<code>Module</code>)           \u2013            <p>Base network receiving <code>n_features</code> inputs.</p> </li> <li> <code>var_index</code>               (<code>int</code>)           \u2013            <p>Index of the feature to overwrite.</p> </li> <li> <code>var_value</code>               (<code>float</code>)           \u2013            <p>Constant value assigned to that feature.</p> </li> </ul> Source code in <code>pfi/utils/nns.py</code> <pre><code>def __init__(\n    self,\n    dnn,\n    var_index,\n    var_value,\n):\n    super(FreezeVarDNN, self).__init__()\n    self.dnn = dnn\n    self.var_index = var_index\n    self.var_value = var_value\n</code></pre>"},{"location":"api/pfi/utils/#pfi.utils.LayerNoWN","title":"<code>LayerNoWN(in_features, out_features, seed, activation)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Linear layer with Xavier initialization and no weight normalization.</p> <p>Parameters:</p> <ul> <li> <code>in_features</code>               (<code>int</code>)           \u2013            <p>Input feature dimension.</p> </li> <li> <code>out_features</code>               (<code>int</code>)           \u2013            <p>Output feature dimension.</p> </li> <li> <code>seed</code>               (<code>int</code>)           \u2013            <p>Random seed used for initialization.</p> </li> <li> <code>activation</code>               (<code>Module</code>)           \u2013            <p>Activation used in surrounding network to set initialization gain.</p> </li> </ul> Source code in <code>pfi/utils/nns.py</code> <pre><code>def __init__(\n    self,\n    in_features,\n    out_features,\n    seed,\n    activation,\n):\n    super(LayerNoWN, self).__init__()\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    self.linear = nn.Linear(in_features=in_features, out_features=out_features)\n    gain = 5 / 3 if isinstance(activation, nn.Tanh) else 1\n    nn.init.xavier_normal_(self.linear.weight, gain=gain)\n    nn.init.zeros_(self.linear.bias)\n</code></pre>"},{"location":"api/pfi/utils/#pfi.utils.SpectralNormDNN","title":"<code>SpectralNormDNN(sizes, mean=0, std=1, seed=0, activation=nn.Tanh())</code>","text":"<p>               Bases: <code>Module</code></p> <p>Fully-connected network with spectral normalization on hidden layers.</p> <p>Parameters:</p> <ul> <li> <code>sizes</code>               (<code>list of int</code>)           \u2013            <p>Layer sizes including input and output dimensions.</p> </li> <li> <code>mean</code>               (<code>float or Tensor</code>, default:                   <code>0</code> )           \u2013            <p>Initial mean used by the internal normalizer.</p> </li> <li> <code>std</code>               (<code>float or Tensor</code>, default:                   <code>1</code> )           \u2013            <p>Initial standard deviation used by the internal normalizer.</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Random seed for layer initialization.</p> </li> <li> <code>activation</code>               (<code>Module</code>, default:                   <code>torch.nn.Tanh()</code> )           \u2013            <p>Hidden activation module.</p> </li> </ul> Source code in <code>pfi/utils/nns.py</code> <pre><code>def __init__(\n    self,\n    sizes,\n    mean=0,\n    std=1,\n    seed=0,\n    activation=nn.Tanh(),\n):\n    super(SpectralNormDNN, self).__init__()\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    self.bn = BatchNorm(mean, std)\n    layers = []\n    for i in range(len(sizes) - 2):\n        linear = nn.Linear(sizes[i], sizes[i + 1])\n        linear = nn_utils.spectral_norm(linear)\n        layers.append(linear)\n        layers.append(activation)\n    final_linear = nn.Linear(sizes[-2], sizes[-1])\n    layers.append(final_linear)\n    self.net = nn.Sequential(*layers)\n</code></pre>"},{"location":"api/pfi/utils/#pfi.utils.SpectralNormDNN.set_scales","title":"<code>set_scales(mean, std)</code>","text":"<p>Update normalization statistics.</p> <p>Parameters:</p> <ul> <li> <code>mean</code>               (<code>torch.Tensor of shape (1, n_features)</code>)           \u2013            <p>New feature means.</p> </li> <li> <code>std</code>               (<code>torch.Tensor of shape (1, n_features)</code>)           \u2013            <p>New feature standard deviations.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>self</code> (              <code>DNN</code> )          \u2013            <p>Estimator instance.</p> </li> </ul> Source code in <code>pfi/utils/nns.py</code> <pre><code>def set_scales(self, mean, std):\n    \"\"\"Update normalization statistics.\n\n    Parameters\n    ----------\n    mean : torch.Tensor of shape (1, n_features)\n        New feature means.\n    std : torch.Tensor of shape (1, n_features)\n        New feature standard deviations.\n\n    Returns\n    -------\n    self : DNN\n        Estimator instance.\n    \"\"\"\n    self.bn = BatchNorm(mean, std)\n    return self\n</code></pre>"},{"location":"api/pfi/utils/#pfi.utils.X_from_snapshots","title":"<code>X_from_snapshots(snaps, times)</code>","text":"<p>Stack snapshots into a single array with appended time column.</p> <p>Parameters:</p> <ul> <li> <code>snaps</code>               (<code>list of ndarray, length n_snaps</code>)           \u2013            <p><code>snaps[k]</code> has shape <code>(n_k, ndim)</code>.</p> </li> <li> <code>times</code>               (<code>array-like of shape (n_snaps,)</code>)           \u2013            <p>Snapshot times aligned with <code>snaps</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>X</code> (              <code>ndarray of shape (sum_k n_k, ndim + 1)</code> )          \u2013            <p>Concatenated dataset where last column stores time.</p> </li> </ul> Source code in <code>pfi/utils/data.py</code> <pre><code>def X_from_snapshots(\n    snaps,\n    times,\n):\n    \"\"\"Stack snapshots into a single array with appended time column.\n\n    Parameters\n    ----------\n    snaps : list of ndarray, length n_snaps\n        ``snaps[k]`` has shape ``(n_k, ndim)``.\n    times : array-like of shape (n_snaps,)\n        Snapshot times aligned with ``snaps``.\n\n    Returns\n    -------\n    X : ndarray of shape (sum_k n_k, ndim + 1)\n        Concatenated dataset where last column stores time.\n    \"\"\"\n    X_list = []\n    for k, t in enumerate(times):\n        xk = snaps[k]\n        tk = t * np.ones((xk.shape[0], 1))\n        X_list.append(np.hstack([xk, tk]))\n\n    return np.vstack(X_list)\n</code></pre>"},{"location":"api/pfi/utils/#pfi.utils.divergence","title":"<code>divergence(field, x)</code>","text":"<p>Compute coordinate-wise divergence terms of a vector field.</p> <p>Parameters:</p> <ul> <li> <code>field</code>               (<code>torch.Tensor of shape (batch_size, ndim)</code>)           \u2013            <p>Vector field evaluated at <code>x</code>.</p> </li> <li> <code>x</code>               (<code>torch.Tensor of shape (batch_size, ndim)</code>)           \u2013            <p>Input points with gradient tracking enabled.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>div</code> (              <code>torch.Tensor of shape (batch_size, ndim)</code> )          \u2013            <p>Diagonal Jacobian terms, one per coordinate.</p> </li> </ul> Source code in <code>pfi/utils/nns.py</code> <pre><code>def divergence(\n    field,\n    x,\n):\n    \"\"\"Compute coordinate-wise divergence terms of a vector field.\n\n    Parameters\n    ----------\n    field : torch.Tensor of shape (batch_size, ndim)\n        Vector field evaluated at ``x``.\n    x : torch.Tensor of shape (batch_size, ndim)\n        Input points with gradient tracking enabled.\n\n    Returns\n    -------\n    div : torch.Tensor of shape (batch_size, ndim)\n        Diagonal Jacobian terms, one per coordinate.\n    \"\"\"\n    dim = field.shape[1]\n    div = torch.zeros((field.shape[0], dim), device=x.device)\n    for i in range(dim):\n        out_ = field[:, i]\n        gradient = grad(\n            outputs=out_,\n            inputs=x,\n            grad_outputs=torch.ones_like(out_),\n            create_graph=True,\n        )[0]\n        div[:, i] = gradient[:, i]\n    return div\n</code></pre>"},{"location":"api/pfi/utils/#pfi.utils.g_rate","title":"<code>g_rate(x1, x2, gr)</code>","text":"<p>Compute the cell growth rate used in the toggle-switch simulator.</p> <p>Parameters:</p> <ul> <li> <code>x1</code>               (<code>ndarray of shape (n_samples,)</code>)           \u2013            <p>First coordinate of the state.</p> </li> <li> <code>x2</code>               (<code>ndarray of shape (n_samples,)</code>)           \u2013            <p>Second coordinate of the state.</p> </li> <li> <code>gr</code>               (<code>float</code>)           \u2013            <p>Growth-rate scale.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>rate</code> (              <code>ndarray of shape (n_samples,)</code> )          \u2013            <p>Growth rate for each sample.</p> </li> </ul> Source code in <code>pfi/utils/simulations.py</code> <pre><code>def g_rate(\n    x1,\n    x2,\n    gr,\n):\n    \"\"\"Compute the cell growth rate used in the toggle-switch simulator.\n\n    Parameters\n    ----------\n    x1 : ndarray of shape (n_samples,)\n        First coordinate of the state.\n    x2 : ndarray of shape (n_samples,)\n        Second coordinate of the state.\n    gr : float\n        Growth-rate scale.\n\n    Returns\n    -------\n    rate : ndarray of shape (n_samples,)\n        Growth rate for each sample.\n    \"\"\"\n    return gr * (1.0 * (x2 ** 2) / (1 + x2 ** 2) + 0.0 * (x1 ** 2) / (1 + x1 ** 2))\n</code></pre>"},{"location":"api/pfi/utils/#pfi.utils.load_data","title":"<code>load_data(path, nsamples, genes, time_key, cell_type_key, seed=0)</code>","text":"<p>Load AnnData snapshots and sample a fixed number of cells per time.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>Path to the <code>.h5ad</code> dataset.</p> </li> <li> <code>nsamples</code>               (<code>int</code>)           \u2013            <p>Number of cells sampled at each time point.</p> </li> <li> <code>genes</code>               (<code>list of str</code>)           \u2013            <p>Selected genes used to build expression snapshots.</p> </li> <li> <code>time_key</code>               (<code>str</code>)           \u2013            <p>Name of the observation column storing time labels.</p> </li> <li> <code>cell_type_key</code>               (<code>str</code>)           \u2013            <p>Name of the observation column storing cell-type labels.</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Random seed for snapshot subsampling.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>ndarray of shape (n_times, nsamples, n_genes)</code> )          \u2013            <p>Subsampled expression snapshots.</p> </li> <li> <code>unique_times</code> (              <code>ndarray of shape (n_times,)</code> )          \u2013            <p>Unique times present in the dataset.</p> </li> <li> <code>ind_array</code> (              <code>ndarray of shape (n_times, nsamples)</code> )          \u2013            <p>Encoded cell-type labels for sampled cells.</p> </li> <li> <code>cell_types</code> (              <code>Series</code> )          \u2013            <p>Full cell-type annotation column.</p> </li> </ul> Source code in <code>pfi/utils/data.py</code> <pre><code>def load_data(\n    path,\n    nsamples,\n    genes,\n    time_key,\n    cell_type_key,\n    seed=0,\n):\n    \"\"\"Load AnnData snapshots and sample a fixed number of cells per time.\n\n    Parameters\n    ----------\n    path : str\n        Path to the ``.h5ad`` dataset.\n    nsamples : int\n        Number of cells sampled at each time point.\n    genes : list of str\n        Selected genes used to build expression snapshots.\n    time_key : str\n        Name of the observation column storing time labels.\n    cell_type_key : str\n        Name of the observation column storing cell-type labels.\n    seed : int, default=0\n        Random seed for snapshot subsampling.\n\n    Returns\n    -------\n    samples : ndarray of shape (n_times, nsamples, n_genes)\n        Subsampled expression snapshots.\n    unique_times : ndarray of shape (n_times,)\n        Unique times present in the dataset.\n    ind_array : ndarray of shape (n_times, nsamples)\n        Encoded cell-type labels for sampled cells.\n    cell_types : pandas.Series\n        Full cell-type annotation column.\n    \"\"\"\n    n_genes = len(genes)\n    import scanpy as sc\n\n    adata = sc.read_h5ad(path)\n\n    unique_times = np.asarray(adata.obs[time_key].unique())\n    samples = np.zeros((len(unique_times), nsamples, n_genes), dtype=np.float32)\n\n    cell_type_categories = list(adata.obs[cell_type_key].cat.categories)\n    cell_type_to_int = {ct: i for i, ct in enumerate(cell_type_categories)}\n    ind_array = np.zeros((len(unique_times), nsamples), dtype=int)\n    rng = np.random.default_rng(seed)\n\n    for k, time_point in enumerate(unique_times):\n        cells_at_time = adata[adata.obs[time_key] == time_point]\n        expr = cells_at_time[:, genes].X\n        expr = expr.toarray() if hasattr(expr, \"toarray\") else np.asarray(expr)\n\n        n_cells = expr.shape[0]\n        if n_cells &gt;= nsamples:\n            selected = rng.choice(n_cells, size=nsamples, replace=False)\n        else:\n            selected = rng.choice(n_cells, size=nsamples, replace=True)\n\n        cell_types = cells_at_time.obs[cell_type_key].iloc[selected].values\n        ind_array[k, :] = [cell_type_to_int[ct] for ct in cell_types]\n        samples[k, :, :] = expr[selected, :]\n\n    return samples, unique_times, ind_array, adata.obs[cell_type_key]\n</code></pre>"},{"location":"api/pfi/utils/#pfi.utils.loss_grad_std","title":"<code>loss_grad_std(loss, net, device)</code>","text":"<p>Estimate the pooled standard deviation of layer gradients.</p> <p>Parameters:</p> <ul> <li> <code>loss</code>               (<code>torch.Tensor of shape ()</code>)           \u2013            <p>Scalar loss value.</p> </li> <li> <code>net</code>               (<code>Module</code>)           \u2013            <p>Network containing linear layers.</p> </li> <li> <code>device</code>               (<code>str or device</code>)           \u2013            <p>Device used for intermediate tensors.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>std</code> (              <code>torch.Tensor of shape ()</code> )          \u2013            <p>Pooled gradient standard deviation across linear layer parameters.</p> </li> </ul> Source code in <code>pfi/utils/nns.py</code> <pre><code>def loss_grad_std(\n    loss,\n    net,\n    device,\n):\n    \"\"\"Estimate the pooled standard deviation of layer gradients.\n\n    Parameters\n    ----------\n    loss : torch.Tensor of shape ()\n        Scalar loss value.\n    net : torch.nn.Module\n        Network containing linear layers.\n    device : str or torch.device\n        Device used for intermediate tensors.\n\n    Returns\n    -------\n    std : torch.Tensor of shape ()\n        Pooled gradient standard deviation across linear layer parameters.\n    \"\"\"\n    var = []\n    siz = []\n    for m in net.modules():\n        if not isinstance(m, nn.Linear):\n            continue\n        w = grad(loss, m.weight, retain_graph=True, allow_unused=True)[0]\n        b = grad(loss, m.bias, retain_graph=True, allow_unused=True)[0]\n        if w is None or b is None:\n            continue\n        wb = torch.cat((w.view(-1), b))\n        nit = torch.numel(wb)\n        var.append((nit - 1) * torch.var(wb))\n        siz.append(nit)\n    vart = torch.tensor(var, dtype=torch.float32, device=device)\n    sizt = torch.tensor(siz, dtype=torch.float32, device=device)\n    return torch.sqrt(torch.sum(vart) / (torch.sum(sizt) - len(sizt)))\n</code></pre>"},{"location":"api/pfi/utils/#pfi.utils.simulate_ornstein_uhlenbeck","title":"<code>simulate_ornstein_uhlenbeck(Om, D, m0, S0, nsamples, ndim, Dt, K, dt=0.006)</code>","text":"<p>Simulate snapshots from a linear Ornstein-Uhlenbeck process.</p> <p>Parameters:</p> <ul> <li> <code>Om</code>               (<code>ndarray of shape (ndim, ndim)</code>)           \u2013            <p>Drift matrix.</p> </li> <li> <code>D</code>               (<code>ndarray of shape (ndim, ndim)</code>)           \u2013            <p>Diffusion matrix.</p> </li> <li> <code>m0</code>               (<code>ndarray of shape (ndim,)</code>)           \u2013            <p>Mean of the initial Gaussian distribution.</p> </li> <li> <code>S0</code>               (<code>float</code>)           \u2013            <p>Isotropic variance factor for the initial covariance <code>S0 * I</code>.</p> </li> <li> <code>nsamples</code>               (<code>int</code>)           \u2013            <p>Number of particles sampled at each snapshot time.</p> </li> <li> <code>ndim</code>               (<code>int</code>)           \u2013            <p>State dimension.</p> </li> <li> <code>Dt</code>               (<code>float</code>)           \u2013            <p>Snapshot interval in simulation time.</p> </li> <li> <code>K</code>               (<code>int</code>)           \u2013            <p>Number of snapshots.</p> </li> <li> <code>dt</code>               (<code>float</code>, default:                   <code>0.006</code> )           \u2013            <p>Euler-Maruyama integration step.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>list of length K</code> )          \u2013            <p><code>samples[k]</code> is an ndarray of shape <code>(nsamples, ndim)</code>.</p> </li> <li> <code>tt</code> (              <code>ndarray of shape (K,)</code> )          \u2013            <p>Snapshot times.</p> </li> </ul> Source code in <code>pfi/utils/simulations.py</code> <pre><code>def simulate_ornstein_uhlenbeck(\n    Om,\n    D,\n    m0,\n    S0,\n    nsamples,\n    ndim,\n    Dt,\n    K,\n    dt=0.006,\n):\n    \"\"\"Simulate snapshots from a linear Ornstein-Uhlenbeck process.\n\n    Parameters\n    ----------\n    Om : ndarray of shape (ndim, ndim)\n        Drift matrix.\n    D : ndarray of shape (ndim, ndim)\n        Diffusion matrix.\n    m0 : ndarray of shape (ndim,)\n        Mean of the initial Gaussian distribution.\n    S0 : float\n        Isotropic variance factor for the initial covariance ``S0 * I``.\n    nsamples : int\n        Number of particles sampled at each snapshot time.\n    ndim : int\n        State dimension.\n    Dt : float\n        Snapshot interval in simulation time.\n    K : int\n        Number of snapshots.\n    dt : float, default=0.006\n        Euler-Maruyama integration step.\n\n    Returns\n    -------\n    samples : list of length K\n        ``samples[k]`` is an ndarray of shape ``(nsamples, ndim)``.\n    tt : ndarray of shape (K,)\n        Snapshot times.\n    \"\"\"\n    samples = []\n    tt = np.zeros((K,))\n    record = int(Dt / dt)\n\n    for j in range(1, K + 1):\n        traj_init = np.random.multivariate_normal(m0, S0 * np.eye(ndim), size=nsamples).T\n        traj = traj_init.copy()\n\n        for i in range(0, j * record):\n            xi = np.random.normal(0, 1, (ndim, nsamples))\n            traj = traj - (Om @ traj) * dt + (np.sqrt(2 * D) @ xi) * np.sqrt(dt)\n\n        tt[j - 1] = dt * i\n        samples.append(traj.T.copy())\n\n    return samples, tt\n</code></pre>"},{"location":"api/pfi/utils/#pfi.utils.simulate_toggle_switch","title":"<code>simulate_toggle_switch(nsamples, init, nsnaps, ndim, seed, maxiter, model_params, vol, gr, growth_flag=False)</code>","text":"<p>Simulate stochastic toggle-switch dynamics with optional growth.</p> <p>Parameters:</p> <ul> <li> <code>nsamples</code>               (<code>int</code>)           \u2013            <p>Initial number of particles.</p> </li> <li> <code>init</code>               (<code>ndarray of shape (ndim, nsamples)</code>)           \u2013            <p>Initial state matrix.</p> </li> <li> <code>nsnaps</code>               (<code>int</code>)           \u2013            <p>Number of snapshot times.</p> </li> <li> <code>ndim</code>               (<code>int</code>)           \u2013            <p>State dimension.</p> </li> <li> <code>seed</code>               (<code>int</code>)           \u2013            <p>Random seed.</p> </li> <li> <code>maxiter</code>               (<code>int</code>)           \u2013            <p>Number of discrete integration iterations.</p> </li> <li> <code>model_params</code>               (<code>array-like of shape (7,)</code>)           \u2013            <p>Toggle-switch parameters <code>[a1, a2, b1, b2, k1, k2, n]</code>.</p> </li> <li> <code>vol</code>               (<code>float</code>)           \u2013            <p>System volume scaling the stochastic term.</p> </li> <li> <code>gr</code>               (<code>float</code>)           \u2013            <p>Growth-rate scale used when <code>growth_flag=True</code>.</p> </li> <li> <code>growth_flag</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, cells are duplicated according to growth probabilities.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples_full</code> (              <code>list of length nsnaps</code> )          \u2013            <p><code>samples_full[k]</code> is an ndarray of shape <code>(n_k, ndim)</code>, where <code>n_k</code> can increase across snapshots when growth is enabled.</p> </li> <li> <code>tt</code> (              <code>ndarray of shape (nsnaps,)</code> )          \u2013            <p>Snapshot times.</p> </li> </ul> Source code in <code>pfi/utils/simulations.py</code> <pre><code>def simulate_toggle_switch(\n    nsamples,\n    init,\n    nsnaps,\n    ndim,\n    seed,\n    maxiter,\n    model_params,\n    vol,\n    gr,\n    growth_flag=False,\n):\n    \"\"\"Simulate stochastic toggle-switch dynamics with optional growth.\n\n    Parameters\n    ----------\n    nsamples : int\n        Initial number of particles.\n    init : ndarray of shape (ndim, nsamples)\n        Initial state matrix.\n    nsnaps : int\n        Number of snapshot times.\n    ndim : int\n        State dimension.\n    seed : int\n        Random seed.\n    maxiter : int\n        Number of discrete integration iterations.\n    model_params : array-like of shape (7,)\n        Toggle-switch parameters ``[a1, a2, b1, b2, k1, k2, n]``.\n    vol : float\n        System volume scaling the stochastic term.\n    gr : float\n        Growth-rate scale used when ``growth_flag=True``.\n    growth_flag : bool, default=False\n        If ``True``, cells are duplicated according to growth probabilities.\n\n    Returns\n    -------\n    samples_full : list of length nsnaps\n        ``samples_full[k]`` is an ndarray of shape ``(n_k, ndim)``, where\n        ``n_k`` can increase across snapshots when growth is enabled.\n    tt : ndarray of shape (nsnaps,)\n        Snapshot times.\n    \"\"\"\n    dt = 0.01\n    lx = 1.0\n    np.random.seed(seed)\n\n    tt = np.zeros(nsnaps)\n    samples_full = []\n    steps = int(maxiter / nsnaps)\n\n    for snap in range(nsnaps):\n        t_snap = snap * steps * dt\n        tt[snap] = t_snap\n\n        xold = init + 0.1 * np.random.normal(0, 1, (ndim, nsamples))\n\n        for _ in range(0, snap * steps + 1):\n            fval = toggle_switch(xold.T, model_params).T\n            noise = np.sqrt(fval + lx * xold) * np.sqrt(dt) * np.random.normal(0, 1, xold.shape)\n            xnew = xold + dt * (fval - lx * xold) + (1 / np.sqrt(vol)) * noise\n\n            xnew = np.where(xnew &lt; 0, xold, xnew)\n            xold = xnew + 0.0\n\n            if growth_flag:\n                x1 = xold[0]\n                x2 = xold[1]\n                growth_probs = g_rate(x1, x2, gr) * dt\n                divide_flags = np.random.rand(x2.shape[0]) &lt; growth_probs\n                new_cells = xold[:, divide_flags]\n                if new_cells.shape[1] &gt; 0:\n                    xold = np.concatenate([xold, new_cells], axis=1)\n\n        samples_full.append(xold.T.copy())\n\n    return samples_full, tt\n</code></pre>"},{"location":"api/pfi/utils/#pfi.utils.snapshots_from_X","title":"<code>snapshots_from_X(X)</code>","text":"<p>Split a time-augmented dataset into per-time snapshots.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>array-like of shape (n_samples_total, ndim + 1)</code>)           \u2013            <p>Input matrix with time stored in the last column.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>snaps</code> (              <code>list of torch.Tensor, length n_unique_times</code> )          \u2013            <p><code>snaps[k]</code> has shape <code>(n_k, ndim)</code>.</p> </li> <li> <code>times</code> (              <code>torch.Tensor of shape (n_unique_times,)</code> )          \u2013            <p>Sorted unique times found in <code>X</code>.</p> </li> </ul> Source code in <code>pfi/utils/data.py</code> <pre><code>def snapshots_from_X(\n    X,\n):\n    \"\"\"Split a time-augmented dataset into per-time snapshots.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples_total, ndim + 1)\n        Input matrix with time stored in the last column.\n\n    Returns\n    -------\n    snaps : list of torch.Tensor, length n_unique_times\n        ``snaps[k]`` has shape ``(n_k, ndim)``.\n    times : torch.Tensor of shape (n_unique_times,)\n        Sorted unique times found in ``X``.\n    \"\"\"\n    x = torch.tensor(X, dtype=torch.float32)\n    t = x[:, -1]\n    times = torch.unique(t)\n    times, _ = torch.sort(times)\n\n    snaps = []\n    for ti in times:\n        snaps.append(x[t == ti][:, :-1])\n\n    return snaps, times\n</code></pre>"},{"location":"api/pfi/utils/#pfi.utils.subsample_shuffle","title":"<code>subsample_shuffle(snaps)</code>","text":"<p>Shuffle each snapshot and subsample to a common sample count.</p> <p>Parameters:</p> <ul> <li> <code>snaps</code>               (<code>list of array-like, length n_snaps</code>)           \u2013            <p><code>snaps[k]</code> has shape <code>(n_k, ndim)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dist</code> (              <code>list of ndarray, length n_snaps</code> )          \u2013            <p>Shuffled snapshots with common shape <code>(n_min, ndim)</code> where <code>n_min = min_k n_k</code>.</p> </li> </ul> Source code in <code>pfi/utils/data.py</code> <pre><code>def subsample_shuffle(\n    snaps,\n):\n    \"\"\"Shuffle each snapshot and subsample to a common sample count.\n\n    Parameters\n    ----------\n    snaps : list of array-like, length n_snaps\n        ``snaps[k]`` has shape ``(n_k, ndim)``.\n\n    Returns\n    -------\n    dist : list of ndarray, length n_snaps\n        Shuffled snapshots with common shape ``(n_min, ndim)`` where\n        ``n_min = min_k n_k``.\n    \"\"\"\n    nsamples = min(s.shape[0] for s in snaps)\n    ndim = snaps[0].shape[1]\n    dist = []\n\n    for s in snaps:\n        ind = np.arange(s.shape[0])\n        np.random.shuffle(ind)\n        dist.append(s[ind[:nsamples], :ndim])\n\n    return dist\n</code></pre>"},{"location":"api/pfi/utils/#pfi.utils.toggle_switch","title":"<code>toggle_switch(x, model_params)</code>","text":"<p>Evaluate the deterministic toggle-switch production term.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>ndarray of shape (n_samples, 2)</code>)           \u2013            <p>State values, where each row is <code>[x1, x2]</code>.</p> </li> <li> <code>model_params</code>               (<code>array-like of shape (7,)</code>)           \u2013            <p>Model parameters <code>[a1, a2, b1, b2, k1, k2, n]</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>f</code> (              <code>ndarray of shape (n_samples, 2)</code> )          \u2013            <p>Deterministic production term for each sample.</p> </li> </ul> Source code in <code>pfi/utils/simulations.py</code> <pre><code>def toggle_switch(\n    x,\n    model_params,\n):\n    \"\"\"Evaluate the deterministic toggle-switch production term.\n\n    Parameters\n    ----------\n    x : ndarray of shape (n_samples, 2)\n        State values, where each row is ``[x1, x2]``.\n    model_params : array-like of shape (7,)\n        Model parameters ``[a1, a2, b1, b2, k1, k2, n]``.\n\n    Returns\n    -------\n    f : ndarray of shape (n_samples, 2)\n        Deterministic production term for each sample.\n    \"\"\"\n    a1, a2, b1, b2, k1, k2, n = model_params\n    xn = x ** n + 0.0\n    f = np.zeros_like(x)\n    f[:, 0] = (a1 * (xn[:, 0]) / (k1 ** n + xn[:, 0])) + b1 * (k1 ** n) / (k1 ** n + xn[:, 1])\n    f[:, 1] = (a2 * (xn[:, 1]) / (k2 ** n + xn[:, 1])) + b2 * (k2 ** n) / (k2 ** n + xn[:, 0])\n    return f\n</code></pre>"},{"location":"api/pfi/utils/data/","title":"data","text":""},{"location":"api/pfi/utils/data/#pfi.utils.data","title":"<code>pfi.utils.data</code>","text":"<p>Data reshaping and snapshot conversion helpers for PFI estimators.</p>"},{"location":"api/pfi/utils/data/#pfi.utils.data.X_from_snapshots","title":"<code>X_from_snapshots(snaps, times)</code>","text":"<p>Stack snapshots into a single array with appended time column.</p> <p>Parameters:</p> <ul> <li> <code>snaps</code>               (<code>list of ndarray, length n_snaps</code>)           \u2013            <p><code>snaps[k]</code> has shape <code>(n_k, ndim)</code>.</p> </li> <li> <code>times</code>               (<code>array-like of shape (n_snaps,)</code>)           \u2013            <p>Snapshot times aligned with <code>snaps</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>X</code> (              <code>ndarray of shape (sum_k n_k, ndim + 1)</code> )          \u2013            <p>Concatenated dataset where last column stores time.</p> </li> </ul> Source code in <code>pfi/utils/data.py</code> <pre><code>def X_from_snapshots(\n    snaps,\n    times,\n):\n    \"\"\"Stack snapshots into a single array with appended time column.\n\n    Parameters\n    ----------\n    snaps : list of ndarray, length n_snaps\n        ``snaps[k]`` has shape ``(n_k, ndim)``.\n    times : array-like of shape (n_snaps,)\n        Snapshot times aligned with ``snaps``.\n\n    Returns\n    -------\n    X : ndarray of shape (sum_k n_k, ndim + 1)\n        Concatenated dataset where last column stores time.\n    \"\"\"\n    X_list = []\n    for k, t in enumerate(times):\n        xk = snaps[k]\n        tk = t * np.ones((xk.shape[0], 1))\n        X_list.append(np.hstack([xk, tk]))\n\n    return np.vstack(X_list)\n</code></pre>"},{"location":"api/pfi/utils/data/#pfi.utils.data.load_data","title":"<code>load_data(path, nsamples, genes, time_key, cell_type_key, seed=0)</code>","text":"<p>Load AnnData snapshots and sample a fixed number of cells per time.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>Path to the <code>.h5ad</code> dataset.</p> </li> <li> <code>nsamples</code>               (<code>int</code>)           \u2013            <p>Number of cells sampled at each time point.</p> </li> <li> <code>genes</code>               (<code>list of str</code>)           \u2013            <p>Selected genes used to build expression snapshots.</p> </li> <li> <code>time_key</code>               (<code>str</code>)           \u2013            <p>Name of the observation column storing time labels.</p> </li> <li> <code>cell_type_key</code>               (<code>str</code>)           \u2013            <p>Name of the observation column storing cell-type labels.</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Random seed for snapshot subsampling.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>ndarray of shape (n_times, nsamples, n_genes)</code> )          \u2013            <p>Subsampled expression snapshots.</p> </li> <li> <code>unique_times</code> (              <code>ndarray of shape (n_times,)</code> )          \u2013            <p>Unique times present in the dataset.</p> </li> <li> <code>ind_array</code> (              <code>ndarray of shape (n_times, nsamples)</code> )          \u2013            <p>Encoded cell-type labels for sampled cells.</p> </li> <li> <code>cell_types</code> (              <code>Series</code> )          \u2013            <p>Full cell-type annotation column.</p> </li> </ul> Source code in <code>pfi/utils/data.py</code> <pre><code>def load_data(\n    path,\n    nsamples,\n    genes,\n    time_key,\n    cell_type_key,\n    seed=0,\n):\n    \"\"\"Load AnnData snapshots and sample a fixed number of cells per time.\n\n    Parameters\n    ----------\n    path : str\n        Path to the ``.h5ad`` dataset.\n    nsamples : int\n        Number of cells sampled at each time point.\n    genes : list of str\n        Selected genes used to build expression snapshots.\n    time_key : str\n        Name of the observation column storing time labels.\n    cell_type_key : str\n        Name of the observation column storing cell-type labels.\n    seed : int, default=0\n        Random seed for snapshot subsampling.\n\n    Returns\n    -------\n    samples : ndarray of shape (n_times, nsamples, n_genes)\n        Subsampled expression snapshots.\n    unique_times : ndarray of shape (n_times,)\n        Unique times present in the dataset.\n    ind_array : ndarray of shape (n_times, nsamples)\n        Encoded cell-type labels for sampled cells.\n    cell_types : pandas.Series\n        Full cell-type annotation column.\n    \"\"\"\n    n_genes = len(genes)\n    import scanpy as sc\n\n    adata = sc.read_h5ad(path)\n\n    unique_times = np.asarray(adata.obs[time_key].unique())\n    samples = np.zeros((len(unique_times), nsamples, n_genes), dtype=np.float32)\n\n    cell_type_categories = list(adata.obs[cell_type_key].cat.categories)\n    cell_type_to_int = {ct: i for i, ct in enumerate(cell_type_categories)}\n    ind_array = np.zeros((len(unique_times), nsamples), dtype=int)\n    rng = np.random.default_rng(seed)\n\n    for k, time_point in enumerate(unique_times):\n        cells_at_time = adata[adata.obs[time_key] == time_point]\n        expr = cells_at_time[:, genes].X\n        expr = expr.toarray() if hasattr(expr, \"toarray\") else np.asarray(expr)\n\n        n_cells = expr.shape[0]\n        if n_cells &gt;= nsamples:\n            selected = rng.choice(n_cells, size=nsamples, replace=False)\n        else:\n            selected = rng.choice(n_cells, size=nsamples, replace=True)\n\n        cell_types = cells_at_time.obs[cell_type_key].iloc[selected].values\n        ind_array[k, :] = [cell_type_to_int[ct] for ct in cell_types]\n        samples[k, :, :] = expr[selected, :]\n\n    return samples, unique_times, ind_array, adata.obs[cell_type_key]\n</code></pre>"},{"location":"api/pfi/utils/data/#pfi.utils.data.snapshots_from_X","title":"<code>snapshots_from_X(X)</code>","text":"<p>Split a time-augmented dataset into per-time snapshots.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>array-like of shape (n_samples_total, ndim + 1)</code>)           \u2013            <p>Input matrix with time stored in the last column.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>snaps</code> (              <code>list of torch.Tensor, length n_unique_times</code> )          \u2013            <p><code>snaps[k]</code> has shape <code>(n_k, ndim)</code>.</p> </li> <li> <code>times</code> (              <code>torch.Tensor of shape (n_unique_times,)</code> )          \u2013            <p>Sorted unique times found in <code>X</code>.</p> </li> </ul> Source code in <code>pfi/utils/data.py</code> <pre><code>def snapshots_from_X(\n    X,\n):\n    \"\"\"Split a time-augmented dataset into per-time snapshots.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples_total, ndim + 1)\n        Input matrix with time stored in the last column.\n\n    Returns\n    -------\n    snaps : list of torch.Tensor, length n_unique_times\n        ``snaps[k]`` has shape ``(n_k, ndim)``.\n    times : torch.Tensor of shape (n_unique_times,)\n        Sorted unique times found in ``X``.\n    \"\"\"\n    x = torch.tensor(X, dtype=torch.float32)\n    t = x[:, -1]\n    times = torch.unique(t)\n    times, _ = torch.sort(times)\n\n    snaps = []\n    for ti in times:\n        snaps.append(x[t == ti][:, :-1])\n\n    return snaps, times\n</code></pre>"},{"location":"api/pfi/utils/data/#pfi.utils.data.subsample_shuffle","title":"<code>subsample_shuffle(snaps)</code>","text":"<p>Shuffle each snapshot and subsample to a common sample count.</p> <p>Parameters:</p> <ul> <li> <code>snaps</code>               (<code>list of array-like, length n_snaps</code>)           \u2013            <p><code>snaps[k]</code> has shape <code>(n_k, ndim)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dist</code> (              <code>list of ndarray, length n_snaps</code> )          \u2013            <p>Shuffled snapshots with common shape <code>(n_min, ndim)</code> where <code>n_min = min_k n_k</code>.</p> </li> </ul> Source code in <code>pfi/utils/data.py</code> <pre><code>def subsample_shuffle(\n    snaps,\n):\n    \"\"\"Shuffle each snapshot and subsample to a common sample count.\n\n    Parameters\n    ----------\n    snaps : list of array-like, length n_snaps\n        ``snaps[k]`` has shape ``(n_k, ndim)``.\n\n    Returns\n    -------\n    dist : list of ndarray, length n_snaps\n        Shuffled snapshots with common shape ``(n_min, ndim)`` where\n        ``n_min = min_k n_k``.\n    \"\"\"\n    nsamples = min(s.shape[0] for s in snaps)\n    ndim = snaps[0].shape[1]\n    dist = []\n\n    for s in snaps:\n        ind = np.arange(s.shape[0])\n        np.random.shuffle(ind)\n        dist.append(s[ind[:nsamples], :ndim])\n\n    return dist\n</code></pre>"},{"location":"api/pfi/utils/nns/","title":"nns","text":""},{"location":"api/pfi/utils/nns/#pfi.utils.nns","title":"<code>pfi.utils.nns</code>","text":"<p>Neural-network building blocks and differential operators for PFI.</p>"},{"location":"api/pfi/utils/nns/#pfi.utils.nns.BatchNorm","title":"<code>BatchNorm(mean, std)</code>","text":"<p>               Bases: <code>object</code></p> <p>Simple affine normalizer <code>(x - mean) / std</code>.</p> <p>Parameters:</p> <ul> <li> <code>mean</code>               (<code>float or torch.Tensor of shape (n_features,) or (1, n_features)</code>)           \u2013            <p>Feature-wise mean.</p> </li> <li> <code>std</code>               (<code>float or torch.Tensor of shape (n_features,) or (1, n_features)</code>)           \u2013            <p>Feature-wise standard deviation.</p> </li> </ul> Source code in <code>pfi/utils/nns.py</code> <pre><code>def __init__(\n    self,\n    mean,\n    std,\n):\n    self.mean = mean\n    self.std = std\n</code></pre>"},{"location":"api/pfi/utils/nns/#pfi.utils.nns.DNN","title":"<code>DNN(sizes, mean=0, std=1, seed=0, activation=nn.Tanh())</code>","text":"<p>               Bases: <code>Module</code></p> <p>Fully-connected network with optional feature normalization.</p> <p>Parameters:</p> <ul> <li> <code>sizes</code>               (<code>list of int</code>)           \u2013            <p>Layer sizes including input and output dimensions.</p> </li> <li> <code>mean</code>               (<code>float or Tensor</code>, default:                   <code>0</code> )           \u2013            <p>Initial mean used by the internal normalizer.</p> </li> <li> <code>std</code>               (<code>float or Tensor</code>, default:                   <code>1</code> )           \u2013            <p>Initial standard deviation used by the internal normalizer.</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Random seed for layer initialization.</p> </li> <li> <code>activation</code>               (<code>Module</code>, default:                   <code>torch.nn.Tanh()</code> )           \u2013            <p>Hidden activation module.</p> </li> </ul> Source code in <code>pfi/utils/nns.py</code> <pre><code>def __init__(\n    self,\n    sizes,\n    mean=0,\n    std=1,\n    seed=0,\n    activation=nn.Tanh(),\n):\n    super(DNN, self).__init__()\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    self.bn = BatchNorm(mean, std)\n    layer = []\n    for i in range(len(sizes) - 2):\n        linear = LayerNoWN(sizes[i], sizes[i + 1], seed, activation)\n        layer += [linear, activation]\n    layer += [LayerNoWN(sizes[-2], sizes[-1], seed, activation)]\n    self.net = nn.Sequential(*layer)\n</code></pre>"},{"location":"api/pfi/utils/nns/#pfi.utils.nns.DNN.set_scales","title":"<code>set_scales(mean, std)</code>","text":"<p>Update normalization statistics.</p> <p>Parameters:</p> <ul> <li> <code>mean</code>               (<code>torch.Tensor of shape (1, n_features)</code>)           \u2013            <p>New feature means.</p> </li> <li> <code>std</code>               (<code>torch.Tensor of shape (1, n_features)</code>)           \u2013            <p>New feature standard deviations.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>self</code> (              <code>DNN</code> )          \u2013            <p>Estimator instance.</p> </li> </ul> Source code in <code>pfi/utils/nns.py</code> <pre><code>def set_scales(self, mean, std):\n    \"\"\"Update normalization statistics.\n\n    Parameters\n    ----------\n    mean : torch.Tensor of shape (1, n_features)\n        New feature means.\n    std : torch.Tensor of shape (1, n_features)\n        New feature standard deviations.\n\n    Returns\n    -------\n    self : DNN\n        Estimator instance.\n    \"\"\"\n    self.bn = BatchNorm(mean, std)\n    return self\n</code></pre>"},{"location":"api/pfi/utils/nns/#pfi.utils.nns.FastTensorDataLoader","title":"<code>FastTensorDataLoader(*tensors, batch_size=32, shuffle=False)</code>","text":"<p>Lightweight mini-batch iterator over in-memory tensors.</p> <p>Parameters:</p> <ul> <li> <code>*tensors</code>               (<code>tuple of torch.Tensor</code>, default:                   <code>()</code> )           \u2013            <p>Tensors with matching first dimension.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>Number of samples per yielded batch.</p> </li> <li> <code>shuffle</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, shuffle samples at each iteration.</p> </li> </ul> Source code in <code>pfi/utils/nns.py</code> <pre><code>def __init__(\n    self,\n    *tensors,\n    batch_size=32,\n    shuffle=False,\n):\n    assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n    self.tensors = tensors\n    self.dataset_len = self.tensors[0].shape[0]\n    self.batch_size = batch_size\n    self.shuffle = shuffle\n    n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n    if remainder &gt; 0:\n        n_batches += 1\n    self.n_batches = n_batches\n</code></pre>"},{"location":"api/pfi/utils/nns/#pfi.utils.nns.FreezeVarDNN","title":"<code>FreezeVarDNN(dnn, var_index, var_value)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper that fixes one input feature to a constant before inference.</p> <p>Parameters:</p> <ul> <li> <code>dnn</code>               (<code>Module</code>)           \u2013            <p>Base network receiving <code>n_features</code> inputs.</p> </li> <li> <code>var_index</code>               (<code>int</code>)           \u2013            <p>Index of the feature to overwrite.</p> </li> <li> <code>var_value</code>               (<code>float</code>)           \u2013            <p>Constant value assigned to that feature.</p> </li> </ul> Source code in <code>pfi/utils/nns.py</code> <pre><code>def __init__(\n    self,\n    dnn,\n    var_index,\n    var_value,\n):\n    super(FreezeVarDNN, self).__init__()\n    self.dnn = dnn\n    self.var_index = var_index\n    self.var_value = var_value\n</code></pre>"},{"location":"api/pfi/utils/nns/#pfi.utils.nns.LayerNoWN","title":"<code>LayerNoWN(in_features, out_features, seed, activation)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Linear layer with Xavier initialization and no weight normalization.</p> <p>Parameters:</p> <ul> <li> <code>in_features</code>               (<code>int</code>)           \u2013            <p>Input feature dimension.</p> </li> <li> <code>out_features</code>               (<code>int</code>)           \u2013            <p>Output feature dimension.</p> </li> <li> <code>seed</code>               (<code>int</code>)           \u2013            <p>Random seed used for initialization.</p> </li> <li> <code>activation</code>               (<code>Module</code>)           \u2013            <p>Activation used in surrounding network to set initialization gain.</p> </li> </ul> Source code in <code>pfi/utils/nns.py</code> <pre><code>def __init__(\n    self,\n    in_features,\n    out_features,\n    seed,\n    activation,\n):\n    super(LayerNoWN, self).__init__()\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    self.linear = nn.Linear(in_features=in_features, out_features=out_features)\n    gain = 5 / 3 if isinstance(activation, nn.Tanh) else 1\n    nn.init.xavier_normal_(self.linear.weight, gain=gain)\n    nn.init.zeros_(self.linear.bias)\n</code></pre>"},{"location":"api/pfi/utils/nns/#pfi.utils.nns.SpectralNormDNN","title":"<code>SpectralNormDNN(sizes, mean=0, std=1, seed=0, activation=nn.Tanh())</code>","text":"<p>               Bases: <code>Module</code></p> <p>Fully-connected network with spectral normalization on hidden layers.</p> <p>Parameters:</p> <ul> <li> <code>sizes</code>               (<code>list of int</code>)           \u2013            <p>Layer sizes including input and output dimensions.</p> </li> <li> <code>mean</code>               (<code>float or Tensor</code>, default:                   <code>0</code> )           \u2013            <p>Initial mean used by the internal normalizer.</p> </li> <li> <code>std</code>               (<code>float or Tensor</code>, default:                   <code>1</code> )           \u2013            <p>Initial standard deviation used by the internal normalizer.</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Random seed for layer initialization.</p> </li> <li> <code>activation</code>               (<code>Module</code>, default:                   <code>torch.nn.Tanh()</code> )           \u2013            <p>Hidden activation module.</p> </li> </ul> Source code in <code>pfi/utils/nns.py</code> <pre><code>def __init__(\n    self,\n    sizes,\n    mean=0,\n    std=1,\n    seed=0,\n    activation=nn.Tanh(),\n):\n    super(SpectralNormDNN, self).__init__()\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    self.bn = BatchNorm(mean, std)\n    layers = []\n    for i in range(len(sizes) - 2):\n        linear = nn.Linear(sizes[i], sizes[i + 1])\n        linear = nn_utils.spectral_norm(linear)\n        layers.append(linear)\n        layers.append(activation)\n    final_linear = nn.Linear(sizes[-2], sizes[-1])\n    layers.append(final_linear)\n    self.net = nn.Sequential(*layers)\n</code></pre>"},{"location":"api/pfi/utils/nns/#pfi.utils.nns.SpectralNormDNN.set_scales","title":"<code>set_scales(mean, std)</code>","text":"<p>Update normalization statistics.</p> <p>Parameters:</p> <ul> <li> <code>mean</code>               (<code>torch.Tensor of shape (1, n_features)</code>)           \u2013            <p>New feature means.</p> </li> <li> <code>std</code>               (<code>torch.Tensor of shape (1, n_features)</code>)           \u2013            <p>New feature standard deviations.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>self</code> (              <code>DNN</code> )          \u2013            <p>Estimator instance.</p> </li> </ul> Source code in <code>pfi/utils/nns.py</code> <pre><code>def set_scales(self, mean, std):\n    \"\"\"Update normalization statistics.\n\n    Parameters\n    ----------\n    mean : torch.Tensor of shape (1, n_features)\n        New feature means.\n    std : torch.Tensor of shape (1, n_features)\n        New feature standard deviations.\n\n    Returns\n    -------\n    self : DNN\n        Estimator instance.\n    \"\"\"\n    self.bn = BatchNorm(mean, std)\n    return self\n</code></pre>"},{"location":"api/pfi/utils/nns/#pfi.utils.nns.divergence","title":"<code>divergence(field, x)</code>","text":"<p>Compute coordinate-wise divergence terms of a vector field.</p> <p>Parameters:</p> <ul> <li> <code>field</code>               (<code>torch.Tensor of shape (batch_size, ndim)</code>)           \u2013            <p>Vector field evaluated at <code>x</code>.</p> </li> <li> <code>x</code>               (<code>torch.Tensor of shape (batch_size, ndim)</code>)           \u2013            <p>Input points with gradient tracking enabled.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>div</code> (              <code>torch.Tensor of shape (batch_size, ndim)</code> )          \u2013            <p>Diagonal Jacobian terms, one per coordinate.</p> </li> </ul> Source code in <code>pfi/utils/nns.py</code> <pre><code>def divergence(\n    field,\n    x,\n):\n    \"\"\"Compute coordinate-wise divergence terms of a vector field.\n\n    Parameters\n    ----------\n    field : torch.Tensor of shape (batch_size, ndim)\n        Vector field evaluated at ``x``.\n    x : torch.Tensor of shape (batch_size, ndim)\n        Input points with gradient tracking enabled.\n\n    Returns\n    -------\n    div : torch.Tensor of shape (batch_size, ndim)\n        Diagonal Jacobian terms, one per coordinate.\n    \"\"\"\n    dim = field.shape[1]\n    div = torch.zeros((field.shape[0], dim), device=x.device)\n    for i in range(dim):\n        out_ = field[:, i]\n        gradient = grad(\n            outputs=out_,\n            inputs=x,\n            grad_outputs=torch.ones_like(out_),\n            create_graph=True,\n        )[0]\n        div[:, i] = gradient[:, i]\n    return div\n</code></pre>"},{"location":"api/pfi/utils/nns/#pfi.utils.nns.loss_grad_std","title":"<code>loss_grad_std(loss, net, device)</code>","text":"<p>Estimate the pooled standard deviation of layer gradients.</p> <p>Parameters:</p> <ul> <li> <code>loss</code>               (<code>torch.Tensor of shape ()</code>)           \u2013            <p>Scalar loss value.</p> </li> <li> <code>net</code>               (<code>Module</code>)           \u2013            <p>Network containing linear layers.</p> </li> <li> <code>device</code>               (<code>str or device</code>)           \u2013            <p>Device used for intermediate tensors.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>std</code> (              <code>torch.Tensor of shape ()</code> )          \u2013            <p>Pooled gradient standard deviation across linear layer parameters.</p> </li> </ul> Source code in <code>pfi/utils/nns.py</code> <pre><code>def loss_grad_std(\n    loss,\n    net,\n    device,\n):\n    \"\"\"Estimate the pooled standard deviation of layer gradients.\n\n    Parameters\n    ----------\n    loss : torch.Tensor of shape ()\n        Scalar loss value.\n    net : torch.nn.Module\n        Network containing linear layers.\n    device : str or torch.device\n        Device used for intermediate tensors.\n\n    Returns\n    -------\n    std : torch.Tensor of shape ()\n        Pooled gradient standard deviation across linear layer parameters.\n    \"\"\"\n    var = []\n    siz = []\n    for m in net.modules():\n        if not isinstance(m, nn.Linear):\n            continue\n        w = grad(loss, m.weight, retain_graph=True, allow_unused=True)[0]\n        b = grad(loss, m.bias, retain_graph=True, allow_unused=True)[0]\n        if w is None or b is None:\n            continue\n        wb = torch.cat((w.view(-1), b))\n        nit = torch.numel(wb)\n        var.append((nit - 1) * torch.var(wb))\n        siz.append(nit)\n    vart = torch.tensor(var, dtype=torch.float32, device=device)\n    sizt = torch.tensor(siz, dtype=torch.float32, device=device)\n    return torch.sqrt(torch.sum(vart) / (torch.sum(sizt) - len(sizt)))\n</code></pre>"},{"location":"api/pfi/utils/simulations/","title":"simulations","text":""},{"location":"api/pfi/utils/simulations/#pfi.utils.simulations","title":"<code>pfi.utils.simulations</code>","text":"<p>Simulation utilities for benchmark dynamical systems.</p>"},{"location":"api/pfi/utils/simulations/#pfi.utils.simulations.g_rate","title":"<code>g_rate(x1, x2, gr)</code>","text":"<p>Compute the cell growth rate used in the toggle-switch simulator.</p> <p>Parameters:</p> <ul> <li> <code>x1</code>               (<code>ndarray of shape (n_samples,)</code>)           \u2013            <p>First coordinate of the state.</p> </li> <li> <code>x2</code>               (<code>ndarray of shape (n_samples,)</code>)           \u2013            <p>Second coordinate of the state.</p> </li> <li> <code>gr</code>               (<code>float</code>)           \u2013            <p>Growth-rate scale.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>rate</code> (              <code>ndarray of shape (n_samples,)</code> )          \u2013            <p>Growth rate for each sample.</p> </li> </ul> Source code in <code>pfi/utils/simulations.py</code> <pre><code>def g_rate(\n    x1,\n    x2,\n    gr,\n):\n    \"\"\"Compute the cell growth rate used in the toggle-switch simulator.\n\n    Parameters\n    ----------\n    x1 : ndarray of shape (n_samples,)\n        First coordinate of the state.\n    x2 : ndarray of shape (n_samples,)\n        Second coordinate of the state.\n    gr : float\n        Growth-rate scale.\n\n    Returns\n    -------\n    rate : ndarray of shape (n_samples,)\n        Growth rate for each sample.\n    \"\"\"\n    return gr * (1.0 * (x2 ** 2) / (1 + x2 ** 2) + 0.0 * (x1 ** 2) / (1 + x1 ** 2))\n</code></pre>"},{"location":"api/pfi/utils/simulations/#pfi.utils.simulations.simulate_ornstein_uhlenbeck","title":"<code>simulate_ornstein_uhlenbeck(Om, D, m0, S0, nsamples, ndim, Dt, K, dt=0.006)</code>","text":"<p>Simulate snapshots from a linear Ornstein-Uhlenbeck process.</p> <p>Parameters:</p> <ul> <li> <code>Om</code>               (<code>ndarray of shape (ndim, ndim)</code>)           \u2013            <p>Drift matrix.</p> </li> <li> <code>D</code>               (<code>ndarray of shape (ndim, ndim)</code>)           \u2013            <p>Diffusion matrix.</p> </li> <li> <code>m0</code>               (<code>ndarray of shape (ndim,)</code>)           \u2013            <p>Mean of the initial Gaussian distribution.</p> </li> <li> <code>S0</code>               (<code>float</code>)           \u2013            <p>Isotropic variance factor for the initial covariance <code>S0 * I</code>.</p> </li> <li> <code>nsamples</code>               (<code>int</code>)           \u2013            <p>Number of particles sampled at each snapshot time.</p> </li> <li> <code>ndim</code>               (<code>int</code>)           \u2013            <p>State dimension.</p> </li> <li> <code>Dt</code>               (<code>float</code>)           \u2013            <p>Snapshot interval in simulation time.</p> </li> <li> <code>K</code>               (<code>int</code>)           \u2013            <p>Number of snapshots.</p> </li> <li> <code>dt</code>               (<code>float</code>, default:                   <code>0.006</code> )           \u2013            <p>Euler-Maruyama integration step.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>list of length K</code> )          \u2013            <p><code>samples[k]</code> is an ndarray of shape <code>(nsamples, ndim)</code>.</p> </li> <li> <code>tt</code> (              <code>ndarray of shape (K,)</code> )          \u2013            <p>Snapshot times.</p> </li> </ul> Source code in <code>pfi/utils/simulations.py</code> <pre><code>def simulate_ornstein_uhlenbeck(\n    Om,\n    D,\n    m0,\n    S0,\n    nsamples,\n    ndim,\n    Dt,\n    K,\n    dt=0.006,\n):\n    \"\"\"Simulate snapshots from a linear Ornstein-Uhlenbeck process.\n\n    Parameters\n    ----------\n    Om : ndarray of shape (ndim, ndim)\n        Drift matrix.\n    D : ndarray of shape (ndim, ndim)\n        Diffusion matrix.\n    m0 : ndarray of shape (ndim,)\n        Mean of the initial Gaussian distribution.\n    S0 : float\n        Isotropic variance factor for the initial covariance ``S0 * I``.\n    nsamples : int\n        Number of particles sampled at each snapshot time.\n    ndim : int\n        State dimension.\n    Dt : float\n        Snapshot interval in simulation time.\n    K : int\n        Number of snapshots.\n    dt : float, default=0.006\n        Euler-Maruyama integration step.\n\n    Returns\n    -------\n    samples : list of length K\n        ``samples[k]`` is an ndarray of shape ``(nsamples, ndim)``.\n    tt : ndarray of shape (K,)\n        Snapshot times.\n    \"\"\"\n    samples = []\n    tt = np.zeros((K,))\n    record = int(Dt / dt)\n\n    for j in range(1, K + 1):\n        traj_init = np.random.multivariate_normal(m0, S0 * np.eye(ndim), size=nsamples).T\n        traj = traj_init.copy()\n\n        for i in range(0, j * record):\n            xi = np.random.normal(0, 1, (ndim, nsamples))\n            traj = traj - (Om @ traj) * dt + (np.sqrt(2 * D) @ xi) * np.sqrt(dt)\n\n        tt[j - 1] = dt * i\n        samples.append(traj.T.copy())\n\n    return samples, tt\n</code></pre>"},{"location":"api/pfi/utils/simulations/#pfi.utils.simulations.simulate_toggle_switch","title":"<code>simulate_toggle_switch(nsamples, init, nsnaps, ndim, seed, maxiter, model_params, vol, gr, growth_flag=False)</code>","text":"<p>Simulate stochastic toggle-switch dynamics with optional growth.</p> <p>Parameters:</p> <ul> <li> <code>nsamples</code>               (<code>int</code>)           \u2013            <p>Initial number of particles.</p> </li> <li> <code>init</code>               (<code>ndarray of shape (ndim, nsamples)</code>)           \u2013            <p>Initial state matrix.</p> </li> <li> <code>nsnaps</code>               (<code>int</code>)           \u2013            <p>Number of snapshot times.</p> </li> <li> <code>ndim</code>               (<code>int</code>)           \u2013            <p>State dimension.</p> </li> <li> <code>seed</code>               (<code>int</code>)           \u2013            <p>Random seed.</p> </li> <li> <code>maxiter</code>               (<code>int</code>)           \u2013            <p>Number of discrete integration iterations.</p> </li> <li> <code>model_params</code>               (<code>array-like of shape (7,)</code>)           \u2013            <p>Toggle-switch parameters <code>[a1, a2, b1, b2, k1, k2, n]</code>.</p> </li> <li> <code>vol</code>               (<code>float</code>)           \u2013            <p>System volume scaling the stochastic term.</p> </li> <li> <code>gr</code>               (<code>float</code>)           \u2013            <p>Growth-rate scale used when <code>growth_flag=True</code>.</p> </li> <li> <code>growth_flag</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, cells are duplicated according to growth probabilities.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples_full</code> (              <code>list of length nsnaps</code> )          \u2013            <p><code>samples_full[k]</code> is an ndarray of shape <code>(n_k, ndim)</code>, where <code>n_k</code> can increase across snapshots when growth is enabled.</p> </li> <li> <code>tt</code> (              <code>ndarray of shape (nsnaps,)</code> )          \u2013            <p>Snapshot times.</p> </li> </ul> Source code in <code>pfi/utils/simulations.py</code> <pre><code>def simulate_toggle_switch(\n    nsamples,\n    init,\n    nsnaps,\n    ndim,\n    seed,\n    maxiter,\n    model_params,\n    vol,\n    gr,\n    growth_flag=False,\n):\n    \"\"\"Simulate stochastic toggle-switch dynamics with optional growth.\n\n    Parameters\n    ----------\n    nsamples : int\n        Initial number of particles.\n    init : ndarray of shape (ndim, nsamples)\n        Initial state matrix.\n    nsnaps : int\n        Number of snapshot times.\n    ndim : int\n        State dimension.\n    seed : int\n        Random seed.\n    maxiter : int\n        Number of discrete integration iterations.\n    model_params : array-like of shape (7,)\n        Toggle-switch parameters ``[a1, a2, b1, b2, k1, k2, n]``.\n    vol : float\n        System volume scaling the stochastic term.\n    gr : float\n        Growth-rate scale used when ``growth_flag=True``.\n    growth_flag : bool, default=False\n        If ``True``, cells are duplicated according to growth probabilities.\n\n    Returns\n    -------\n    samples_full : list of length nsnaps\n        ``samples_full[k]`` is an ndarray of shape ``(n_k, ndim)``, where\n        ``n_k`` can increase across snapshots when growth is enabled.\n    tt : ndarray of shape (nsnaps,)\n        Snapshot times.\n    \"\"\"\n    dt = 0.01\n    lx = 1.0\n    np.random.seed(seed)\n\n    tt = np.zeros(nsnaps)\n    samples_full = []\n    steps = int(maxiter / nsnaps)\n\n    for snap in range(nsnaps):\n        t_snap = snap * steps * dt\n        tt[snap] = t_snap\n\n        xold = init + 0.1 * np.random.normal(0, 1, (ndim, nsamples))\n\n        for _ in range(0, snap * steps + 1):\n            fval = toggle_switch(xold.T, model_params).T\n            noise = np.sqrt(fval + lx * xold) * np.sqrt(dt) * np.random.normal(0, 1, xold.shape)\n            xnew = xold + dt * (fval - lx * xold) + (1 / np.sqrt(vol)) * noise\n\n            xnew = np.where(xnew &lt; 0, xold, xnew)\n            xold = xnew + 0.0\n\n            if growth_flag:\n                x1 = xold[0]\n                x2 = xold[1]\n                growth_probs = g_rate(x1, x2, gr) * dt\n                divide_flags = np.random.rand(x2.shape[0]) &lt; growth_probs\n                new_cells = xold[:, divide_flags]\n                if new_cells.shape[1] &gt; 0:\n                    xold = np.concatenate([xold, new_cells], axis=1)\n\n        samples_full.append(xold.T.copy())\n\n    return samples_full, tt\n</code></pre>"},{"location":"api/pfi/utils/simulations/#pfi.utils.simulations.toggle_switch","title":"<code>toggle_switch(x, model_params)</code>","text":"<p>Evaluate the deterministic toggle-switch production term.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>ndarray of shape (n_samples, 2)</code>)           \u2013            <p>State values, where each row is <code>[x1, x2]</code>.</p> </li> <li> <code>model_params</code>               (<code>array-like of shape (7,)</code>)           \u2013            <p>Model parameters <code>[a1, a2, b1, b2, k1, k2, n]</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>f</code> (              <code>ndarray of shape (n_samples, 2)</code> )          \u2013            <p>Deterministic production term for each sample.</p> </li> </ul> Source code in <code>pfi/utils/simulations.py</code> <pre><code>def toggle_switch(\n    x,\n    model_params,\n):\n    \"\"\"Evaluate the deterministic toggle-switch production term.\n\n    Parameters\n    ----------\n    x : ndarray of shape (n_samples, 2)\n        State values, where each row is ``[x1, x2]``.\n    model_params : array-like of shape (7,)\n        Model parameters ``[a1, a2, b1, b2, k1, k2, n]``.\n\n    Returns\n    -------\n    f : ndarray of shape (n_samples, 2)\n        Deterministic production term for each sample.\n    \"\"\"\n    a1, a2, b1, b2, k1, k2, n = model_params\n    xn = x ** n + 0.0\n    f = np.zeros_like(x)\n    f[:, 0] = (a1 * (xn[:, 0]) / (k1 ** n + xn[:, 0])) + b1 * (k1 ** n) / (k1 ** n + xn[:, 1])\n    f[:, 1] = (a2 * (xn[:, 1]) / (k2 ** n + xn[:, 1])) + b2 * (k2 ** n) / (k2 ** n + xn[:, 0])\n    return f\n</code></pre>"}]}